{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Lab Assignment 1",
      "provenance": [],
      "collapsed_sections": [
        "Wwo1PN7F1oCj",
        "nNyKutLr1dis",
        "UPDjDRjJ04G_",
        "1HQCbyzm06J2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GlGRWcpuDVt"
      },
      "source": [
        "## NLP Lab Assignment 1\n",
        "\n",
        "***Student Details:***\n",
        "\n",
        "- Name : Anjishnu Mukherjee\n",
        "- Registration Number : B05-511017020\n",
        "- Exam Roll Number : 510517086\n",
        "- Email : 511017020.anjishnu@students.iiests.ac.in"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJUxdRxG1hBd"
      },
      "source": [
        "## Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P36wvOdZ0HCD",
        "outputId": "ebeb0eca-6c63-465b-b55e-e94812f559d6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-C6QAXD1kYi"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OzS0GrV0O81"
      },
      "source": [
        "import random\n",
        "import string\n",
        "from typing import List, Optional\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import brown, stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQLRSXGS0Qec",
        "outputId": "4f0bb30f-7092-4229-c075-b523c73e22df"
      },
      "source": [
        "nltk.download(\"brown\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wwo1PN7F1oCj"
      },
      "source": [
        "## Data files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnVJzcO10Swr"
      },
      "source": [
        "data_dir = \"/content/drive/MyDrive/NLP_LAB/Assignment-1/\"\n",
        "data_file_1 = \"sample-text-1.txt\"\n",
        "data_file_2 = \"sample-text-2.txt\"\n",
        "bengali_stopwords = \"stopwords-bn.txt\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNyKutLr1dis"
      },
      "source": [
        "## Solution class\n",
        "\n",
        "- Here, I define a class which takes the language, stopwords and the input text file as parameters. \n",
        "- All the required functionality of the assignment are defined as methods of this class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djqPSftfkGO4"
      },
      "source": [
        "class Basics:\n",
        "    ''' Assignment 1 Solution Class'''\n",
        "\n",
        "    def __init__(self, path: str, language: str,\n",
        "                 stop_words: Optional[str] = None) -> None:\n",
        "        '''Creates an object of the class with given parameters.'''\n",
        "\n",
        "        super().__init__()\n",
        "        self.PATH = path\n",
        "        self.language = language\n",
        "        self.stop_words = stop_words\n",
        "\n",
        "        try:\n",
        "            with open(self.PATH, 'r') as inputFile:\n",
        "                self.raw_text = inputFile.read()\n",
        "        except IOError:\n",
        "            self.raw_text = None\n",
        "            print(\"Couldn't read input file.\")\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        '''Formalized string representation of this class.'''\n",
        "\n",
        "        rep = self.__class__.__qualname__ + \\\n",
        "            '(' + self.PATH + ',' + self.language + ') object.'\n",
        "        return rep\n",
        "\n",
        "    def add_line_numbers(self) -> None:\n",
        "        '''Add a line number to every non-empty line of input file and print it.'''\n",
        "\n",
        "        if self.raw_text is not None:\n",
        "            line_number = 1\n",
        "            for line in self.raw_text.splitlines():\n",
        "\n",
        "                # empty lines and whitespaces are ignored\n",
        "                if not line.isspace() and line != \"\":\n",
        "                    print(line_number, \"\\t\", line)\n",
        "                    line_number += 1\n",
        "        else:\n",
        "            raise Exception(\"Couldn't read input file.\")\n",
        "\n",
        "    def __tokenize(self, text: Optional[str] = None) -> List:\n",
        "        '''Normalizes input text by converting to lower case and then tokenizing\n",
        "        it.'''\n",
        "\n",
        "        if not text:\n",
        "            if self.raw_text is not None:\n",
        "                return word_tokenize(self.raw_text.lower())\n",
        "            else:\n",
        "                raise Exception(\"Couldn't read input file.\")\n",
        "        else:\n",
        "            if text:\n",
        "                return word_tokenize(text.lower())\n",
        "            else:\n",
        "                raise Exception(\"Can't tokenize input string.\")\n",
        "\n",
        "    def __remove_punctuations(self, tokens: List) -> List:\n",
        "        '''Removes all possible punctuations from the list of tokens.'''\n",
        "\n",
        "        if tokens:\n",
        "            # list of all English punctuations\n",
        "            punctuations = set(string.punctuation)\n",
        "\n",
        "            # this is the only Bengali punctuation which is not there in English\n",
        "            if self.language == 'bn':\n",
        "                punctuations.add('ред')\n",
        "\n",
        "            # if at least one of the characters is not a punctuation, it is a word.\n",
        "            # this removes all the single character punctuations from the list of\n",
        "            # tokens.\n",
        "            words = []\n",
        "            for token in tokens:\n",
        "                for char in token:\n",
        "                    if char not in punctuations:\n",
        "                        words.append(token)\n",
        "                        break\n",
        "\n",
        "            return words\n",
        "        else:\n",
        "            raise Exception(\"No tokens found.\")\n",
        "\n",
        "    def __clean_tokens(self, tokens: List) -> List:\n",
        "        '''Removes stopwords, contractions, conjunctions.'''\n",
        "\n",
        "        if tokens:\n",
        "            # use english stopwords by default, otherwise use specified stopwords\n",
        "            if self.stop_words == None:\n",
        "                stop_words = set(stopwords.words('english'))\n",
        "            else:\n",
        "                stop_words = self.stop_words\n",
        "\n",
        "            # remove stopwords from collection of tokens\n",
        "            clean_tokens = []\n",
        "            for word in tokens:\n",
        "                if word not in stop_words:\n",
        "                    clean_tokens.append(word)\n",
        "\n",
        "            # remove contractions (eg. hasn't) and conjunctions (eg. on-campus)\n",
        "            # the following list comprehension removes a token if any character in\n",
        "            # the token is not an alphabet\n",
        "            if self.language == 'en':\n",
        "                proper_english_tokens = []\n",
        "                for word in clean_tokens:\n",
        "                    if word.isalpha():\n",
        "                        proper_english_tokens.append(word)\n",
        "\n",
        "                return proper_english_tokens\n",
        "\n",
        "            return clean_tokens\n",
        "        else:\n",
        "            raise Exception(\"No tokens found.\")\n",
        "\n",
        "    def vocab_size(self, text: Optional[str] = None) -> int:\n",
        "        '''The size of the vocabulary is the number of unique tokens in input\n",
        "        text.'''\n",
        "\n",
        "        # use a set to represent the vocabulary, as a set only stores unique\n",
        "        # elements\n",
        "        if not text:\n",
        "            if self.raw_text is not None:\n",
        "                vocabulary = set(self.__tokenize(self.raw_text))\n",
        "                return len(vocabulary)\n",
        "            else:\n",
        "                raise Exception(\"Couldn't read input file.\")\n",
        "        else:\n",
        "            if text:\n",
        "                vocabulary = set(self.__tokenize(text))\n",
        "                return len(vocabulary)\n",
        "            else:\n",
        "                raise Exception(\"Can't create vocabulary.\")\n",
        "\n",
        "    def word_freq(self, word: str, section: str) -> int:\n",
        "        '''Computes frequency of input word in given section of Brown corpus.'''\n",
        "\n",
        "        # find all the words in the given section\n",
        "        section_words = self.__remove_punctuations(\n",
        "            list(brown.words(categories=section)))\n",
        "\n",
        "        # find frequency of given word\n",
        "        freq = 0\n",
        "        for token in section_words:\n",
        "            if token == word:\n",
        "                freq += 1\n",
        "        return freq\n",
        "\n",
        "    def test_word_freq(self) -> None:\n",
        "        '''Test the word_freq method of the class by comparing with\n",
        "        nltk.FreqDist.'''\n",
        "\n",
        "        # choose 5 random categories of brown corpus\n",
        "        brown_categories = brown.categories()\n",
        "        test_categories = random.sample(brown_categories, 5)\n",
        "\n",
        "        print(\"-\" * 26)\n",
        "        print('{:>5} |{:>16}'.format(\"word_freq\", \"nltk.FreqDist\"))\n",
        "        print(\"-\" * 26)\n",
        "\n",
        "        # choose 3 random words from each of the 5 random categories chosen above\n",
        "        for category in test_categories:\n",
        "            section_words = self.__remove_punctuations(\n",
        "                list(brown.words(categories=category)))\n",
        "            test_words = random.sample(section_words, 3)\n",
        "            nltk_freq_dist = nltk.FreqDist(section_words)\n",
        "            for word in test_words:\n",
        "                    # verify the correctness using assert statement\n",
        "                assert self.word_freq(word, category) == nltk_freq_dist[word]\n",
        "\n",
        "                # print the values as well for visual comparison\n",
        "                print('{:>5}{:>15}'.format(self.word_freq(word, category),\n",
        "                                           nltk_freq_dist[word]))\n",
        "\n",
        "    def percent(self, word: str, text: Optional[str] = None) -> float:\n",
        "        '''Calculates how often a word occurs in a text as a percentage.'''\n",
        "\n",
        "        if not text:\n",
        "            if self.raw_text is not None:\n",
        "                # calculate total number of words\n",
        "                text_words = self.__remove_punctuations(\n",
        "                    self.__tokenize(self.raw_text))\n",
        "                total_count = len(text_words)\n",
        "\n",
        "                # calculate frequency of given word\n",
        "                frequency = 0\n",
        "                for token in text_words:\n",
        "                    if token == word:\n",
        "                        frequency += 1\n",
        "\n",
        "                # return percentage for word\n",
        "                return (frequency / total_count) * 100\n",
        "            else:\n",
        "                raise Exception(\"Couldn't read input file.\")\n",
        "        else:\n",
        "            if text:\n",
        "                # calculate total number of words\n",
        "                text_words = self.__remove_punctuations(self.__tokenize(text))\n",
        "                total_count = len(text_words)\n",
        "\n",
        "                # calculate frequency of given word\n",
        "                frequency = 0\n",
        "                for token in text_words:\n",
        "                    if token == word:\n",
        "                        frequency += 1\n",
        "\n",
        "                # return percentage for word\n",
        "                return (frequency / total_count) * 100\n",
        "            else:\n",
        "                raise Exception(\n",
        "                    \"Can't calculate frequency of word in input text.\")\n",
        "\n",
        "    def n_most_frequent(self, text: Optional[str] = None,\n",
        "                        num_words: Optional[int] = -1) -> List:\n",
        "        '''Finds N most frequent words of text, except stopwords, contractions,\n",
        "        conjunctions, punctuations.'''\n",
        "\n",
        "        # remove punctuations\n",
        "        if not text:\n",
        "            if self.raw_text is not None:\n",
        "                tokens = self.__remove_punctuations(\n",
        "                    self.__tokenize(self.raw_text))\n",
        "            else:\n",
        "                raise Exception(\"Couldn't read input file.\")\n",
        "        else:\n",
        "            if text:\n",
        "                tokens = self.__remove_punctuations(self.__tokenize(text))\n",
        "            else:\n",
        "                raise Exception(\"Can't tokenize input.\")\n",
        "\n",
        "        # remove stopwords, contractions, conjunctions\n",
        "        cleaned_tokens = self.__clean_tokens(tokens)\n",
        "\n",
        "        # calculate freq distribution of the tokens\n",
        "        freq_dist = nltk.FreqDist(cleaned_tokens)\n",
        "\n",
        "        # sort the frequency distribution in decreasing order of frequency\n",
        "        sorted_freq_dist = sorted(freq_dist.items(), key=lambda item: -item[1])\n",
        "\n",
        "        # if num_words is -1 (default), then return all word frequencies\n",
        "        most_freq_words = []\n",
        "        if num_words == -1:\n",
        "            for i in range(len(sorted_freq_dist)):\n",
        "                most_freq_words.append(sorted_freq_dist[i][0])\n",
        "        else:\n",
        "            for i in range(min(len(sorted_freq_dist), num_words)):\n",
        "                most_freq_words.append(sorted_freq_dist[i][0])\n",
        "\n",
        "        return most_freq_words\n",
        "\n",
        "    def n_letter_words(self, text: Optional[str] = None,\n",
        "                       num_words: Optional[int] = 4) -> List:\n",
        "        '''Finds all n letter words and prints them in decreasing order of\n",
        "        frequency.'''\n",
        "\n",
        "        # find reverse-sorted frequency of all words\n",
        "        if not text:\n",
        "            if self.raw_text is not None:\n",
        "                freq_sorted_words = self.n_most_frequent()\n",
        "            else:\n",
        "                raise Exception(\"Couldn't read input file.\")\n",
        "        else:\n",
        "            if text:\n",
        "                freq_sorted_words = self.n_most_frequent(text)\n",
        "            else:\n",
        "                raise Exception(\"Can't tokenize input.\")\n",
        "\n",
        "        # choose only n letter words from above output\n",
        "        n_letter_words = []\n",
        "        for word in freq_sorted_words:\n",
        "            if len(word) == num_words:\n",
        "                n_letter_words.append(word)\n",
        "\n",
        "        return n_letter_words\n",
        "\n",
        "    def words_occuring_n_times(self, count: Optional[int] = 3) -> List:\n",
        "        '''Finds all words that occur atleast n times in the Brown Corpus.'''\n",
        "\n",
        "        # all categories in brown corpus\n",
        "        brown_categories = brown.categories()\n",
        "\n",
        "        # aggregate all words from each category\n",
        "        all_words = []\n",
        "        for category in brown_categories:\n",
        "            section_words = self.__remove_punctuations(\n",
        "                list(brown.words(categories=category)))\n",
        "            all_words.extend(section_words)\n",
        "\n",
        "        # words from the list above, which have frequency >= count\n",
        "        valid_words = []\n",
        "        nltk_freq_dist = dict(nltk.FreqDist(all_words))\n",
        "        for word, freq in nltk_freq_dist.items():\n",
        "            if freq >= count:\n",
        "                valid_words.append(word)\n",
        "\n",
        "        return valid_words"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPDjDRjJ04G_"
      },
      "source": [
        "## English\n",
        "\n",
        "- For stopwords, I use the default stopwords collection from nltk for english.\n",
        "- The tasks mentioned in the assignment are executed consecutively in separate cells to show the output for each."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uunEqJ-50iLb"
      },
      "source": [
        "english_solution = Basics(data_dir+data_file_1, 'en')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0Pj9WCO0n4V",
        "outputId": "a374a6e7-5676-4310-8708-a34d23a203c1"
      },
      "source": [
        "english_solution.add_line_numbers()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 \t я╗┐Braving winter rain, thousands gathered at the Bhupen Hazarika memorial at Jalukbari here to join a protest rally against the Citizenship Amendment Act (CAA).\n",
            "2 \t Organised by the artistes of the state and backed by the All Assam StudentsтАЩ Union (AASU), thousands of protesters, wearing black masks, hit the city streets to express their anger against the amended citizenship law and vowed to uproot the current government if it failed to respect public sentiment.\n",
            "3 \t The huge rally was organised a day after the BJP, in a show of strength, held a massive gathering of its workers in support of CAA at Khanapara ground.\n",
            "4 \t Leading SundayтАЩs rally from Jalukbari to Dighalipukhuri, a distance of around 14km, AASU general secretary Lurinjyoti Gogoi said, тАЬThe BJP showed its strength with its party workers but the anti-CAA movement has public support. In a democracy, public is the power and this movement has witnessed spontaneous public support from day one.тАЭ\n",
            "5 \t The AASU leader also warned the government against using power to suppress the movement. тАЬThe government has killed five people and many of their hired goons are attacking those who are opposing the Act. We warn the government not to apply force or power to dominate the movement,тАЭ he added.\n",
            "6 \t AASU chief adviser Samujjal Bhattacharjya said the movement would continue till the ultimate goal of securing the constitutional safeguards for the indigenous people of the state was achieved.\n",
            "7 \t Bhattacharjya said, тАЬThis movement will not stop till we achieve our final goal of securing the rights of the indigenous communities of the state.тАЭ\n",
            "8 \t Countering state BJPтАЩs president Ranjeet Kumar DassтАЩs that тАЬduck sellers, chicken sellers, vegetable vendors and egg sellersтАЭ were participating in the anti-CAA rallies, Assam Jatiyatabadi Yuva Chatra Parishad (AJYCP) general secretary Palash Changmai said, тАЬWhy canтАЩt vegetable vendors or chicken sellers join the movement? If a tea seller can become the Prime Minister then I am sure a vegetable seller is also a patriot and can assert his democratic voice.тАЭ\n",
            "9 \t Singer Zubeen Garg who led the artiste community on Sunday, added, тАЬThe government is not listening to our voices and so we have worn these black masks in protest. If needed we are ready for a political alternative. We are ready to uproot this government if it continues to be blind and deaf to the peopleтАЩs cry.тАЭ\n",
            "10 \t Amid the anti-CAA movement in the state, the demand for a political alternative to oust the ruling BJP in the next Assembly election is also growing.\n",
            "11 \t Popular singer Manas Robin said the artiste community would decide to take forward the protest against the amended citizenship law on January 17, when the state celebrates Shilpi Divas in memory of AssamтАЩs cultural icon Jyoti Prasad Agarwala.\n",
            "12 \t He also called upon the various communities and organisations of the state to join hands in opposing CAA.\n",
            "13 \t Rain also failed to sdampen the spirit of the CAA protesters who gathered at Jeotimoral Sangha playground in Dibrugarh on Sunday.\n",
            "14 \t The protest was organised by Shilpi Samaj. The artiste fraternity protested through songs and poems.\n",
            "15 \t Addressing the gathering, Dibrugarh-based musician Prasanta Bordoloi said the CAA was against the people of Assam.\n",
            "16 \t тАЬThe ongoing anti-CAA movement reflects our rational and emotional sides. The CAA is against the secular fabric of the country and will divide the people of India. We, the artiste fraternity, assembled here to protest against it. The government has forcefully imposed it on Assam for vote bank politics,тАЭ Bordoloi said.\n",
            "17 \t The artistes expressed resentment by singing songs of Bhupen Hazarika.\n",
            "18 \t тАЬAssam is not a dumping ground for illegal Bangladeshis. We do not accept the CAA in Assam because it violates the Assam Accord. We will continue our protest till CAA is scrapped,тАЭ said another protester.\n",
            "19 \t Additional reporting by Avik Chakraborty in Dibrugarh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9wJGIk80pmI",
        "outputId": "76120ce8-45ec-4cd0-b9d6-520d13534c00"
      },
      "source": [
        "print('Vocabulary size : ', english_solution.vocab_size())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size :  298\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4zzvJqn0sHm",
        "outputId": "8ead9ce5-d94e-4bee-f8f5-4305641fad63"
      },
      "source": [
        "english_solution.test_word_freq()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------\n",
            "word_freq |   nltk.FreqDist\n",
            "--------------------------\n",
            "  806            806\n",
            "  263            263\n",
            "   44             44\n",
            "   36             36\n",
            "   61             61\n",
            "   95             95\n",
            "    3              3\n",
            "    6              6\n",
            "   85             85\n",
            "  127            127\n",
            "   16             16\n",
            "    2              2\n",
            "  530            530\n",
            "    1              1\n",
            "    2              2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWDzfld00tq5",
        "outputId": "45b5493e-3c74-4ae2-9c82-8804618f0bc7"
      },
      "source": [
        "word = 'we'\n",
        "print('Percentage of \\\"', word, '\\\" is : ', english_solution.percent(word), '%.')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of \" we \" is :  1.2326656394453006 %.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9S7O5Od0vSB",
        "outputId": "0101ab00-8b9d-46c8-e442-7ced07642650"
      },
      "source": [
        "print('10 most frequent words : ')\n",
        "print(english_solution.n_most_frequent(None, 10))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 most frequent words : \n",
            "['movement', 'caa', 'assam', 'said', 'government', 'protest', 'state', 'also', 'people', 'aasu']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkKPgy3e0wyj",
        "outputId": "427eb692-4321-49ae-9126-93e3e9f08c31"
      },
      "source": [
        "print('4 letter words in decreasing order of frequency from left to right: ')\n",
        "print(english_solution.n_letter_words())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4 letter words in decreasing order of frequency from left to right: \n",
            "['said', 'also', 'aasu', 'join', 'till', 'rain', 'goal', 'city', 'huge', 'show', 'held', 'five', 'many', 'warn', 'stop', 'dass', 'duck', 'yuva', 'sure', 'garg', 'worn', 'deaf', 'amid', 'oust', 'next', 'take', 'icon', 'upon', 'vote', 'bank', 'avik']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3MXt1VT0yUe",
        "outputId": "a084ea0b-0942-4a7b-f1ab-060344857fd0"
      },
      "source": [
        "print('Words occuring thrice in Brown corpus : ')\n",
        "english_solution.words_occuring_n_times()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words occuring thrice in Brown corpus : \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dan',\n",
              " 'Morgan',\n",
              " 'told',\n",
              " 'himself',\n",
              " 'he',\n",
              " 'would',\n",
              " 'forget',\n",
              " 'Ann',\n",
              " 'Turner',\n",
              " 'He',\n",
              " 'was',\n",
              " 'well',\n",
              " 'rid',\n",
              " 'of',\n",
              " 'her',\n",
              " 'certainly',\n",
              " \"didn't\",\n",
              " 'want',\n",
              " 'a',\n",
              " 'wife',\n",
              " 'who',\n",
              " 'as',\n",
              " 'If',\n",
              " 'had',\n",
              " 'married',\n",
              " \"he'd\",\n",
              " 'have',\n",
              " 'been',\n",
              " 'asking',\n",
              " 'for',\n",
              " 'trouble',\n",
              " 'But',\n",
              " 'all',\n",
              " 'this',\n",
              " 'Sometimes',\n",
              " 'woke',\n",
              " 'up',\n",
              " 'in',\n",
              " 'the',\n",
              " 'middle',\n",
              " 'night',\n",
              " 'thinking',\n",
              " 'and',\n",
              " 'then',\n",
              " 'could',\n",
              " 'not',\n",
              " 'get',\n",
              " 'back',\n",
              " 'to',\n",
              " 'sleep',\n",
              " 'His',\n",
              " 'plans',\n",
              " 'dreams',\n",
              " 'revolved',\n",
              " 'around',\n",
              " 'so',\n",
              " 'much',\n",
              " 'long',\n",
              " 'that',\n",
              " 'now',\n",
              " 'felt',\n",
              " 'if',\n",
              " 'nothing',\n",
              " 'The',\n",
              " 'easiest',\n",
              " 'thing',\n",
              " 'be',\n",
              " 'sell',\n",
              " 'out',\n",
              " 'Al',\n",
              " 'Budd',\n",
              " 'leave',\n",
              " 'country',\n",
              " 'but',\n",
              " 'there',\n",
              " 'stubborn',\n",
              " 'streak',\n",
              " 'him',\n",
              " \"wouldn't\",\n",
              " 'allow',\n",
              " 'it',\n",
              " 'best',\n",
              " 'bitterness',\n",
              " 'disappointment',\n",
              " 'poisoned',\n",
              " 'hard',\n",
              " 'work',\n",
              " 'found',\n",
              " 'tired',\n",
              " 'enough',\n",
              " 'at',\n",
              " 'went',\n",
              " 'simply',\n",
              " 'because',\n",
              " 'too',\n",
              " 'exhausted',\n",
              " 'stay',\n",
              " 'awake',\n",
              " 'Each',\n",
              " 'day',\n",
              " 'less',\n",
              " 'often',\n",
              " 'each',\n",
              " 'hurt',\n",
              " 'little',\n",
              " 'poignant',\n",
              " 'plenty',\n",
              " 'do',\n",
              " 'Because',\n",
              " 'summer',\n",
              " 'unusually',\n",
              " 'dry',\n",
              " 'hot',\n",
              " 'spring',\n",
              " 'produced',\n",
              " 'smaller',\n",
              " 'stream',\n",
              " 'than',\n",
              " 'ordinary',\n",
              " 'years',\n",
              " 'grass',\n",
              " 'meadows',\n",
              " 'came',\n",
              " 'fast',\n",
              " 'warm',\n",
              " 'weather',\n",
              " 'here',\n",
              " 'afford',\n",
              " 'lose',\n",
              " 'drop',\n",
              " 'precious',\n",
              " 'water',\n",
              " 'spent',\n",
              " 'most',\n",
              " 'his',\n",
              " 'waking',\n",
              " 'hours',\n",
              " 'along',\n",
              " 'no',\n",
              " 'idea',\n",
              " 'how',\n",
              " 'time',\n",
              " 'give',\n",
              " 'In',\n",
              " 'any',\n",
              " 'case',\n",
              " 'intention',\n",
              " 'being',\n",
              " 'caught',\n",
              " 'asleep',\n",
              " 'carried',\n",
              " 'revolver',\n",
              " 'its',\n",
              " 'holster',\n",
              " 'on',\n",
              " 'hip',\n",
              " 'took',\n",
              " 'Winchester',\n",
              " 'with',\n",
              " 'leaned',\n",
              " 'against',\n",
              " 'fence',\n",
              " 'stopped',\n",
              " 'every',\n",
              " 'few',\n",
              " 'minutes',\n",
              " 'shovel',\n",
              " 'studied',\n",
              " 'horizon',\n",
              " 'happened',\n",
              " 'dragging',\n",
              " 'monotonous',\n",
              " 'calm',\n",
              " 'When',\n",
              " 'late',\n",
              " 'afternoon',\n",
              " 'last',\n",
              " 'June',\n",
              " 'saw',\n",
              " 'two',\n",
              " 'people',\n",
              " 'top',\n",
              " 'ridge',\n",
              " 'south',\n",
              " 'walk',\n",
              " 'toward',\n",
              " 'house',\n",
              " 'quit',\n",
              " 'immediately',\n",
              " 'strode',\n",
              " 'rifle',\n",
              " 'It',\n",
              " 'some',\n",
              " 'kind',\n",
              " 'trick',\n",
              " 'thought',\n",
              " 'No',\n",
              " 'one',\n",
              " 'walked',\n",
              " 'least',\n",
              " 'Ed',\n",
              " 'Dow',\n",
              " 'or',\n",
              " 'Dutch',\n",
              " 'rest',\n",
              " 'Bar',\n",
              " 'B',\n",
              " 'crew',\n",
              " 'watched',\n",
              " 'figures',\n",
              " 'puzzled',\n",
              " 'they',\n",
              " 'were',\n",
              " 'closer',\n",
              " 'woman',\n",
              " 'more',\n",
              " 'ever',\n",
              " 'cleaned',\n",
              " 'left',\n",
              " 'picked',\n",
              " 'started',\n",
              " 'downstream',\n",
              " 'visitors',\n",
              " 'crawled',\n",
              " 'through',\n",
              " 'crossing',\n",
              " 'meadow',\n",
              " 'Now',\n",
              " 'both',\n",
              " 'man',\n",
              " 'moving',\n",
              " 'slowly',\n",
              " 'irregularly',\n",
              " 'staggering',\n",
              " 'struggle',\n",
              " 'remain',\n",
              " 'their',\n",
              " 'feet',\n",
              " 'Reaching',\n",
              " 'ahead',\n",
              " 'them',\n",
              " 'waited',\n",
              " 'hands',\n",
              " 'They',\n",
              " 'north',\n",
              " 'young',\n",
              " 'nineteen',\n",
              " 'twenty',\n",
              " 'dirty',\n",
              " 'clothes',\n",
              " 'torn',\n",
              " 'girl',\n",
              " 'she',\n",
              " 'fell',\n",
              " 'when',\n",
              " 'still',\n",
              " 'from',\n",
              " 'front',\n",
              " 'door',\n",
              " 'She',\n",
              " 'lay',\n",
              " 'making',\n",
              " 'effort',\n",
              " 'boy',\n",
              " 'porch',\n",
              " 'sat',\n",
              " 'down',\n",
              " 'gaze',\n",
              " 'half',\n",
              " 'expecting',\n",
              " 'shoot',\n",
              " 'really',\n",
              " 'caring',\n",
              " 'hesitated',\n",
              " 'good',\n",
              " 'think',\n",
              " 'possible',\n",
              " 'couple',\n",
              " 'pretending',\n",
              " 'licked',\n",
              " 'lips',\n",
              " 'asked',\n",
              " 'Could',\n",
              " 'we',\n",
              " 'drink',\n",
              " 'jerked',\n",
              " 'head',\n",
              " 'kitchen',\n",
              " 'said',\n",
              " 'Leaning',\n",
              " 'Get',\n",
              " \"There's\",\n",
              " 'move',\n",
              " 'say',\n",
              " 'anything',\n",
              " 'Her',\n",
              " 'eyes',\n",
              " 'glazed',\n",
              " 'hear',\n",
              " 'even',\n",
              " 'see',\n",
              " 'reached',\n",
              " 'point',\n",
              " 'which',\n",
              " 'care',\n",
              " 'looked',\n",
              " 'face',\n",
              " 'very',\n",
              " 'thin',\n",
              " 'burned',\n",
              " 'by',\n",
              " 'sun',\n",
              " 'until',\n",
              " 'skin',\n",
              " 'dead',\n",
              " 'peeling',\n",
              " 'new',\n",
              " 'under',\n",
              " 'red',\n",
              " 'angry',\n",
              " 'blond',\n",
              " 'hair',\n",
              " 'dress',\n",
              " 'several',\n",
              " 'places',\n",
              " 'shoes',\n",
              " 'completely',\n",
              " 'worn',\n",
              " 'practically',\n",
              " 'protection',\n",
              " 'must',\n",
              " 'sole',\n",
              " 'off',\n",
              " 'foot',\n",
              " 'bruised',\n",
              " 'bleeding',\n",
              " 'sliding',\n",
              " 'hand',\n",
              " 'shoulders',\n",
              " 'other',\n",
              " 'knees',\n",
              " 'into',\n",
              " 'amazingly',\n",
              " 'light',\n",
              " 'relaxed',\n",
              " 'arms',\n",
              " \"wasn't\",\n",
              " 'sure',\n",
              " 'conscious',\n",
              " 'Any',\n",
              " 'lingering',\n",
              " 'suspicion',\n",
              " 'dispelled',\n",
              " 'go',\n",
              " 'far',\n",
              " 'fool',\n",
              " 'kill',\n",
              " 'Besides',\n",
              " 'sweet',\n",
              " 'attracted',\n",
              " 'put',\n",
              " 'couch',\n",
              " 'going',\n",
              " 'dropped',\n",
              " 'chair',\n",
              " 'beside',\n",
              " 'table',\n",
              " 'deal',\n",
              " 'alike',\n",
              " 'Both',\n",
              " 'blonde',\n",
              " 'blue',\n",
              " 'faint',\n",
              " 'similarity',\n",
              " 'features',\n",
              " 'filled',\n",
              " 'dipper',\n",
              " 'bucket',\n",
              " 'shelf',\n",
              " 'room',\n",
              " 'lifted',\n",
              " \"girl's\",\n",
              " 'held',\n",
              " 'edge',\n",
              " 'mouth',\n",
              " 'drank',\n",
              " 'murmured',\n",
              " 'Thank',\n",
              " 'you',\n",
              " 'lowered',\n",
              " 'stood',\n",
              " 'looking',\n",
              " 'moment',\n",
              " 'wondering',\n",
              " 'what',\n",
              " 'reduced',\n",
              " 'condition',\n",
              " 'seen',\n",
              " 'wagons',\n",
              " 'families',\n",
              " 'almost',\n",
              " 'starving',\n",
              " 'death',\n",
              " 'never',\n",
              " 'bad',\n",
              " 'these',\n",
              " 'returned',\n",
              " 'built',\n",
              " 'fire',\n",
              " 'buckets',\n",
              " 'poured',\n",
              " 'copper',\n",
              " 'placed',\n",
              " 'stove',\n",
              " 'brought',\n",
              " 'faced',\n",
              " 'Who',\n",
              " 'are',\n",
              " \"I'm\",\n",
              " 'Billy',\n",
              " 'Jones',\n",
              " 'answered',\n",
              " \"That's\",\n",
              " 'my',\n",
              " 'Sharon',\n",
              " 'We',\n",
              " 'ran',\n",
              " 'money',\n",
              " \"haven't\",\n",
              " 'eaten',\n",
              " 'days',\n",
              " 'What',\n",
              " 'doing',\n",
              " 'Are',\n",
              " 'Wyoming',\n",
              " 'nodded',\n",
              " 'About',\n",
              " 'five',\n",
              " 'miles',\n",
              " 'line',\n",
              " 'sighed',\n",
              " 'relieved',\n",
              " \"We've\",\n",
              " 'ranchers',\n",
              " 'turned',\n",
              " 'us',\n",
              " 'You',\n",
              " 'mean',\n",
              " 'dragged',\n",
              " 'your',\n",
              " 'over',\n",
              " 'demanded',\n",
              " 'town',\n",
              " 'only',\n",
              " 'about',\n",
              " 'six',\n",
              " 'Why',\n",
              " 'This',\n",
              " 'is',\n",
              " 'mighty',\n",
              " 'empty',\n",
              " 'ranch',\n",
              " 'three',\n",
              " \"You'd\",\n",
              " 'starved',\n",
              " \"you'd\",\n",
              " 'missed',\n",
              " 'Then',\n",
              " \"we're\",\n",
              " 'lucky',\n",
              " 'got',\n",
              " 'job',\n",
              " 'Mr.',\n",
              " 'silent',\n",
              " 'use',\n",
              " 'year',\n",
              " 'cook',\n",
              " 'knew',\n",
              " 'might',\n",
              " 'dismissed',\n",
              " 'possibility',\n",
              " 'once',\n",
              " 'haunted',\n",
              " 'killer',\n",
              " \"hadn't\",\n",
              " 'shaved',\n",
              " 'weeks',\n",
              " 'sparse',\n",
              " 'beard',\n",
              " 'giving',\n",
              " 'pathetic',\n",
              " 'expression',\n",
              " 'There',\n",
              " 'running',\n",
              " 'something',\n",
              " \"He'd\",\n",
              " 'an',\n",
              " 'let',\n",
              " \"couldn't\",\n",
              " 'send',\n",
              " 'either',\n",
              " 'I',\n",
              " 'help',\n",
              " 'finally',\n",
              " \"can't\",\n",
              " 'pay',\n",
              " 'guess',\n",
              " 'better',\n",
              " 'morning',\n",
              " \"We'll\",\n",
              " 'our',\n",
              " 'keep',\n",
              " 'eagerly',\n",
              " \"I've\",\n",
              " 'mine',\n",
              " 'San',\n",
              " 'Juan',\n",
              " 'used',\n",
              " \"she's\",\n",
              " 'cooked',\n",
              " 'restaurant',\n",
              " \"I'll\",\n",
              " 'Right',\n",
              " 'need',\n",
              " 'meal',\n",
              " 'bath',\n",
              " 'Your',\n",
              " \"wife's\",\n",
              " 'terrible',\n",
              " 'shape',\n",
              " 'know',\n",
              " 'box',\n",
              " 'wood',\n",
              " 'again',\n",
              " 'supper',\n",
              " 'set',\n",
              " 'ready',\n",
              " 'wash',\n",
              " 'rubbed',\n",
              " 'stretched',\n",
              " 'mess',\n",
              " 'suddenly',\n",
              " 'alarmed',\n",
              " 'How',\n",
              " 'did',\n",
              " 'Aj',\n",
              " 'gave',\n",
              " 'Oh',\n",
              " 'stared',\n",
              " 'wide',\n",
              " \"You're\",\n",
              " 'Do',\n",
              " 'take',\n",
              " 'strays',\n",
              " 'come',\n",
              " \"don't\",\n",
              " 'many',\n",
              " 'coming',\n",
              " 'Think',\n",
              " 'can',\n",
              " 'Of',\n",
              " 'course',\n",
              " 'staggered',\n",
              " 'arm',\n",
              " 'helped',\n",
              " 'shaking',\n",
              " 'sorry',\n",
              " 'usually',\n",
              " 'strong',\n",
              " 'awfully',\n",
              " 'And',\n",
              " 'hungry',\n",
              " 'Start',\n",
              " \"It's\",\n",
              " \"it's\",\n",
              " 'eat',\n",
              " 'Not',\n",
              " 'cried',\n",
              " 'food',\n",
              " 'satisfied',\n",
              " \"he's\",\n",
              " 'grateful',\n",
              " 'way',\n",
              " 'made',\n",
              " 'ashamed',\n",
              " 'Lord',\n",
              " 'looks',\n",
              " 'fools',\n",
              " 'drunkards',\n",
              " 'laughed',\n",
              " 'Which',\n",
              " \"We're\",\n",
              " 'dishes',\n",
              " 'before',\n",
              " 'dark',\n",
              " 'tub',\n",
              " 'where',\n",
              " 'hung',\n",
              " 'nail',\n",
              " 'wall',\n",
              " \"You'll\",\n",
              " 'feel',\n",
              " 'lot',\n",
              " 'after',\n",
              " 'Mrs.',\n",
              " \"she'll\",\n",
              " 'right',\n",
              " 'quickly',\n",
              " 'expect',\n",
              " 'just',\n",
              " 'find',\n",
              " 'me',\n",
              " 'needle',\n",
              " 'thread',\n",
              " 'My',\n",
              " 'needs',\n",
              " 'bedroom',\n",
              " 'bed',\n",
              " 'spare',\n",
              " \"isn't\",\n",
              " \"you'll\",\n",
              " 'blankets',\n",
              " 'Some',\n",
              " 'early',\n",
              " 'followed',\n",
              " 'closing',\n",
              " 'behind',\n",
              " 'slept',\n",
              " 'together',\n",
              " 'since',\n",
              " 'chances',\n",
              " 'getting',\n",
              " 'pregnant',\n",
              " 'sleeping',\n",
              " 'embarrassed',\n",
              " 'understand',\n",
              " 'why',\n",
              " 'jobs',\n",
              " 'first',\n",
              " 'place',\n",
              " 'fired',\n",
              " 'pair',\n",
              " 'lost',\n",
              " 'whipped',\n",
              " 'kids',\n",
              " 'Gavin',\n",
              " 'paused',\n",
              " 'wearily',\n",
              " \"they'd\",\n",
              " 'dawn',\n",
              " 'make',\n",
              " 'sank',\n",
              " 'began',\n",
              " 'rock',\n",
              " 'life',\n",
              " 'guilt',\n",
              " 'Beneath',\n",
              " 'black',\n",
              " 'shirt',\n",
              " 'frail',\n",
              " 'shook',\n",
              " 'pain',\n",
              " 'broke',\n",
              " 'throat',\n",
              " 'stored',\n",
              " 'shattering',\n",
              " 'free',\n",
              " 'slow',\n",
              " 'gasps',\n",
              " 'Clayton',\n",
              " 'tried',\n",
              " 'call',\n",
              " 'known',\n",
              " 'Against',\n",
              " 'rally',\n",
              " 'anger',\n",
              " 'bent',\n",
              " 'powerless',\n",
              " \"Gavin's\",\n",
              " 'moved',\n",
              " 'stoop',\n",
              " 'catch',\n",
              " 'words',\n",
              " 'remember',\n",
              " 'Big',\n",
              " 'Charlie',\n",
              " 'whispered',\n",
              " 'stuck',\n",
              " 'Just',\n",
              " 'half-breed',\n",
              " 'meant',\n",
              " 'fight',\n",
              " 'Tom',\n",
              " 'English',\n",
              " \"brother's\",\n",
              " 'son',\n",
              " 'fair',\n",
              " 'provoked',\n",
              " 'believed',\n",
              " 'killed',\n",
              " 'dumped',\n",
              " 'body',\n",
              " 'rose',\n",
              " 'garden',\n",
              " 'nights',\n",
              " 'ago',\n",
              " 'men',\n",
              " 'cleared',\n",
              " 'Clay',\n",
              " 'treated',\n",
              " 'wiped',\n",
              " 'sleeve',\n",
              " 'childish',\n",
              " 'wonder',\n",
              " 'shyly',\n",
              " 'wherever',\n",
              " \"you're\",\n",
              " 'goin',\n",
              " 'Yes',\n",
              " 'hate',\n",
              " 'choked',\n",
              " 'murmuring',\n",
              " 'Come',\n",
              " 'old',\n",
              " 'beckoned',\n",
              " 'finger',\n",
              " 'forward',\n",
              " 'slipped',\n",
              " 'chest',\n",
              " 'fiercely',\n",
              " 'All',\n",
              " 'away',\n",
              " 'wanted',\n",
              " 'part',\n",
              " \"there's\",\n",
              " 'nothin',\n",
              " 'gone',\n",
              " 'God',\n",
              " 'Heaven',\n",
              " 'refuse',\n",
              " 'That',\n",
              " 'mock',\n",
              " \"Can't\",\n",
              " 'closed',\n",
              " 'tears',\n",
              " 'freed',\n",
              " 'embrace',\n",
              " 'stepped',\n",
              " 'fearfully',\n",
              " 'horses',\n",
              " 'saddle',\n",
              " 'bring',\n",
              " 'round',\n",
              " 'burst',\n",
              " 'confinement',\n",
              " 'cold',\n",
              " 'air',\n",
              " 'stallion',\n",
              " 'barn',\n",
              " 'tightened',\n",
              " 'blanket',\n",
              " 'working',\n",
              " 'touch',\n",
              " 'darkness',\n",
              " 'comforting',\n",
              " 'animal',\n",
              " 'easy',\n",
              " 'finished',\n",
              " 'led',\n",
              " 'mare',\n",
              " 'smelled',\n",
              " 'heat',\n",
              " 'paw',\n",
              " 'turf',\n",
              " 'reins',\n",
              " 'knot',\n",
              " 'veranda',\n",
              " 'post',\n",
              " 'patted',\n",
              " 'flesh',\n",
              " 'neck',\n",
              " 'backed',\n",
              " \"doesn't\",\n",
              " 'will',\n",
              " 'figure',\n",
              " 'taken',\n",
              " 'carbine',\n",
              " 'trailed',\n",
              " 'stock',\n",
              " 'floor',\n",
              " 'called',\n",
              " 'steps',\n",
              " 'To',\n",
              " 'valley',\n",
              " 'someone',\n",
              " 'may',\n",
              " 'California',\n",
              " 'yet',\n",
              " 'crazy',\n",
              " 'nod',\n",
              " 'land',\n",
              " 'A',\n",
              " 'mark',\n",
              " 'Two',\n",
              " 'like',\n",
              " 'fine',\n",
              " 'maybe',\n",
              " \"one's\",\n",
              " 'fresh',\n",
              " 'father',\n",
              " 'stand',\n",
              " 'approached',\n",
              " 'horse',\n",
              " 'laid',\n",
              " 'quivering',\n",
              " 'Help',\n",
              " 'stiff',\n",
              " 'gently',\n",
              " 'child',\n",
              " \"They'll\",\n",
              " 'trample',\n",
              " 'loved',\n",
              " 'grow',\n",
              " 'alone',\n",
              " 'river',\n",
              " 'nice',\n",
              " 'peaceful',\n",
              " 'quiet',\n",
              " 'swung',\n",
              " 'yard',\n",
              " 'circle',\n",
              " 'cast',\n",
              " 'lamp',\n",
              " 'burning',\n",
              " 'Thirty-five',\n",
              " 'rode',\n",
              " 'measured',\n",
              " 'pace',\n",
              " 'Dawn',\n",
              " 'soon',\n",
              " 'coldest',\n",
              " 'moon',\n",
              " 'sunk',\n",
              " 'below',\n",
              " 'crest',\n",
              " 'mountains',\n",
              " 'grown',\n",
              " 'accustomed',\n",
              " 'absence',\n",
              " 'primeval',\n",
              " 'trespassed',\n",
              " 'those',\n",
              " 'rustle',\n",
              " 'blade',\n",
              " 'wind',\n",
              " 'savage',\n",
              " 'brooding',\n",
              " 'broken',\n",
              " 'bitterly',\n",
              " 'inert',\n",
              " 'landscape',\n",
              " 'caravan',\n",
              " 'desires',\n",
              " 'passed',\n",
              " 'mind',\n",
              " 'strewn',\n",
              " 'dying',\n",
              " 'vain',\n",
              " 'groped',\n",
              " 'reassemble',\n",
              " 'bones',\n",
              " 'relationships',\n",
              " 'sought',\n",
              " 'desperately',\n",
              " 'silence',\n",
              " 'oppressed',\n",
              " 'bend',\n",
              " 'low',\n",
              " \"horse's\",\n",
              " 'hide',\n",
              " 'begun',\n",
              " 'blow',\n",
              " 'twisting',\n",
              " 'search',\n",
              " 'framed',\n",
              " 'gray',\n",
              " 'hills',\n",
              " 'dissolve',\n",
              " 'bold',\n",
              " 'loose',\n",
              " 'high',\n",
              " 'feathers',\n",
              " 'swept',\n",
              " 'stars',\n",
              " 'sky',\n",
              " 'spread',\n",
              " 'ground',\n",
              " 'revealed',\n",
              " 'glimmer',\n",
              " 'contours',\n",
              " 'trees',\n",
              " 'fences',\n",
              " 'shadowed',\n",
              " 'weary',\n",
              " 'though',\n",
              " 'posted',\n",
              " 'hundred',\n",
              " 'yards',\n",
              " 'shelter',\n",
              " 'flung',\n",
              " 'themselves',\n",
              " 'saloon',\n",
              " 'crying',\n",
              " 'intelligence',\n",
              " \"night's\",\n",
              " 'drinking',\n",
              " 'faces',\n",
              " 'baggy',\n",
              " 'liquor',\n",
              " 'flushed',\n",
              " 'courage',\n",
              " 'greeted',\n",
              " 'news',\n",
              " 'angrily',\n",
              " 'cheated',\n",
              " 'purpose',\n",
              " 'Lester',\n",
              " 'heard',\n",
              " 'muttering',\n",
              " 'reveal',\n",
              " 'desire',\n",
              " 'worked',\n",
              " 'tongue',\n",
              " 'hollow',\n",
              " 'cheek',\n",
              " 'voice',\n",
              " 'cracked',\n",
              " \"He's\",\n",
              " 'Keep',\n",
              " 'Purvis',\n",
              " 'snarled',\n",
              " 'brother',\n",
              " 'lied',\n",
              " 'Joe',\n",
              " 'First',\n",
              " 'ridden',\n",
              " 'cattle',\n",
              " 'end',\n",
              " 'remembered',\n",
              " 'smirk',\n",
              " 'own',\n",
              " 'cringing',\n",
              " 'feeling',\n",
              " \"Clayton's\",\n",
              " 'fallen',\n",
              " 'Gap',\n",
              " 'Nellie',\n",
              " 'love',\n",
              " 'Roy',\n",
              " 'dance',\n",
              " 'party',\n",
              " 'treats',\n",
              " 'dirt',\n",
              " 'mocking',\n",
              " 'smile',\n",
              " 'poker',\n",
              " 'game',\n",
              " 'thinkin',\n",
              " 'halfway',\n",
              " \"won't\",\n",
              " \"they'll\",\n",
              " 'ride',\n",
              " \"that's\",\n",
              " 'shoulder',\n",
              " 'drunk',\n",
              " 'wild',\n",
              " 'break',\n",
              " 'cloying',\n",
              " 'warmth',\n",
              " 'fled',\n",
              " 'grunted',\n",
              " 'pushing',\n",
              " 'side',\n",
              " 'jacket',\n",
              " 'raised',\n",
              " 'shut',\n",
              " 'mounted',\n",
              " 'others',\n",
              " 'safe',\n",
              " 'distance',\n",
              " 'mist',\n",
              " 'swirled',\n",
              " 'sudden',\n",
              " 'clear',\n",
              " 'clouds',\n",
              " 'parted',\n",
              " 'sunlight',\n",
              " 'swooped',\n",
              " 'stain',\n",
              " 'earth',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HQCbyzm06J2"
      },
      "source": [
        "## Bengali\n",
        "\n",
        "- Bengali is written using unicode.\n",
        "- I use language specific stopwords for Bengali from [this](https://github.com/stopwords-iso/stopwords-bn/blob/master/stopwords-bn.txt) source.\n",
        "- The tasks mentioned in the assignment are executed consecutively in separate cells to show the output for each."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbUIv76I096u"
      },
      "source": [
        "stopwords_bn = set(open(data_dir+bengali_stopwords).read().split())"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61BfdtDD1Bhm"
      },
      "source": [
        "bengali_solution = Basics(data_dir+data_file_2, 'bn', stopwords_bn)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjJ-VTzk1ENs",
        "outputId": "5932467e-3dc1-42f5-c3aa-ff9c21710bce"
      },
      "source": [
        "bengali_solution.add_line_numbers()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 \t я╗┐ржПржХ ржЯрж╛ржХрж╛ ржмрж╛ржбрж╝рж▓рзЗржЗ рж░рж╛ржЬрж╕рзНржерж╛ржирзЗ рж╕рзЗржЮрзНржЪрзБрж░рж┐ рж╣рж╛ржБржХрж╛ржмрзЗ ржкрзЗржЯрзНрж░рзЛрж▓рзЗрж░ ржжрж╛ржоред ржбрж┐ржЬрзЗрж▓рзЗрж░ ржжрж╛ржоржУ рзпрзж-ржПрж░ ржХрзЛржЯрж╛ ржкрж╛рж░ ржХрж░рзЗржЫрзЗ рж╕рзЗржЦрж╛ржирзЗред ржпрж╛ ржжрж╛ржорзЗрж░ ржирж┐рж░рж┐ржЦрзЗ ржжрзЗрж╢рзЗрж░ ржоржзрзНржпрзЗ рж╕ржмржЪрзЗржпрж╝рзЗ ржмрзЗрж╢рж┐ред\n",
            "2 \t ржжрж╛ржо ржмрзГржжрзНржзрж┐рж░ ржХрж╛рж░ржг рж╣рж┐рж╕рзЗржмрзЗ ржмрж┐рж╢рзНржм ржмрж╛ржЬрж╛рж░рзЗ ржЕрж╢рзЛржзрж┐ржд рждрзЗрж▓ ржПржмржВ ржжрзБржЗ ржЬрзНржмрж╛рж▓рж╛ржирж┐рж░ ржжрж░рзЗрж░ рж╣рж┐рж╕рзЗржмржХрзЗ ржжрж╛ржпрж╝рзА ржХрж░рзЗржЫрзЗ ржХрзЗржирзНржжрзНрж░ред рждржмрзЗ ржмрж┐рж░рзЛржзрзАржжрзЗрж░ ржЕржнрж┐ржпрзЛржЧ, ржкрзЗржЯрзНрж░рзЛрж▓ ржПржмржВ ржбрж┐ржЬрзЗрж▓рзЗрж░ ржЙржкрж░ ржХрзЗржирзНржжрзНрж░рзЗрж░ рж╢рзБрж▓рзНржХ ржмрзГржжрзНржзрж┐рж░ ржХрж╛рж░ржгрзЗржЗ ржХрзНрж░рзЗрждрж╛ржжрзЗрж░ ржмрж╛ржбрж╝рждрж┐ ржХржбрж╝рж┐ ржЧрзБржирждрзЗ рж╣ржЪрзНржЫрзЗред ржХржВржЧрзНрж░рзЗрж╕ рж╕рж╛ржВрж╕ржж ржЕржзрзАрж░рж░ржЮрзНржЬржи ржЪрзМржзрзБрж░рзА рждрзЗрж▓рзЗрж░ ржжрж╛ржо ржмрзГржжрзНржзрж┐ ржирж┐ржпрж╝рзЗ ржХрзЗржирзНржжрзНрж░ржХрзЗ ржХржЯрж╛ржХрзНрж╖ ржХрж░рзЗ ржЯрзБржЗржЯ ржХрж░рзЗржи, тАШржЖрждрзНржоржирж┐рж░рзНржнрж░ ржнрж╛рж░ржд ржмрзГржжрзНржзрж┐рж░ ржЖрж░ржУ ржПржХржЯрж╛ рж╢рж┐ржЦрж░рзЗ ржкрзМржБржЫрждрзЗ ржЪрж▓рзЗржЫрзЗред рж╕рзЗржЮрзНржЪрзБрж░рж┐ ржерзЗржХрзЗ ржкрж╛ржБржЪ ржкржпрж╝рзЗржирзНржЯ ржжрзВрж░рзЗ рж░ржпрж╝рзЗржЫрзЗ ржкрзЗржЯрзНрж░рзЛрж▓рзЗрж░ ржжрж╛ржоред ржЦрзБржм рж╢рзАржШрзНрж░ржЗ ржирж░рзЗржирзНржжрзНрж░ ржорзЛржжрзАржЬрж┐ ржкрзЗржЯрзНрж░рзЛрж▓рзЗрж░ ржжрж╛ржо рззрзжрзж ржЯрж╛ржХрж╛ ржХрж░ржмрзЗржиредтАЩ\n",
            "3 \t рж░ржмрж┐ржмрж╛рж░ ржЧрзЛржЯрж╛ ржжрзЗрж╢рзЗ рж▓рж┐ржЯрж╛рж░ржкрж┐ржЫрзБ рзирзп ржкржпрж╝рж╕рж╛ ржмрзЗржбрж╝рзЗржЫрзЗ ржкрзЗржЯрзНрж░рзЛрж▓рзЗрж░ ржжрж╛ржоред рж▓рж┐ржЯрж╛рж░ржкрж┐ржЫрзБ ржбрж┐ржЬрзЗрж▓рзЗрж░ ржжрж╛ржо ржмрзЗржбрж╝рзЗржЫрзЗ рзйрзи ржкржпрж╝рж╕рж╛ред ржкрж░ ржкрж░ ржЯрж╛ржирж╛ рзм ржжрж┐ржи ржмрж╛ржбрж╝рж▓ ржПржЗ ржЬрзАржмрж╛рж╢рзНржо ржЬрзНржмрж╛рж▓рж╛ржирж┐рж░ ржжрж╛ржоред ржП ржХтАЩржжрж┐ржирзЗ рж▓рж┐ржЯрж╛рж░ржкрж┐ржЫрзБ ржкрзЗржЯрзНрж░рзЛрж▓рзЗрж░ ржорзЛржЯ ржжрж╛ржо ржмрзЗржбрж╝рзЗржЫрзЗ рзз.рзорзж ржЯрж╛ржХрж╛ ржПржмржВ ржбрж┐ржЬрзЗрж▓рзЗрж░ рзз.рзорзо ржЯрж╛ржХрж╛ред\n",
            "4 \t ржжрж╛ржо ржмрзГржжрзНржзрж┐рж░ ржирж┐рж░рж┐ржЦрзЗ рж░рж╛ржЬрж╕рзНржерж╛ржирзЗрж░ ржкрж░рзЗржЗ рж░ржпрж╝рзЗржЫрзЗ ржорзБржорзНржмржЗред рж╕рзЗржЦрж╛ржирзЗ ржкрзЗржЯрзНрж░рзЛрж▓рзЗрж░ ржжрж╛ржо рзпрзл ржЯрж╛ржХрж╛ ржЫрж╛ржбрж╝рж┐ржпрж╝рзЗ ржЧрж┐ржпрж╝рзЗржЫрзЗред ржбрж┐ржЬрзЗрж▓ рзорзм.рзжрзк ржЯрж╛ржХрж╛ред ржжрзЗрж╢рзЗрж░ ржЪрж╛рж░ ржорзЗржЯрзНрж░рзЛ рж╢рж╣рж░рзЗрж░ ржоржзрзНржпрзЗ ржорзБржорзНржмржЗрждрзЗржЗ ржПржЗ ржжрзБржЗ ржЬрзНржмрж╛рж▓рж╛ржирж┐рж░ ржжрж╛ржо рж╕ржмржЪрзЗржпрж╝рзЗ ржмрзЗрж╢рж┐ред ржжрж┐рж▓рзНрж▓рж┐рждрзЗ ржкрзЗржЯрзНрж░рзЛрж▓ ржУ ржбрж┐ржЬрзЗрж▓рзЗрж░ ржжрж╛ржо ржпржерж╛ржХрзНрж░ржорзЗ ржжрж╛ржо рзорзо.рзнрзй ржПржмржВ рзнрзп.рзжрзм ржЯрж╛ржХрж╛ред ржЪрзЗржирзНржирж╛ржЗржпрж╝рзЗржУ ржкрзЗржЯрзНрж░рзЛрж▓ рзпрзж-ржПрж░ ржЧржирзНржбрж┐ ржЯржкржХрзЗржЫрзЗред ржПржХржЗ ржЫржмрж┐ ржХрж▓ржХрж╛рждрж╛рж░ржУред ржПржЦрж╛ржирзЗ рж▓рж┐ржЯрж╛рж░ржкрж┐ржЫрзБ ржкрзЗржЯрзНрж░рзЛрж▓рзЗрж░ ржжрж╛ржо рзпрзж.рзжрзз ржЯрж╛ржХрж╛ред ржбрж┐ржЬрзЗрж▓ рзорзи.рзмрзл ржЯрж╛ржХрж╛ред\n",
            "5 \t ржжрзАрж░рзНржШ ржПржХ ржорж╛рж╕ ржжрж╛ржорзЗрж░ ржХрзЛржиржУ рж╣рзЗрж░ржлрзЗрж░ ржирж╛ рж╣ржУржпрж╝рж╛рж░ ржкрж░ ржЧржд рзм ржЬрж╛ржирзБржпрж╝рж╛рж░рж┐ ржерзЗржХрзЗ ржкрзНрж░рждрж┐ ржжрж┐ржиржЗ ржПржХржЯрзБ ржПржХрзБржЯ рждрзЗрж▓рзЗрж░ ржжрж╛ржо ржмрж╛ржбрж╝рждрзЗ рж╢рзБрж░рзБ ржХрж░рзЗред ржП ржнрж╛ржмрзЗ ржмрж╛ржбрж╝рждрзЗ ржерж╛ржХрж▓рзЗ ржЦрзБржм рж╢рзАржШрзНрж░ржЗ ржорзЗржЯрзНрж░рзЛ рж╢рж╣рж░ржЧрзБрж▓рзЛрждрзЗржУ ржкрзЗржЯрзНрж░рзЛрж▓ ржПржмржВ ржбрж┐ржЬрзЗрж▓рзЗрж░ ржжрж╛ржо рж╕рзЗржЮрзНржЪрзБрж░рж┐ рж╣рж╛ржБржХрж╛ржмрзЗред\n",
            "6 \t ржжрзЗрж╢рзЗрж░ ржорзЗржЯрзНрж░рзЛ рж╢рж╣рж░ржЧрзБрж▓рж┐рждрзЗ ржкрзЗржЯрзНрж░рж▓-ржбрж┐ржЬрзЗрж▓рзЗрж░ ржжрж╛ржо ржлрзЗрж░ ржКрж░рзНржзрзНржмржорзБржЦрзАред ржПржЗ ржирж┐ржпрж╝рзЗ ржЯрж╛ржирж╛ ржЪрж╛рж░ ржжрж┐ржи ржХрж▓ржХрж╛рждрж╛-рж╕рж╣ ржУржЗ рж╢рж╣рж░ржЧрзБрж▓рж┐рждрзЗ ржЬрзНржмрж╛рж▓рж╛ржирзАрж░ ржжрж╛ржо ржмрж╛ржбрж╝рж▓ред рж╢рзБржХрзНрж░ржмрж╛рж░ ржПрж░ ржжрж╛ржо ржПржЦржиржУ ржкрж░рзНржпржирзНржд рж╕ржмржЪрзЗржпрж╝рзЗ ржмрзЗрж╢рж┐ рж╣ржпрж╝рзЗржЫрзЗред\n",
            "7 \t рж╢рзБржХрзНрж░ржмрж╛рж░ ржжрзЗрж╢рзЗрж░ рждрзЗрж▓ ржмрж┐ржХрзНрж░ржпрж╝ржХрж╛рж░рзА рж╕ржВрж╕рзНржерж╛ржЧрзБрж▓рж┐ ржЬрзНржмрж╛рж▓рж╛ржирзАрж░ ржжрж╛ржо ржкрзНрж░рждрж┐ рж▓рж┐ржЯрж╛рж░рзЗ ржкрзНрж░рж╛ржпрж╝ рзйрзж ржкржпрж╝рж╕рж╛ ржХрж░рзЗ ржмрж╛ржбрж╝рж┐ржпрж╝рзЗржЫрзЗред ржлрж▓рзЗ рж╢рзБржХрзНрж░ржмрж╛рж░ рж╕ржХрж╛рж▓ ржерзЗржХрзЗ ржХрж▓ржХрж╛рждрж╛ржпрж╝ ржкрзЗржЯрзНрж░рж▓рзЗрж░ ржжрж╛ржо рж▓рж┐ржЯрж╛рж░ ржкрзНрж░рждрж┐ рзорзп.рзкрзк ржЯрж╛ржХрж╛ред ржЕржирзНржп ржжрж┐ржХрзЗ, ржбрж┐ржЬрзЗрж▓рзЗрж░ ржжрж╛ржо ржмрзЗржбрж╝рзЗ рж╣ржпрж╝рзЗржЫрзЗ ржкрзНрж░рждрж┐ рж▓рж┐ржЯрж╛рж░рзЗ рзорзз.рзпрзм ржЯрж╛ржХрж╛ред ржжрж┐рж▓рзНрж▓рж┐рждрзЗ рзирзп ржкржпрж╝рж╕рж╛ ржмрзЗржбрж╝рзЗ рзз рж▓рж┐ржЯрж╛рж░ ржкрзЗржЯрзНрж░рж▓рзЗрж░ ржжрж╛ржо ржжрж╛ржБржбрж╝рж┐ржпрж╝рзЗржЫрзЗ рзорзо.рззрзк ржЯрж╛ржХрж╛ред рж░рж╛ржЬржзрж╛ржирзАрждрзЗ ржбрж┐ржЬрзЗрж▓рзЗрж░ ржжрж╛ржо ржмрзЗржбрж╝рзЗржЫрзЗ рзйрзл ржкржпрж╝рж╕рж╛ред рж╢рзБржХрзНрж░ржмрж╛рж░ рждрж╛ ржмрж┐ржХрзНрж░рж┐ рж╣ржЪрзНржЫрзЗ рж▓рж┐ржЯрж╛рж░ ржкрзНрж░рждрж┐ рзнрзо.рзйрзо ржЯрж╛ржХрж╛ржпрж╝ред ржорзБржорзНржмржЗрждрзЗ рзз рж▓рж┐ржЯрж╛рж░ ржкрзЗржЯрзНрж░рж▓рзЗрж░ ржжрж╛ржо рж╣ржпрж╝рзЗржЫрзЗ рзпрзк.рзмрзк ржЯрж╛ржХрж╛ ржПржмржВ ржУржЗ ржПржХржЗ ржкрж░рж┐ржорж╛ржг ржбрж┐ржЬрзЗрж▓ ржХрж┐ржирждрзЗ ржЦрж░ржЪ ржХрж░рждрзЗ рж╣ржЪрзНржЫрзЗ рзорзл.рзйрзи ржЯрж╛ржХрж╛ред ржПржЗ рждрж┐ржи рж╢рж╣рж░рзЗрж░ ржкрж╛рж╢рж╛ржкрж╛рж╢рж┐ ржжрзЗрж╢рзЗрж░ ржЕржирзНржп ржорзЗржЯрзНрж░рзЛ рж╢рж╣рж░ ржЪрзЗржирзНржирж╛ржЗрждрзЗ ржкрзНрж░рждрж┐ рж▓рж┐ржЯрж╛рж░ ржкрзЗржЯрзНрж░рж▓ ржПржмржВ ржбрж┐ржЬрзЗрж▓рзЗрж░ ржжрж╛ржо ржпржерж╛ржХрзНрж░ржорзЗ рзпрзж.рзкрзк ржУ рзорзй.рзлрзи ржЯрж╛ржХрж╛ред\n",
            "8 \t ржжрзЗрж╢рзЗрж░ рж╕ржмржЪрзЗржпрж╝рзЗ ржмржбрж╝ рждрзЗрж▓ ржмрж┐ржХрзНрж░ржпрж╝ржХрж╛рж░рзА рж╕ржВрж╕рзНржерж╛ ржЗржирзНржбрж┐ржпрж╝рж╛ржи ржЕржпрж╝рзЗрж▓ ржХрж░рзНржкрзЛрж░рзЗрж╢ржи ржЬрж╛ржирж┐ржпрж╝рзЗржЫрзЗ, ржЪрж▓рждрж┐ ржмржЫрж░рзЗрж░ рзм ржЬрж╛ржирзБржпрж╝рж╛рж░рж┐ ржерзЗржХрзЗ ржП ржжрзЗрж╢рзЗ рждрзЗрж▓рзЗрж░ ржжрж╛ржо ржКрж░рзНржзрзНржмржорзБржЦрзА рж╣рждрзЗ рж╢рзБрж░рзБ ржХрж░рзЗржЫрзЗред рждрж╛рж░ ржЖржЧрзЗ ржорж╛рж╕ржЦрж╛ржирзЗржХ рждрзЗрж▓рзЗрж░ ржжрж░рзЗ ржХрзЛржиржУ рж╣рзЗрж░ржлрзЗрж░ рж╣ржпрж╝ржирж┐ред ржмрж┐рж╢рзНржм ржЬрзБржбрж╝рзЗ ржЕржкрж░рж┐рж╢рзЛржзрж┐ржд рждрзЗрж▓рзЗрж░ ржжрж╛ржо ржмрж╛ржбрж╝рж╛рж░ ржЬржирзНржп ржП ржжрзЗрж╢рзЗрж░ ржЬрзНржмрж╛рж▓рж╛ржирзАрж░ ржжрж╛ржо ржмрзЗржбрж╝рзЗржЫрзЗред ржкрж╛рж╢рж╛ржкрж╛рж╢рж┐, ржмрж┐ржжрзЗрж╢рж┐ ржорзБржжрзНрж░рж╛ ржмрж┐ржирж┐ржоржпрж╝ржорзВрж▓рзНржпрзЗ ржУржарж╛ржирж╛ржорж╛рж░ ржЙржкрж░ ржирж┐рж░рзНржнрж░ ржХрж░рзЗ рждрзЗрж▓рзЗрж░ ржжрж╛ржоред\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UE-5Ia061F0b",
        "outputId": "ef637501-e943-4751-c472-b82eee4c58c0"
      },
      "source": [
        "print('Vocabulary size : ', bengali_solution.vocab_size())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size :  234\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvt4rEqH1K3X",
        "outputId": "24534970-df7f-4b34-988f-8bea43c53264"
      },
      "source": [
        "word = 'ржЯрж╛ржХрж╛'\n",
        "print('Percentage of \\\"', word, '\\\" is : ', bengali_solution.percent(word), '%.')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of \" ржЯрж╛ржХрж╛ \" is :  1.2345679012345678 %.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SV1u1zI1Lc2",
        "outputId": "e743b34c-c885-428e-c352-c15acb980939"
      },
      "source": [
        "print('10 most frequent words : ')\n",
        "print(bengali_solution.n_most_frequent(None, 10))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 most frequent words : \n",
            "['ржжрж╛ржо', 'ржЯрж╛ржХрж╛ред', 'ржбрж┐ржЬрзЗрж▓рзЗрж░', 'ржкрзЗржЯрзНрж░рзЛрж▓рзЗрж░', 'ржжрзЗрж╢рзЗрж░', 'рждрзЗрж▓рзЗрж░', 'ржЯрж╛ржХрж╛', 'ржжрж╛ржоред', 'рж▓рж┐ржЯрж╛рж░', 'рж╕ржмржЪрзЗржпрж╝рзЗ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBVHIcO_1O9b",
        "outputId": "b293c4c2-5c82-4320-e6d0-8f8cac9692cf"
      },
      "source": [
        "print('4 letter words in decreasing order of frequency from left to right: ')\n",
        "print(bengali_solution.n_letter_words())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4 letter words in decreasing order of frequency from left to right: \n",
            "['ржЯрж╛ржХрж╛', 'ржжрж╛ржоред', 'ржкржпрж╝рж╕рж╛', 'ржжрзЗрж╢рзЗ', 'ржЯрж╛ржирж╛', 'ржжрж╛ржоржУ', 'ржХрзЛржЯрж╛', 'ржжрж░рзЗрж░', 'ржжрж╛ржпрж╝рзА', 'ржХржбрж╝рж┐', 'ржЯрзБржЗржЯ', 'ржнрж╛рж░ржд', 'ржПржХржЯрж╛', 'ржкрж╛ржБржЪ', 'ржжрзВрж░рзЗ', 'ржжрж┐ржирзЗ', 'рзз.рзорзж', 'рзз.рзорзо', 'ржжрж┐ржиржЗ', 'ржПржХржЯрзБ', 'ржПржХрзБржЯ', 'ржХрж░рзЗред', 'рж╕ржХрж╛рж▓', 'ржЕржпрж╝рзЗрж▓', 'ржЪрж▓рждрж┐']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}