
    In 2018, I wrote Career advice for recent Computer Science graduates about joining a big company instead of a startup after college.

In 2019, when I left NVIDIA, I wrote Lessons learned after my first full-time job about leaving a big company for a startup.

Now that I’ve left my first full-time job at a startup, I want to revisit the topic. This is based on some personal experience, but most come from friends’ experiences, including the intensive note on startups from a friend who had worked at 3 startups before and who would like to remain anonymous. I hope that it’ll give some pointers to those trying to decide whether to take the leap.

Some asked if this post is about Snorkel. It’s not. Snorkel is an exception. It’s a great startup, which is a reason why I joined in the first place, and I recommend it to all my friends who are looking to join a startup.

Disclaimer:

  Each of the points below is true for many startups, but not for all startups, and it’s more true for early-stage startups (e.g. before series B). There are always exceptions, extreme exceptions, unreasonable exceptions, which make the startup world so exciting.
  A friend told me that these points are only true for bad startups. Most startups are, unfortunately, bad startups.



Table of contents
…. 7 reasons not to join a startup
…….. Reason 1. Goodbye work-life balance
…….. Reason 2. You’ll pick up bad engineering practices
…….. Reason 3. Less mentorship
…….. Reason 4. You won’t get rich
…….. Reason 5. Bad management
…….. Reason 6. You might have to do a lot of things you don’t want to do
…….. Reason 7. No clear career growth trajectory
…. One reason to join a startup
…. What’s next for me?



7 reasons not to join a startup

Reason 1. Goodbye work-life balance

A friend at a tech giant told me that he and his co-workers once mused about how long they could go on not working until someone noticed. The answers were between a week and two months. At an early-stage startup, the answer is likely a couple of hours.

My transition from NVIDIA to Snorkel was a culture shock. At NVIDIA, you can have a predictable schedule, e.g. coming in at 9am and leaving at 6pm every day. If you don’t finish something by Friday afternoon, just push the deadline to next week and go to happy hour. It’s okay, even expected, to not check emails or Slack for the entire weekend.

On my first day at Snorkel, when I left at 7pm, I was the first one to leave.

Nobody told me how to spend my time, but when everyone else worked over the weekend and responded to my Slack messages any time of the night, I wanted to do the same. Nobody forced me to take on a hefty task that would require me to cancel plans with friends, but I also knew that everybody else had their hands full and if I didn’t do it, we wouldn’t be able to finish this feature on time and the company would lose a contract or even die.

By the time that I left, the work-life balance had got a lot more balanced. Snorkel had hired a ton more people to share the workload and we had worked out processes to speed things up.

In general, I’ve observed that the bigger the startup, the better the work-life balance. Possible explanations:


  The earlier the startup, the more precarious its survival, and the harder everyone has to push.
  In very early-stage startups, the working culture is dominated by those with high ownership in the company (the founding team), who are incentivized to work harder. Later on, the working culture is dominated by people with much lower ownership in the company (e.g. 0.1% over 4 years for the 20th engineer vs. 20% for the founder), who are more incentivized to keep a work-life balance.


Caveat: The work-life balance at an early-stage startup depends a lot on how much the existing team members work. When interviewing at a startup, don’t ask the founders how much they value work-life balance (they’ll say “A lot”), but ask every team member you can talk to how much they work. If all of them work during evenings and weekends, you might likely feel pressured to do the same.

Reason 2. You’ll pick up bad engineering practices

Consider the following scenario. A customer requires a new feature and you have to deliver it in a week. This feature is similar to one of your existing features, so the best solution is to refactor the existing code to allow some of it to be reused.

However, refactoring alone would require a week. Your tech lead decides that you should just duplicate the existing code and turn it into a new feature. Now you have two massive code structures that are similar but not quite. When making changes to one structure, you have to remember to change the other too.

Then, somebody forgets and a wild bug appears. The person assigned to fix it isn’t given a lot of time, so instead of investigating the duplicate code, they write a hacky function on top.

Startups build 1 from 0, something from nothing. Adding new things fast takes precedence over both adding good things slow and fixing existing things. You might get used to writing quick and dirty code, cargo cult programming, merging code that has no tests, merging before tests complete, committing without comments, spaghetti code, magic numbers.

Bad practices might be a mere dissatisfaction at first, but can gradually become a habit, then become the only way you know how to work.

Reason 3. Less mentorship

The thing I missed the most when leaving NVIDIA was mentorship. Large companies, by virtue of having a lot of employees, tend to have many people whose diverse life experience can provide you invaluable advice. At NVIDIA, I could come to my mentors for questions from general career dilemmas to obscure engineering knowledge. Once in a while, I browsed the org chart of tens of thousands of employees, identified people I want to learn from, and asked them to meet at the coffee machine, which they usually accepted.

Startups don’t have that many people for you to reach out to in the first place. Your handful of coworkers might have backgrounds and experiences similar to yours (cue founders who say they prefer hiring from their existing networks) and are unlikely to give you dramatically different perspectives. Even if there are people who could mentor you, given the pace at which startups move, they might not have the time for it.

To be clear, you can still learn a lot from your coworkers at startups, just a different kind of learning.

Reason 4. You won’t get rich

Despite a plethora of articles warning people that joining startups is a bad way to get rich (1, 2, 3), many people still think joining a startup is a get-rich-quick scheme. Here’s the gist of the math. Imagine you’re an engineer with 2-3 years of experience.

If you join a startup as the 15th engineer (not executive), your compensation might look like the following.


  Base salary: Your base salary is usually lower than you would have got at a big company (e.g. $120K instead of $160K) because at startups, equity makes a large chunk of your compensation.
  Equity: You might get 0.05% - 0.25% equity vested over 4 years. After subsequent fundraising rounds, this amount of equity is diluted to 0.02% - 0.1% for 4 years.



    
        table {
          border-collapse: collapse;
          width: 100%;
        }

        th, td {
          padding: 8px;
          text-align: left;
          border-bottom: 1px solid #ddd;
        }
    
  
   
Probability(appx)
   
   Startup scenario
   
   Startup value
   
   Your equity valueover 4 year
   
   Your yearly comp(base + equity)
   
  
  
   80%
   
   Fails
   
   0
   
   0
   
   $120K
   
  
  
   5%
   
   IPO
   
   $1 billion
   
   $200K - 1M
   
   $170K - 270K
   
  
  
   0.5%
   
   IPO
   
   $10 billion
   
   $2M - 10M
   
   $620K - 2.62M
   
  
  
   0.05%
   
   IPO
   
   $100 billion
   
   $20M - 100M
   
   $5M - 25M
   
  
  
   14.45%
   
   Acquired
   
   $$$
   
   $0 - 8M
   
   $120K - 2.12M
   
  



If you join late at the startup (say employee number 100+), even if the company succeeds wildly, your equity will be worth very little.

If you want to get rich, join a big company and climb their rank. You can find the detailed analysis of compensations for 19,000 FAAAM-dominated tech workers here, but below is a plausible, even conservative, scenario if you join a company like Google with 2-3 years of experience.


  1st year, L4 $250K/year.
  2nd year, L4, $280K/year.
  3rd year, L4, $320K/year.
  4th year, L5, $360K/year.


After the first 4 years at Google, you’ve already made over $1 million, not counting “perks” like work-life balance.

Reason 5. Bad management

There’s a trend among startups to not fixate on titles until they have to. Some avoid “manager” to not endanger the “everyone is equal” mindset (protip: everyone isn’t equal at startups – some have much more equity than others). In the early stage of a startup (e.g. before the 20th employee), there might not be anyone with “manager” in their title. If you join during that phase, you’re expected to get things done with little to no guidance.

Even if your startup has managers, they are likely bad managers. A startup’s first managers are likely its founding team who might have little to no real-world working experience, let alone managerial experience (e.g. recent dropouts, recent graduates). It doesn’t mean that people without working experience can’t be good managers (I know a few), it’s just more rare.

Bad management can manifest in the lack of feedback. At startups, you might get a lot of work-specific feedback – demos, design docs, even code (though it might not be good feedback) – because people at startups are generally more invested in the company. However, you won’t get much you-specific feedback that can help you grow such as what skills you’re lacking or what you need to do to get to the level you want to get to.

Even if there are processes in place for feedback, everyone might be too caught up in sprinting to think about you, what you want, or what opportunities they can give you to grow.

Bad management can be especially frustrating during conflicts, which will inevitably arise when you work in a high-stress environment (e.g. you’re all trying to push a feature at 11pm on a Saturday, everyone is tired and snappy). When something bothers you, you might feel like there’s no one you can talk to because you either don’t trust your manager or don’t trust that they are capable of resolving it.

Bad management is frequently the cause of conflicts. Bad managers might make you feel like they don’t listen to you, don’t value you, don’t treat you fairly, or just don’t have your interest at heart.

Reason 6. You might have to do a lot of things you don’t want to do

Big companies can afford to hire specialists – people specialize in a very narrow field. Startups want generalists – people who can do multiple things. The upside of being a generalist is that you get exposed to many different aspects of a company. The downside is that you have to do a lot of things you don’t want to do.

You might not enjoy writing documentation, but someone has to do it. You might not enjoy writing hacky JavaScript, but nobody else on the team has the bandwidth for it. You might think that it’s stupid to make your software work on Windows, but a customer specifically requested it.

There are things that you don’t want to do because it contradicts your sense of morality. A friend of mine left a startup because she couldn’t lie about the company’s product any longer. Many companies pursue the “fake it until you make it” strategy, painting their product as the product they want to have, not the product they already have. The line between hustling and lying to people is thinner than you think, and you’ll find yourself walking it for the sake of the company’s success.

Disclaimer: Just want to reiterate that this post isn’t about Snorkel. Snorkel has a great product and I’ve been very impressed at how fast the team has been able to build out the platform.

Reason 7. No clear career growth trajectory

Big tech companies tend to have a structured way of defining career growth. Engineers can even climb numeric ranks such as L3, L4, L5, etc. Each level often comes with (technically) well-defined responsibilities and requires a set of skills that can be achieved through an approximate number of years. This structure guides you in growing in your role.

Early startups have no such structure. If you join as a software engineer, nobody tells you what you need to do to become a senior software engineer, and frankly, nobody cares. You might be doing a lot of ad-hoc projects that teach you different engineering aspects, but don’t allow you to become really good at any of them. After three years, your friend at Google might become a superstar engineer that is really good at distributed engineering, and you might still be an engineer who’s average at everything.

Assuming leadership roles at startups is also different from assuming leadership roles at big companies. At big companies, after a certain level, you have the option to remain an individual contributor (IC) or become a manager (for Google, it’s usually L5 or L6).

At startups, it’s based on what the company needs right now. When leadership opportunities open up, they are usually filled by members hand-picked by the founding team. To be hand-picked, you’ll have to put yourself in a position to be seen by the founding team, which requires a lot of self-advocacy if you didn’t already know the founding team in advance (e.g. went to school with them).

This can actually be a reason why you should join a startup – you’ll learn to stick out for yourself. If you know what you want and are proactive about it, you might even be able to assume a leadership role after a very short period of time. Joining a startup can also be a much faster way to climb big co’s ladders. For example, after 3 years at a startup, you became a team lead of 15 engineers, and might be able to join Google at L5/6 instead of L4.

One reason to join a startup

There are many reasons not to join a startup. You just need one good reason to join. The reason is that it’s all worth it. I once read somewhere that joining startups is the only way to get 15 years of experience in 5 years. I couldn’t agree more.

I learned so much during my time at Snorkel, across different disciplines, from engineering to hiring to sales and marketing. I learned how to structure a codebase, automate tests, develop the hiring pipeline, even use fancy terms during go-to-market meetings. I was exposed to so many problems I wouldn’t have been exposed to no matter how much time I spend at big companies.

I learned a lot more about the MLOps space because when working at a startup, you just have to keep an eye out for potential competitors/customers/partners. I also became better at evaluating startups in the space.

You might get frustrated at a startup, but this frustration will make you realize what you want and what you don’t want in your career. You might not get rich at a startup, but all the skills you learn there might help you get rich in the future.

If I had to go back in time, knowing what I know now, I’d still choose to join a startup.


    I've been at this startup for less than a month and I've been exposed to so many problems I didn't even know existed. I'm of the increasing belief that everyone should try a startup early in their career, e.g. within the first 3 years/before settling into complacency.— Chip Huyen (@chipro) January 13, 2020 


If you consider joining a startup, my only advice is always choose the people. Don’t join for money. Don’t join for titles. Join because of the great people you want to work with. Most startups eventually fail. Most great people eventually succeed.

And if you still consider joining a startup, you should definitely check Snorkel out!

What’s next for me?

When I was interviewing at Snorkel, I told Alex Ratner (the CEO and a great person to talk to), that a reason I wanted to join an early-stage startup is to learn how to build a company. Alex told me that whenever I decided to leave to start my own company, the company would support it. I’m grateful to Alex, Henry, Devang, and the rest of the company for having been incredibly understanding and supportive of my next step.

So yes, I’m building something that I hope I can share with you soon.

In the meantime, I want to devote a lot of my time to learning. I’m hoping to find a group of people with similar interests and learn together. Here are some of the topics that I want to learn:


  How to bring machine learning to browsers
  Online predictions and online learning for machine learning
  MLOps in general


If you want to learn any of the above topics, join our Discord chat. We’ll be sharing learning resources and strategies. We might even host learning sessions and discussions if there’s interest. Serious learners only!

Acknowledgment

Thank you, Maximilian Sieb, Shreya Shankar, and Luke Metz for making this post better.

This post was inspired by intensive personal notes on startups written by a friend, who would like to stay anonymous.


  
    [Twitter thread]

Last June, I published the post What I learned from looking at 200 machine learning tools. The post got some attention and I got a lot of messages from people telling me about new tools. I updated the old list to now include 284 tools. I’ll keep on updating the list as I find out about new tools. Any lead would be much appreciated!

While looking for these MLOps tools, I discovered some interesting points about the MLOps landscape:


  Increasing focus on deployment
  The Bay Area is still the epicenter of machine learning, but not the only hub
  MLOps infrastructures in the US and China are diverging
  More interests in machine learning production from academia


Click here to see the list in Google Sheets. To see a fancy interactive chart, scroll to the end of this blog post.

1. Increasing focus on deployment
In the list of 284 MLOps tools, there are 180 startups. Out of these 180 startups, 65 raised money in 2020. Most startups that raised money in 2020 are in still in the Data pipeline category, with an increasing number of in all-in-one (end-to-end platforms), hardware, and serving.

Here are further breakdowns on the most popular categories:

  Accelerators (building chips optimized for machine learning algorithms, often with a focus on data centers)
  AI Apps platform (end-to-end platforms for developing & deploying AI applications)
  Data management
  Monitoring
  Edge devices (building chips optimized for inference on consumer devices with low power)














2. The Bay Area is still the epicenter of machine learning, but not the only hub
Among 65 MLOps startups that raised money in 2020, more than half of them are outside the Bay Area, with growing hubs in Boston, New York City, Israel.

10 notable MLOps startups outside the Bay Area that raised money in 2020:

  DataRobot - Boston - $320M: enterprise AI apps platform
  Graphcore - UK - $250M: chips for machine learning
  Dataiku - NYC - $100M: enterprise AI apps platform
  Hailo - Israel - $60M: chips for machine learning
  DefinedCrowd - Seattle - $50.5M: training data generation
  Zilliz - China - $43M: open-source software for processing unstructured data
  Starburst Data - Boston- $42M: distributed SQL query engine
  Anodot - Israel - $35M: enterprise data monitoring
  Materialize - Boston -  $32M: stream processing
  Rasa - Germany - $26M: API for conversational AI


This is an encouraging signals for people who want to build tools for machine learning production but don’t want to:
succumb to the Bay Area monoculture and astronomical cost of living
deal with American fickle immgration policies.

It’s cheaper to find good engineers outside the Bay, and it might also be easier since you don’t have to compete with hundreds of tech giants and other startups for top engineers.







3. MLOps infrastructures in the US and China are diverging
One term I saw while reading China-focused AI newsletters is “localization” (国产化替代) – replace foreign technologies with Chinese alternatives. Chinese companies use tools similar to what American companies use, but not quite the same.

In the US, companies rely on Google Cloud, Amazon AWS for hosting services. In China, companies rely on Tencent Cloud, Alibaba Cloud. In the West, the Cloud Native Computing Foundation (CNCF) acts as the caretaker for container technologies including Kubernetes. In China, they have TARS Foundation for open-source microservices projects.

One reason for a separate foundation is language barriers. Chinese engineers might fork a popular repo, modify it for their use, then either can’t merge back because they don’t know how to communicate with the maintainers in English, or don’t want to merge back because it’d take a lot of time.

The list includes a few Chinese tooling startups, but it missed out so many because Chinese startups don’t get enough attention from the Western media. Many of them have their websites and documents only in Mandarin, which makes it hard to find information about them.

4. More interests in machine learning production from academia
The AI research scene seemed to have calmed down in 2020 – Google freezed hiring for AI researchers, Uber laid off their entire AI research team, Element AI was sold for cheap.

However, the ML production scene is still growing with more and more tools on the market. There’s also a growing interest in ML production even from academia. Here are some of the venues for those who want to learn about ML production in academia:
Challenges in Deploying and Monitoring Machine Learning Systems workshop at ICML
MLSys Seminars at Stanford
Conference on Machine Learning and Systems

The seemingly less steep curve from 2019 to 2020 is because a lot of companies that started this year are still in stealth.







Click on a category to see the tools in that category. Click on the white space in the center to go up a level.



Acknowledgment: Thanks Luke Metz for being a faithful first reader.


I want to devote a lot of my time to learning. I’m hoping to find a group of people with similar interests and learn together. Here are some of the topics that I want to learn:


  How to bring machine learning to browsers
  Online predictions and online learning for machine learning
  MLOps in general


If you want to learn any of the above topics, join our Discord chat. We’ll be sharing learning resources and strategies. We might even host learning sessions and discussions if there’s interest. Serious learners only!

  
    [Twitter thread]

After talking to machine learning and infrastructure engineers at major Internet companies across the US, Europe, and China, I noticed two groups of companies. One group has made significant investments (hundreds of millions of dollars) into infrastructure to allow real-time machine learning and has already seen returns on their investments. Another group still wonders if there’s value in real-time ML.

There seems to be little consensus on what real-time ML means, and there hasn’t been a lot of in-depth discussion on how it’s done in the industry. In this post, I want to share what I’ve learned after talking to about a dozen companies that are doing it.

There are two levels of real-time machine learning that I’ll go over in this post.

  Level 1: Your ML system makes predictions in real-time (online predictions).
  Level 2: Your system can incorporate new data and update your model in real-time (online learning).


I use “model” to refer to the machine learning model and “system” to refer to the infrastructure around it, including data pipeline and monitoring systems.


Table of contents
…. Level 1: Online predictions - your system can make predictions in real-time
…….. Use cases
………… Problems with batch predictions
…….. Solutions
………… Fast inference
………… Real-time pipeline
……………. Stream processing vs. batch processing
……………. Event-driven vs. request-driven
…….. Challenges
…. Level 2: Online learning - your system can incorporate new data and update in real-time
…….. Defining “online learning”
…….. Use case
…….. Solutions
…….. Challenges
………… Theoretical
………… Practical
…. The MLOps race between the US and China
…. Conclusion



Level 1: Online predictions - your system can make predictions in real-time
Real-time here is defined to be in the order of milliseconds to seconds.

Use cases
Latency matters, especially for user-facing applications. In 2009, Google’s experiments demonstrated that increasing web search latency 100 to 400 ms reduces the daily number of searches per user by 0.2% to 0.6%. In 2019, Booking.com found that an increase of 30% in latency cost about 0.5% in conversion rates — “a relevant cost for our business.”

No matter how great your ML models are, if they take just milliseconds too long to make predictions, users are going to click on something else.

Problems with batch predictions
One non-solution is to avoid making predictions online. You can generate predictions in batch offline, store them (e.g. in SQL tables), and pull out pre-computed predictions when needed.

This can work when the input space is finite – you know exactly how many possible inputs to make predictions for. One example is when you need to generate movie recommendations for your users – you know exactly how many users there are. So you predict a set of recommendations for each user periodically, such as every few hours.

To make their user input space finite, many apps make their users choose from categories instead of entering wild queries. For example, if you go to TripAdvisor, you first have to pick a predefined metropolis area instead of being able to enter just any location.

This approach has many limitations. TripAdvisor results are okay within their predefined categories, such as “Restaurants” in “San Francisco”, but are pretty bad when you try to enter wild queries like “high rating Thai restaurants in Hayes Valley”.







Limitations caused by batch predictions exist even in more technologically progressive companies like Netflix. Say, you’ve been watching a lot of horrors lately, so when you first log into Netflix, horror movies dominate recommendations. But you’re feeling bright today so you search “comedy” and start browsing the comedy category. Netflix should learn and show you more comedy in your list of their recommendations, right? But it can’t update the list until the next time batch recommendations are generated.

In the two examples above, batch predictions lead to decreases in user experience (which is tightly coupled with user engagement/retention), not catastrophic failures. Other examples are ad ranking, Twitter’s trending hashtag ranking, Facebook’s newsfeed ranking, estimating time of arrival, etc.

There are also many applications that, without online predictions, would lead to catastrophic failures or just wouldn’t work. Examples include high frequency trading, autonomous vehicles, voice assistants, unlocking your phones using face/fingerprints, fall detection for elderly care, fraud detection, etc. Being able to detect a fraudulent transaction that happened 3 hours ago is still better than not detecting it at all, but being able to detect it in real-time can prevent it from going through.

Switching from batch predictions to real-time predictions allows you to use dynamic features to make more relevant predictions. Static features are information that changes slowly or rarely – age, gender, job, neighborhood, etc. Dynamic features are features based on what’s happening right now – what you’re watching, what you’ve just liked, etc. Knowing a user’s interests right now will allow your systems to make recommendations much more relevant to them.







Solutions
For your system to be able to make online predictions, it has to have two components:


  Fast inference: model that can make predictions in the order of milliseconds
  Real-time pipeline: a pipeline that can process data, input it into model, and return a prediction in real-time


Fast inference
When a model is too big and taking too long to make predictions, there are three approaches:

1. Make models faster (inference optimization)

E.g. fusing operations, distributing computations, memory footprint optimization, writing high performance kernels targeting specific hardwares, etc.

2. Make models smaller (model compression)

Originally, this family of technique is to make models smaller to make them fit on edge devices. Making models smaller often makes them run faster. The most common, general technique for model compression is quantization, e.g. using 16-bit floats (half precision) or 8-bit integers (fixed-point) instead of 32-bit floats (full precision) to represent your model weights. In the extreme case, some have attempted 1-bit representation (binary weight neural networks), e.g. BinaryConnect and Xnor-Net. The authors of Xnor-Net spun off Xnor.ai, a startup focused on model compression which was acquired by Apple for a reported $200M.

Another popular technique is knowledge distillation – a small model (student) is trained to mimic a larger model or an ensemble of models (teacher). Even though the student is often trained with a pre-trained teacher, both may also be trained at the same time. One example of a distilled network used in production is DistilBERT, which reduces the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster.

Other techniques include pruning (finding parameters least useful to predictions and setting them to 0) and low-rank factorization (replacing the over-parametric convolution filters with compact blocks to both reduce the number of parameters and increase speed). See A Survey of Model Compression and Acceleration for Deep Neural Networks (Cheng et al.. 2017) for a detailed analysis.

The number of research papers on model compression is growing. Off-the-shelf utilities are proliferating. Awesome Open Source has a list of The Top 40 Model Compression Open Source Projects.

3. Make hardware faster

This is another research area that is booming. Big companies and startups alike are in a race to develop hardware that allows large ML models to do inference, even training, faster both on the cloud and especially on devices. IDC forecasts that by 2020, the combination of edge and mobile devices doing inferencing will total 3.7 billion units, with a further 116 million units doing training.

Real-time pipeline
Suppose you have a ride sharing app and want to detect fraudulent transactions e.g. payments using stolen credit cards. When the true credit owner discovers unauthorized payments, they’ll dispute with their bank and you’ll have to refund the charges. To maximize profits, fraudsters might call multiple rides either in succession or from multiple accounts. In 2019, merchants estimate fraudulent transactions account for an average of 27% of their annual online sales. The longer it takes for you to detect the stolen credit card, the more money you’ll lose.

To detect whether a transaction is fraudulent, looking at that transaction alone isn’t enough. You need to at least look into the recent history of the user involved in that transaction, their recent trips and activities in-app, the credit card’s recent transactions, and other transactions happening around the same time.

To quickly access these types of information, you want to keep as much of them in-memory as possible. Every time an event you care about happens – a user choosing a location, booking a trip, contacting a driver, canceling a trip, adding a credit card, removing a credit card, etc. – information about that event goes into your in-memory storage. It stays there for as long as they are useful (usually in order of days) then either goes into permanent storage (e.g. S3) or is discarded. The most common tool for this is Apache Kafka, with alternatives such as Amazon Kinesis. Kafka is a stream storage: it stores data as it streams.

Streaming data is different from static data – data that already exists somewhere in its entirety, such as CSV files. When reading from CSV files, you know when the job is finished. Streams of data never finish.

Once you’ve had a way to manage streaming data, you want to extract features to input into your ML models. On top of features from streaming data, you might also need features from static data (when was this account created, what’s the user’s rating, etc.). You need a tool that allows you to process streaming data as well as static data and join them together from various data sources.

Stream processing vs. batch processing

People generally use “batch processing” to refer to static data processing because you can process them in batches. This is opposed to “stream processing”, which processes each event as it arrives. Batch processing is efficient – you can leverage tools like MapReduce to process large amounts of data. Stream processing is fast because you can process each piece of data as soon as it comes. Robert Metzger, a PMC member at Apache Flink, disputed that streaming processing can be as efficient as batch processing because batch is a special case of streaming.

Processing stream data is more difficult because the data amount is unbounded and the data comes in at variable rates and speeds. It’s easier to make a stream processor do batch processing than making a batch processor do stream processing.

Apache Kafka has some capacity for stream processing and some companies use this capacity on top of their Kafka stream storage, but Kafka stream processing is limited in its ability to deal with various data sources. There have been efforts to extend SQL, the popular query language intended for static data tables, to handle data streams [1, 2]. However, the most popular tool for stream processing is Apache Flink, with native support for batch processing.

In the early days of machine learning production, many companies built their ML systems on top of their existing MapReduce/Spark/Hadoop data pipeline. When these companies want to do real-time inference, they need to build a separate pipeline for streaming data.

Having two different pipelines to process your data is a common cause for bugs in ML production, e.g. the changes in one pipeline aren’t correctly replicated in the other leading to two pipelines extracting two different sets of features. This is especially common if the two pipelines are maintained by two different teams, e.g. the development team maintains the batch pipeline for training while the deployment team maintains the stream pipeline for inference. Companies including Uber and Weibo have made major infrastructure overhaul to unify their batch and stream processing pipelines with Flink.

Event-driven vs. request-driven

The software world has gone microservices in the last decade. The idea is to break your business logic into small components – each component is a self-contained service – that can be maintained independently. The owner of each component can update to and test that component quickly without having to consult the rest of the system.

Microservices often go hand-in-hand with REST, a set of methods that let these microservices communicate. REST APIs are request-driven. A client (service) sends requests to tell its server exactly what to do via methods such as POST and GET, and its server responds with the results. A server has to listen to the request for the request to register.

Because in a request-driven world, data is handled via requests to different services, no one has an overview of how data flows through the entire system. Consider a simple system with 3 services:


  A manages drivers availability
  B manages ride demand
  C predicts the best possible price to show customers each time they request a ride


Because prices depend on availability and demands, service C’s output depends on the outputs from service A and B. First, this system requires inter-service communication: C needs to ping A and B for predictions, A needs to ping B to know whether to mobilize more drivers and ping C to know what price incentive to give them. Second, there’d be no easy way to monitor how changes in A or B logics affect the performance of service C, or to map the data flow to debug if service C’s performance suddenly goes down.

With only 3 services, things are already getting complicated. Imagine having hundreds, if not thousands of services like what major Internet companies have. Inter-service communication would blow up. Sending data as JSON blobs over HTTP – the way REST requests are commonly done – is also slow. Inter-service data transfer can become a bottleneck, slowing down the entire system.

Instead of having 20 services ping service A for data, what if whenever an event happens within service A, this event is broadcasted to a stream, and whichever service wants data from A can subscribe to that stream and pick out what it needs? What if there’s a stream all services can broadcast their events and subscribe to? This model is called pub/sub: publish & subscribe. This is what solutions like Kafka allow you to do. Since all data flows through a stream, you can set up a dashboard to monitor your data and its transformation across your system. Because it’s based on events broadcasted by services, this architecture is event-driven.





Beyond Microservices: Streams, State and Scalability (Gwen Shapira, QCon 2019)



Request-driven architecture works well for systems that rely more on logics than on data. Event-driven architecture works better for systems that are data-heavy.

Challenges
Many companies are switching from batch processing to stream processing, from request-driven architecture to event-driven architecture. My impression from talking to major Internet companies in the US and China is that this change is still slow in the US, but much faster in China. The adoption of streaming architecture is tied to the popularity of Kafka and Flink. Robert Metzger told me that he observed more machine learning workloads with Flink in Asia than in the US. Google Trends for “Apache Flink” is consistent with this observation.







There are many reasons why streaming isn’t more popular.

  Companies don’t see the benefits of streaming
    
      Their system isn’t at a scale where inter-service communication is a bottleneck.
      They don’t have applications that benefit from online predictions.
      They have applications that might benefit from online predictions but they don’t know that yet because they have never done online predictions before.
    
  
  High initial investment on infrastructure
 Infrastructure updates are expensive and can jeopardize existing applications. Managers might not be willing to invest to upgrade their infra to allow online predictions.
  Mental shift
 Switching from batch processing to stream processing requires a mental shift. With batch processing, you know when a job is done. With stream processing, it’s never done. You can make rules such as get the average of all data points in the last 2 minutes, but what if an event that happened 2 minutes ago got delayed and hasn’t entered the stream yet? With batch processing, you can have well-defined tables and join them, but in streaming, there are no tables to join, then what does it mean to do a join operation on two streams?
  Python incompatibility
 Python is the lingua franca of machine learning whereas Kafka and Flink run on Java and Scala. Introducing streaming might create language incompatibility in the workflows. Apache Beam provides a Python interface on top of Flink for communicating with streams, but you’d still need people who can work with Java/Scala.
  Higher processing cost
 Batch processing means you can use your computing resources more efficiently. If your hardware is capable of processing 1000 data points at a time, it’s wasteful to use it to process only 1 data point at a time.


Level 2: Online learning - your system can incorporate new data and update in real-time
Real-time here is defined to be in the order of minutes

Defining "online learning"
I used “online learning” instead of “online training” because the latter term is contentious. By definition, online training means learning from each incoming data point. Very, very few companies actually do this because:


  This method suffers from catastrophic forgetting – neural networks abruptly forget previously learned information upon learning new information.
  It can be more expensive to run a learning step on only one data point than on a batch (this can be mitigated by having hardware just powerful enough to process exactly one data point).


Even if a model is learning with each incoming data point, it doesn’t mean the new weights are deployed after each data point. With our current limited understanding of how ML algorithms learn, the updated model needs to be evaluated first to see how well it does.

For most companies that do so-called online training, their models learn in micro batches and are evaluated after a certain period of time. Only after its performance is evaluated to be satisfactory that the model is deployed wider. For Weibo, their iteration cycle from learning to deploying model updates is 10 minutes.





Machine learning with Flink in Weibo (Qian Yu, Flink Forward 2020)



Use cases
TikTok is incredibly addictive. Its secret lies in its recommendation systems that can learn your preferences quickly and suggest videos that you are likely to watch next, giving its users an incredible scrolling experience. It’s possible because ByteDance, the company behind TikTok, has set up a mature infrastructure that allows their recommendation systems to learn their user preferences (“user profiles” in their lingo) in real-time.

Recommendation systems are perfect candidates for online learning. They have natural labels – if a user clicks on a recommendation, it’s a correct prediction. Not all recommendation systems need online learning. User preferences for items like houses, cars, flights, hotels are unlikely to change from a minute to the next, so it would make little sense for systems to continually learn. However, user preferences for online content – videos, articles, news, tweets, posts, memes – can change very quickly (“I just read that octopi sometimes punch fish for no reason and now I want to see a video of it”). As preferences for online content change in real-time, ads systems also need to be updated in real-time to show relevant ads.

Online learning is crucial for systems to adapt to rare events. Consider online shopping on Black Friday. Because Black Friday happens only once a year, there’s no way Amazon or other ecommerce sites can get enough historical data to learn how users are going to behave that day, so their systems need to continually learn on that day to adapt.

Or consider Twitter search when someone famous tweets something stupid. For example, as soon as the news about “Four Seasons Total Landscaping” went live, many people were going to search “total landscaping”. If your system doesn’t immediately learn that “total landscaping” here refers to the press conference, your users are going to get a lot of gardening recommendations.

Online learning can also help with the cold start problem. A user just joined your app and you have no information on them yet. If you don’t have the capacity for any form of online learning, you’ll have to serve your users generic recommendations until the next time your model is trained offline.

Solutions
Since online learning is still fairly new and most companies who are doing it aren’t talking publicly about it in detail yet, there’s no standard solution.

Online learning doesn’t mean “no batch training”. The companies that have most successfully used online learning also train their models offline in parallel and then combine the online version with the offline version.

Challenges
There are many challenges facing online learning, both theoretical and practical.

Theoretical
Online learning flips a lot of what we’ve learned about machine learning on its head. In introductory machine learning classes, students are probably taught different versions of “train your model with a sufficient number of epochs until convergence.” In online learning, there’s no epoch – your model sees each data point only once. There’s no such thing as convergence either. Your underlying data distribution keeps on shifting. There’s nothing stationary to converge to.

Another theoretical challenge for online learning is model evaluation. In traditional batch training, you evaluate your models on stationary held out test sets. If a new model performs better than the existing model on the same test set, we say the new model is better. However, the goal of online learning is to adapt your model to constantly changing data. If your updated model is trained to adapt to data now, and we know that data now is different from data in the past, it wouldn’t make sense to use old data to test your updated model.

Then how do we know that the model trained on data from the last 10 minutes is better than the model trained on data from 20 minutes ago? We have to compare these two models on current data. Online training demands online evaluation, but serving a model that hasn’t been tested to users sounds like a recipe for disaster.

Many companies do it anyway. New models are first subject to offline tests to make sure they aren’t disastrous, then evaluated online in parallel with the existing models via a complex A/B testing system. Only when a model is shown to be better than an existing model in some metrics the company cares about that it can be deployed wider. (Don’t get me started on choosing a metric for online evaluation).

Practical
There are not yet standard infrastructures for online training. Some companies have converged to streaming architecture with parameter servers, but other than that, companies that do online training that I’ve talked to have to build a lot of their infrastructures in house. I’m reluctant to discuss this online since some companies asked me to keep this information confidential because they’re building solutions for them – it’s their competitive advantage.

The MLOps race between the US and China
I’ve read a lot about the AI race between the US and China, but most comparisons seem to focus on the number of research papers, patents, citations, funding. Only after I’ve started talking to both American and Chinese companies about real-time machine learning that I noticed a staggering difference in their MLOps infrastructures.

Few American Internet companies have attempted online learning, and even among these companies, online learning is used for simple models such as logistic regression. My impression from both talking directly to Chinese companies and talking with people who have worked with companies in both countries is that online learning is more common in China, and Chinese engineers are more eager to make the jump. You can see some of the conversations here and here.







Conclusion
Machine learning is going real-time, whether you’re ready or not. While the majority of companies are still debating whether there’s value in online inference and online learning, some of those who do it correctly have already seen returns on investment, and their real-time algorithms might be a major contributing factor that helps them stay ahead of their competitors.

I have a lot more thoughts on real-time machine learning but this post is already long. If you’re interested in chatting about this, shoot me an email.

Acknowledgments
This post is a synthesis of many conversations with the following wonderful engineers and academics. I’d like to thank Robert Metzger, Neil Lawrence, Savin Goyal, Zhenzhong Xu, Ville Tuulos, Dat Tran, Han Xiao, Hien Luu, Ledio Ago, Peter Skomoroch, Piero Molino, Daniel Yao, Jason Sleight, Becket Qin, Tien Le, Abraham Starosta, Will Deaderick, Caleb Kaiser, Miguel Ramos.

There are several more people who have chosen to stay anonymous. Without them, this post would be incomplete.

Thanks Luke Metz for being an amazing first reader!


I want to devote a lot of my time to learning. I’m hoping to find a group of people with similar interests and learn together. Here are some of the topics that I want to learn:


  How to bring machine learning to browsers
  Online predictions and online learning for machine learning
  MLOps in general


If you want to learn any of the above topics, join our Discord chat. We’ll be sharing learning resources and strategies. We might even host learning sessions and discussions if there’s interest. Serious learners only!


  
    Update: The course website is up! You can find the latest syllabus as well as lecture notes and slides there.

Ever since teaching TensorFlow for Deep Learning Research, I’ve known that I love teaching and want to do it again.

In early 2019, I started talking with Stanford’s CS department about the possibility of coming back to teach. After almost two years in development, the course has finally taken shape. I’m excited to let you know that I’ll be teaching CS 329S: Machine Learning Systems Design at Stanford in January 2021.

The course wouldn’t have been possible with the help of many people including Christopher Ré, Jerry Cain, Mehran Sahami, Michele Catasta, Mykel J. Kochenderfer.

Here’s a short description of the course. You can find the (tentative) syllabus below.

This project-based course covers the iterative process for designing, developing, and deploying machine learning systems. It focuses on systems that require massive datasets and compute resources, such as large neural networks. Students will learn about the different layers of the data pipeline, approaches to model selection, training, scaling, as well as how to deploy, monitor, and maintain ML systems. In the process, students will learn about important issues including privacy, fairness, and security.

Pre-requisites: At least one of the following; CS229, CS230, CS231N, CS224N, or equivalent. Students should have a good understanding of machine learning algorithms and should be familiar with at least one framework such as TensorFlow, PyTorch, JAX.

For Stanford students interested in taking the course, you can fill in the application here. The course will be evaluated based on one final project (at least 50%), three short assignments, and class participation.

For those outside Stanford, I’ll try to make as much of the course materials available as possible. I’ll post updates about the course on Twitter or you can check back here from time to time.

Since these are all new materials, I’m hoping to get early feedback. If you’re interested in becoming a reviewer for the course materials, please shoot me an email. Thank you!

Tentative syllabus

Week 1: Overview of machine learning systems design

  When to use ML
  ML in research vs. ML in production
  ML systems vs. traditional software
  ML production myths
  ML applications
  Case studies


Week 2: Iterative process

  Principles of a good ML system
  Iterative process
  Scoping the project


Week 3: Data management

  Challenges of real- world data
  How to collect, store, and handle massive data
  Different layers of the data pipeline
  Data processor & monitor
  Data controller
  Data storage
  Data ingestion: database- engines


Week 4: Creating training datasets

  Feature engineering
  Data labeling
  Data leakage
  Data partitioning, slicing, and sampling


Week 5: Building and training machine learning models

  Baselines
  Model selection
  Training, debugging, and experiment tracking
  Distributed training
  Evaluation and benchmarking
  AutoML


Week 6: Deployment

  Inference constraints
  Model compression and optimization
  Training vs. serving skew
  Concept drift
  Server- side ML vs. client- side ML
  Releasing strategies
  Deployment evaluation


Week 7: Project milestone and discussion

  Ethical concerns


Week 8: Monitoring and maintenance

  What to monitor
  Metrics, logging, tags, alerts
  Updates and rollbacks
  Iterative improvement


Week 9: Hardware & infrastructure

  Architectural choices
  Hardware design
  Edge devices
  Clouds vs. private data centers
  Future of high- performance computing


Week 10: Integrating ML into business

  Model performance vs. business goals vs. user experience
  Team structure
  Why ML projects fail
  Best practices
  State of ML production


This blog post was edited by the wonderful Andrey Kurenkov.


I want to devote a lot of my time to learning. I’m hoping to find a group of people with similar interests and learn together. Here are some of the topics that I want to learn:


  How to bring machine learning to browsers
  Online predictions and online learning for machine learning
  MLOps in general


If you want to learn any of the above topics, join our Discord chat. We’ll be sharing learning resources and strategies. We might even host learning sessions and discussions if there’s interest. Serious learners only!

  
    [Twitter thread, Hacker News discussion]

Click here to see the new version of this list with an interactive chart (updated December 30, 2020).

To better understand the landscape of available tools for machine learning production, I decided to look up every AI/ML tool I could find. The resources I used include:


  Full stack deep learning
  LF AI Foundation landscape
  AI Data Landscape
  Various lists of top AI startups by the media
  Responses to my tweet and LinkedIn post
  People (friends, strangers, VCs) share with me their lists


After filtering out applications companies (e.g. companies that use ML to provide business analytics), tools that aren’t being actively developed, and tools that nobody uses, I got 202 tools. See the full list. Please let me know if there are tools you think I should include but aren’t on the list yet!

Disclaimer


  This list was made in November 2019, and the market must have changed in the last 6 months.
  Some tech companies just have a set of tools so large that I can’t enumerate them all. For example, Amazon Web Services offer over 165 fully featured services.
  There are many stealth startups that I’m not aware of, and many that died before I heard of them.


This post consists of 6 parts:

I. Overview
II. The landscape over time
III. The landscape is under-developed
IV. Problems facing MLOps
V. Open source and open-core
VI. Conclusion

I. Overview
In one way to generalize the ML production flow that I agreed with, it consists of 4 steps:


  Project setup
  Data pipeline
  Modeling & training
  Serving


I categorize the tools based on which step of the workflow that it supports. I don’t include Project setup since it requires project management tools, not ML tools. This isn’t always straightforward since one tool might help with more than one step. Their ambiguous descriptions don’t make it any easier: “we push the limits of data science”, “transforming AI projects into real-world business outcomes”, “allows data to move freely, like the air you breathe”, and my personal favorite: “we lived and breathed data science”.

I put the tools that cover more than one step of the pipeline into the category that they are best known for. If they’re known for multiple categories, I put them in the All-in-one category. I also include the Infrastructure category to include companies that provide infrastructure for training and storage. Most of these are Cloud providers.

II. The landscape over time
I tracked the year each tool was launched. If it’s an open-source project, I looked at the first commit to see when the project began its public appearance. If it’s a company, I looked at the year it started on Crunchbase. Then I plotted the number of tools in each category over time.







As expected, this data shows that the space only started exploding in 2012 with the renewed interest in deep learning.

Pre-AlexNet (pre-2012)
Up until 2011, the space is dominated by tools for modeling and training, with some frameworks that either are still very popular (e.g. scikit-learn) or left influence on current frameworks (Theano). A few ML tools that started pre-2012 and survived until today have either had their IPOs (Cloudera, Datadog, Alteryx), been acquired (Figure Eight), or become popular open-source projects actively developed by the community (Spark, Flink, Kafka).

Development phase (2012-2015)
As the machine learning community took the “let’s throw data at it” approach, the ML space became the data space. This is even more clear when we look into the number of tools started each year in each category. In 2015, 57% (47 out of 82 tools) are data pipeline tools.







Production phase (2016-now)
While it’s important to pursue pure research, most companies can’t afford it unless it leads to short-term business applications. As ML research, data, and off-the-shelf models become more accessible, more people and organizations would want to find applications for them, which increases the demand for tools to help productionize machine learning.

In 2016, Google announced its use of neural machine translation to improve Google Translate, marking the one of the first major applications of deep learning in the real world. Since then, many tools have been developed to facilitate serving ML applications.

III. The landscape is under-developed
While there are many AI startups, most of them are application startups (providing applications such as business analytics or customer support) instead of tooling startups (creating tools to help other companies build their own applications). Or in VC terms, most startups are vertical AI. Among Forbes 50 AI startups in 2019, only 7 companies are tooling companies.

Applications are easier to sell, since you can go to a company and say: “We can automate half of your customer support effort.” Tools take longer to sell but can have a larger impact since you’re not targeting a single application but a part of the ecosystem. Many companies can coexist providing the same application, but for a part of the process, usually a selected few tools can coexist.

After extensive search, I could only find ~200 AI tools, which is puny compared to the number of traditional software engineering tools. If you want testing for traditional Python application development, you can find at least 20 tools within 2 minutes of googling. If you want testing for machine learning models, there’s none.

IV. Problems facing MLOps
Many traditional software engineering tools can be used to develop and serve machine learning applications. However, many challenges are unique to ML applications and require their own tools.

In traditional SWE, coding is the hard part, whereas in ML, coding is a small part of the battle. Developing a new model that can provide significant performance improvements in real world tasks is very hard and very costly. Most companies won’t focus on developing ML models but will use an off-the-shelf model, e.g. “if you want it put a BERT on it.”

For ML, applications developed with the most/best data win. Instead of focusing on improving deep learning algorithms, most companies will focus on improving their data. Because data can change quickly, ML applications need faster development and deployment cycles. In many cases, you might have to deploy a new model every night.

The size of ML algorithms is also a problem. The pretrained large BERT model has 340M parameters and is 1.35GB. Even if it can fit on a consumer device (e.g. your phone), the time it takes for BERT to run inference on a new sample makes it useless for many real world applications. For example, an autocompletion model is useless if the time it takes to suggest the next character is longer than the time it takes for you to type.

Git does versioning by comparing differences line by line and therefore works well for most traditional software engineering programs. However, it’s not suitable for versioning datasets or model checkpoints. Pandas works well for most traditional dataframe manipulation, but doesn’t work on GPUs.

Row-based data formats like CSV work well for applications using less data. However, if your samples have many features and you only want to use a subset of them, using row-based data formats still requires you to load all features. Columnar file formats like PARQUET and OCR are optimized for that use case.

Some of the problems facing ML applications development:


  Monitoring: How to know that your data distribution has shifted and you need to retrain your model? Example: Dessa, supported by Alex Krizhevsky from AlexNet and acquired by Square in Feb 2020.
  Data labeling: How to quickly label the new data or re-label the existing data for the new model? Example: Snorkel.
  CI/CD test: How to run tests to make sure your model still works as expected after each change, since you can’t spend days waiting for it to train and converge? Example: Argo.
  Deployment: How to package and deploy a new model or replace an existing model? Example: OctoML.
  Model compression: How to compress an ML model to fit in consumer devices? Example: Xnor.ai, a startup spun out of Allen Institute to focus on model compression, raised $14.6M at the valuation of $62M in May 2018. In January 2020, Apple bought it for ~$200M and shut down its website.
  Inference Optimization: How to speed up inference time for your models? Can we fuse operations together? Can we use lower precision? Making a model smaller might make its inference faster. Example: TensorRT.
  Edge device: Hardware designed to run ML algorithms fast and cheap. Example: Coral SOM.
  Privacy: How to use user data to train your models while preserving their privacy? How to make your process GDPR-compliant? Example: PySyft.


I plotted the number of tools by the main problems they address.







A large portion focuses on the data pipeline: data management, labeling, database/query, data processing, data generation. Data pipeline tools are also likely to aim to be all-in-one platforms. Because data handling is the most resource-intensive phase of a project, once you’ve had people put their data on your platform, it’s tempting to provide them with a couple of pre-built/pre-trained models.

Tools for modeling & training are mostly frameworks. The deep learning frameworks competition cooled down to be mostly between PyTorch and TensorFlow, and higher-level frameworks that wrap around these two for specific families of tasks such as NLP, NLU, and multimodal problems. There are frameworks for distributed training. There’s also this new framework coming out of Google that every Googler who hates TensorFlow has been raving about: JAX.

There are standalone tools for experiment tracking, and popular frameworks also have their own experiment tracking features built-in. Hyperparameter tuning is important and it’s not surprising to find several that focus on it, but none seems to catch on because the bottleneck for hyperparameter tuning is not the setup, but the computing power needed to run it.

The most exciting problems yet to be solved are in the deployment and serving space. One reason for the lack of serving solutions is the lack of communication between researchers and production engineers. At companies that can afford to pursue AI research (e.g. big companies), the research team is separated from the deployment team, and the two teams only communicate via the p-managers: product managers, program managers, project managers. Small companies, whose employees can see the entire stack, are constrained by their immediate product needs. Only a few startups, usually those founded by accomplished researchers with enough funding to hire accomplished engineers, have managed to bridge the gap. These startups are poised to take a big chunk of the AI tooling market.

V. Open-source and open-core
109 out of 202 tools I looked at are OSS. Even tools that aren’t open-source are usually accompanied by open-source tools.

There are several reasons for OSS. One is the reason that all pro-OSS people have been talking about for years: transparency, collaboration, flexibility, and it just seems like the moral thing to do. Clients might not want to use a new tool without being able to see its source code. Otherwise, if that tool gets shut down – which happens a lot with startups – they’ll have to rewrite their code.

OSS means neither non-profit nor free. OSS maintenance is time-consuming and expensive. The size of the TensorFlow team is rumored to be close to 1000. Companies don’t offer OSS tools without business objectives in mind, e.g. if more people use their OSS tools, more people know about them, trust their technical expertise, and might buy their proprietary tools and want to join their teams.

Google might want to popularize their tools so that people use their cloud services. NVIDIA maintains cuDF (and previously dask) so that they can sell more GPUs. Databricks offers MLflow for free but sells their data analytics platform. Netflix started their dedicated machine learning team very recently, and released their Metaflow framework to put their name on the ML map to attract talents. Explosion offers SpaCy for free but charges for Prodigy. HuggingFace offers transformers for free and I have no idea how they make money.

Since OSS has become a standard, it’s challenging for startups to figure out a business model that works. Any tooling company started has to compete with existing open-source tools. If you follow the open-core business model, you have to decide which features to include in the OSS, which to include in the paid version without appearing greedy, or how to get free users to start paying.

VI. Conclusion
There has been a lot of talk on whether the AI bubble will burst. A large portion of AI investment is in self-driving cars, and as fully autonomous vehicles are still far from being a commodity, some hypothesize that investors will lose hope in AI altogether. Google has freezed hiring for ML researchers. Uber laid off the research half of their AI team. Both decisions were made pre-covid. There’s rumor that due to a large number of people taking ML courses, there will be far more people with ML skills than ML jobs.

Is it still a good time to get into ML? I believe that the AI hype is real and at some point, it has to calm down. That point might have already happened. However, I don’t believe that ML will disappear. There might be fewer companies that can afford to do ML research, but there will be no shortage of companies that need tooling to bring ML into their production.

If you have to choose between engineering and ML, choose engineering. It’s easier for great engineers to pick up ML knowledge, but it’s a lot harder for ML experts to become great engineers. If you become an engineer who builds great tools for ML, I’d forever be in your debt.

Acknowledgment: Thanks Andrey Kurenkov for being the most generous editor one could ask for. Thanks Luke Metz for being a wonderful first reader.


I want to devote a lot of my time to learning. I’m hoping to find a group of people with similar interests and learn together. Here are some of the topics that I want to learn:


  How to bring machine learning to browsers
  Online predictions and online learning for machine learning
  MLOps in general


If you want to learn any of the above topics, join our Discord chat. We’ll be sharing learning resources and strategies. We might even host learning sessions and discussions if there’s interest. Serious learners only!

  
    [Twitter thread, Hacker News discussion, Reddit discussion]

There are several metrics important to our career progress: How much should I be paid? How long should it take me to get to the next level? Does my field really favor the young and if so, does my career have a deadline?

Employers know the answers as they have the benefit of seeing an aggregated picture through dealing with many candidates and employees. The majority of us, however, may only work with a handful of employers in our entire life. And when we’re early in our careers, this type of information is a blackbox.

This imbalance of information gives employers more leverage, both during offer negotiations and promotion discussions. Last year, I’ve helped around 20 friends negotiate their offers – most of them for their first jobs. I notice that they all originally aimed for a package lower than what people with their backgrounds should be paid. One notable example is a candidate who told me he would be happy with $120k/year and ended up with an offer of $180k/year.

This imbalance of information especially hurts candidates of underrepresented groups who don’t already know people who can help guide them through the process.

I was glad to see an initiative by levels.fyi to let people share their compensation details anonymously and everyone can see the range of compensation for people at certain levels at certain companies.

Recently, levels.fyi shared with me the data. As far as I know, this is the largest data on compensation and level details of tech workers. I used their dataset as a proxy to answer four questions:


  How long does it take for software engineers to reach a certain level? E.g. how long, on average, does it take for one person to become a senior engineer or a principal engineer?
  How much money do tech workers actually make? How is it different across jobs?
  Do women get paid less than men in tech? This compares the compensation details of people at the same company, same job, same level, same state.
  The tech industry is notorious for favoring the young. Do younger engineers really make more money? What’s the correlation between the number of years of experience and the total yearly compensation?


Disclaimer

  The data is self-reported anonymously, and therefore suffers from all the biases of self-reported, anonymous data.
  Just because there’s no evidence in the data to support something, doesn’t mean that something doesn’t exist.


Table of contents

  Data
  Career progression analysis
  Compensation analysis
  Compensation by gender


1. Data

After some basic cleaning, deduplication, and outlier removal, I get the self-reported compensation details of 18.8k tech workers. Each entry contains:

  compensation details (broken down to base salaries, equity, bonus)
  job
  level
  gender
  years of experience
  years at company


There are 13k samples with gender specified as Female or Male. Of which, 13.27% are female. This is close to a PEW research in 2018 that women make up only 14% of the engineering workforce.

Approximately 80% of these entries come from the US.


  40% from California
  24% from Washington


Most popular cities:
Seattle, WA         3119
San Francisco, CA   2331
New York, NY        1133
Redmond, WA         1091
Sunnyvale, CA       877
Mountain View, CA   859
San Jose, CA        648


FAAAM (Facebook, Alphabet/Google, Amazon, Apple, Microsoft) employees make up 40% of the entries.
amazon         2629
microsoft      1971
google         1552
facebook        873
apple           781


65% of the entries are software engineers. 7 most popular jobs account for 88% of all the entries.

software engineer   12405
product manager     1311
swe manager         1125
data scientist      649
hardware engineer   460
product designer    366
solution architect  350


Tech companies have different levels. For example, Google’s levels go from L3 to L10 but Microsoft goes from SDE to Technical Fellow while NVIDIA goes from IC1 to IC9. levels.fyi tried to standardize most levels for software engineering across major companies to 6 levels from Entry Level Engineer to Distinguished Engineer. They didn’t annotate standard levels for entries from smaller companies, because roles at smaller companies are more fluid.

Entry Level Engineer      3526
Software Engineer         3965
Senior Engineer           2795
Staff Engineer             774
Principal Engineer         203
Distinguished Engineer      26








For more details on how these levels compare across companies, visit levels.fyi.

2. Career progression analysis for software engineers
We don’t have the number of years people take to reach a level. What we have is the number of years of experience someone has, and what level they are right now. The number we have is the sum of:

  the number of years it takes for them to reach that level.
  the number of years they have been at that level.


For engineering roles at some companies, after you reach a certain level, it’s very hard to progress to the next level, e.g. it’s hard to progress beyond level SDE III at Amazon or beyond L6 at Google. Some people either stay at the role for a long time, change company, or change careers.

Reporting bias: people tend to round up their number of years of experience to the nearest integer.

Analysis

  If after graduation, people become an entry level engineer, then on average, it takes 1 year to get out of that entry level position.
  After spending 4 years as a software engineer to become a senior sotware engineer, it takes another 4 years to become a staff engineer.
  It takes, on average, 18 years to become a Distinguished Engineer.








2.1 Career progression at major companies
Let’s look at the time it takes to move from a level to the next at FAAAM companies.

At Google, on average, people at L7 has one year of experience less than people at L6. One theory is that to get to L7 at Google, you have to be really good, and really good people get promoted faster.










2.2 As level increases, the percentage of female software engineers decreases
Both at standard levels across all companies, and within each major company. This is consistent with the known women’s leadership gap, both in tech and out of tech [1, 2].

Three hypotheses:

  Because of the ceiling, it’s harder for women to be promoted.
  Women leave tech before they get promoted. According to a research by the Center for Work-Life Policy study, more than half of women in tech leave the industry by the mid-point of their career, which is more than double the rate of men. Indeed surveyed 1,000 women in the field and found that the main reasons women leave tech are: advancement opportunities, wage disparity, and work-life balance.
  Because more women entered tech recently, there are more women at lower levels.











3. Compensation analysis
The compensations of tech workers follow a skew normal distribution with a small percentage making an outlier amount of money: e.g. tech workers in India or Russia can make under $20k, while top engineers at Google, Facebook, OpenAI can make millions a year.

Each compensation package consists of 3 components:

  base salary
  equity (stock)
  bonus


3.1 Distribution of Total Yearly Compensation
The median yearly compensation is $195,000, while the mean is $225,000.







This is significantly higher than the results of the StackOverflow survey in 2019, which states that the median salary for Engineering Manager in the US is only $152,000. This might be because of the differences between StackOverflow data and Levels.fyi data:

  StackOverflow data is smaller (14.7k samples)
  StackOverflow data is more fragmented with respondents from all over the world while Levels.fyi data is more US/FAAAM-focused.
  StackOverflow surveyed salary, while Levels.fyi asks for the whole compensation package, which includes base salary, stock grants, and bonus.








3.2 Total Yearly Compensation by Standard Level
As level increases, base salary and bonus increase little, but stock grants increases the most.

Stock grant is often vested over 4 years – if you’re granted 100 shares, then each year 25 shares are vested. Your stock grant is usually refreshed with new shares each year. After your first year, you’re grant 100 extra shares, so next year you can vest 25 old shares + 25 new shares = 50 shares. The longer you stay at a company, the more stock you have left to vest. This structure of compensation is to incentivize employees to stay longer at the company.

Tip: when negotiating with big companies, it’s harder to make substantial change to the base salary offer than to improve the stock grant and bonus offers.







3.3 Compensation by Years of Experience
There’s a medium correlation between income and years of experience. The correlation is stronger for people early in their careers. As you gain more years of experience, your compensation generally improves, until you reach 18 years of experience. The median compensation for those with more than 18 years of experience is actually lower for those with less than 18 years of experience.

For tech workers – software engineers, hardware engineers, and product managers – with more than 15 years of experience, the correlation regresses to slightly below 0.







3.4 Compensation by job
For each job that has at least 350 entries, the median ranges between 180k to 320k. It’s not surprising that SWE manager has the highest median. There’s also a very low percentage of women in SWE Manager (8.75%).

The job with the lowest median income, product designer, coincidentally also has the highest percentage of women.






4. Compensation by gender
4.1 In all data
The distributions of income for men and women in the entire dataset – across all companies, all locations, all levels, all jobs – match each other surprisingly well.

Among all those report their incomes here, the median for male is $11k (6%) higher than that for female. It’s due to more male engineers at the right tail of the distribution (higher compensation).







4.2 By job
The median compensation of women is slightly lower than the median compensation for men for most jobs, except for the role of Data Scientist and Software Engineering Manager.







4.3 By company
The median income of women is less than that of men in all FAAAM companies.







4.4 By standard level
There’s virtually no difference in compensation between men and women at Entry Level Engineer and Software Engineer. However, as the standard level increases, the percentage of women decreases and the compensation gap widens. This might be because at entry levels, women and men are both given standard offers. As levels increase, pay raises require negotiation and according to research, when it comes to negotiating salaries, women are less assertive than men.

I’d also like to restate the disclaimer: just because there’s no evidence in the data to support something, doesn’t mean that something doesn’t exist.







4.5 By years of experience
There’s virtually no difference in compensation between men and women at for those with less than 3 years of experience. The gap starts appearing later.







4.6 Income by gender for those of the same level, same company, same state, same job
When controlled by all the above variables, there’s virtually no difference in compensations at the entrance level for software engineers at the big companies that we analyzed. In the case of Amazon, the median of compensation for female engineers at SDE and SDE II is actually higher than that for male engineers at the same level.

The gap might start appearing after the first two levels. However, we don’t have enough data for female samples to make any meaningful visualization.

Due to the small number of female samples, we sample without replacement from male samples so that we have the same number of samples for men and women.

4.6.1 Software Engineers at Microsoft in Washington
We only analyze SDE and SDE II, the first two levels at Microsoft, because we don’t have enough female samples for higher levels.







4.6.2 Software Engineers at Google in California
We only analyze L3 and L4, the first two levels at Google, because we don’t have enough female samples for higher levels.







4.6.3 Software Engineers at Amazon in both California and Washington
We only analyze SDE I and SDE II, the first two levels at Amazon, because we don’t have enough female samples for higher levels. We use two states instead of one because one doesn’t give us enough data.







5. Conclusion
Some key points:

  The median compensation for software engineers in our data (skewed towards FAAAM, CA/WA-based) starts at around $160k/year for entry level, and reaches around half a million.
  As level increases, the percentage of women decreases both across companies and in each FAAAM company.
  Our data shows that at entry level, there’s virtually no difference between the compensation of men and women. The gap starts appearing only as the level progresses.
  Tech favors the young. For people with more than 15 years of experience, there’s practically no correlation between years of experience and income (corr < 0). After 15 years of experience, you either retire, switch to management, or change career.


I hope that this analysis can guide people in making important career decisions. I don’t wish to make people look at this analysis and compare themselves to the median or whatever statistics there is. Each of us is more than a statistics. Everyone has their own struggle. The average person doesn’t exist.

I also hope that big companies would start publishing analysis of their internal data for transparency, so that we don’t have to rely on anonymous, self-reported data to answer these important questions.

Acknowledgment
I’d like to thank Zaheer Mohiuddin, Bruno Faviero, Andrey Kurenkov, and Paul Marum for giving me helpful feedback. They, however, aren’t responsible for any error that might exist in this piece.


This analysis was done as part of the research for my upcoming Machine Learning Interviews Book.

  
    Like most people overconfident in their tastes in books, I love giving book recommendations. As soon as I hear somebody say: “I’m looking for books to read,” I feel the urge to come flying at them with a pitchfork in one hand and, in the other, a long list of books that I swear will change their life forever, the way they changed mine.

But I restrain myself. Reading is personal. What we enjoy reading depends on our experience, our personality, where we’re in life and where we’re headed. I’ve read somewhere that the worst kind of books are those given to you by other people.

This list isn’t meant to be a recommendation, but a self exposition. These books shaped my last decade, and since it was my coming-of-age, these books shaped who I am. It was compiled in response to the conversations I’ve had with some of my close friends – Paul Warren, Paul Marum, and Lucio Dery. They promised to show me their lists if I show them mine.

What makes a book good
In the last 10 years, I read about 400 books. The average rating I gave these books is 3.95 star, which is within the average range of 3.8 to 4.0 star for books on Goodreads. I enjoyed 78% of the books I read (4 and 5 star). For every three books I chose to read, I loved one of them (5 star).




Distribution of my ratings of books on Goodreads (some missing)



I consider a book good if it has at least one of the four things:

  It gives me one idea that changes the way I see the world and my place in it.
  It takes me on an adventure.
  It lets me get inside someone’s head.
  It’s a masterpiece of prose.


My general principles when choosing and reading a book:

  The popularity of a book correlates poorly with how much I like it.
  I avoid reading more than one book on the same topic by the same author. Subsequent books tend to repeat the first.
  I avoid authors who fall for what I call the Malcolm Gladwell fallacy: they sacrifice the truth for the sake of crafting a narrative.
  I avoid books whose marketing strategies rely on the authors’ names instead of the books’ actual content, e.g. memoirs of celebrities.
  I avoid books on fashionable topics. What is timely is the least likely to stand the test of time.
  I love it when someone spends their entire life on one topic and then writes a book about it.
  Some books follow the 80/20 rule: you can get 80% out of a book by reading 20% of it. The first 20% of The Power Broker is a masterpiece. The rest is just dragging on.
  It’s ok to appreciate someone’s intellect despite their personality.


Based on this list, if you think there are some books I’d enjoy, feel free to make recommendations. I’d love to read the list of books that shaped your decade too.

The books that changed the way I see the world and my place in it
1. The Selfish Gene by Richard Dawkins
This book corrected my misunderstanding of natural selection and validated my existential crisis. There’s no free will.

We are survival machines – robot vehicles blindly programmed to preserve the selfish molecules known as genes.

2. Black Swan and Antifragile by Nassim Nicholas Taleb
There are two types of randomness. The first type is predictable randomness such as when you roll a die. You don’t know for sure if a number will come up but you won’t be surprised if it does. The second type is unpredictable randomness such as the 9/11 attack. If we expected something like that to happen, we would have prevented it. Those events are known as black swans. Their existence invalidates what we consider normal, similar to how the existence of one single black swan invalidates the belief that all swans are white.

We can’t predict black swan events, but we make ourselves resistant to them by making ourselves antifragile. Antifragile is different from robust. Robust is not become weaker in the face of adversity. Antifragile is become stronger in the face of adversity. Curiosity and love are naturally antifragile. The more we try to stop them, the stronger they become.

3. From Third World to First: The Singapore Story: 1965-2000 by Lee Kuan Yew
We faced tremendous odds with an improbable chance of survival. Singapore was not a natural country but man-made, a trading post the British had developed into a nodal point in their worldwide maritime empire. We inherited the island without its hinterland, a heart without a body.

In one generation, Lee Kuan Yew turned Singapore into the country of the future. This book discusses his views and decisions on internal public policies as well as external diplomatic relations.

4. The Righteous Mind: Why Good People Are Divided by Politics and Religion by Jonathan Haidt
Haidt argued that the divergence between left and right ideologies can be explained by his Moral Foundations Theory. According to this theory, there are six foundations to morality:


  Care/harm
  Fairness/cheating
  Liberty/oppression
  Loyalty/betrayal
  Authority/subversion
  Sanctity/degradation


His research shows that liberals base their moral judgments on only the first three foundations, while conservatives use all six. Many on the left even see these last three as the foundations of xenophobia, authoritarianism, and puritanism.

5. Age of Ambition: Chasing Fortune, Truth, and Faith in the New China by Evan Osnos
“One-sixth of the world’s population speaks Chinese. Why are we studying English?” he asked. He turned and gestured to a row of foreign teachers seated glumly behind him. “Because we pity them for not being able to speak Chinese!” The crowd roared.

6. How to change your mind by Michael Pollan
Psychedelic diminishes our sense of self and allows us to merge into something bigger.

Honorable mentions
7. Dataclysm: Who We Are by Christian Rudder
8. A People's History of the United States by Howard Zinn
9. Misbehaving: The Making of Behavioral Economics by Richard H. Thaler
10. On Intelligence by Jeff Hawkins

The books that take me on an adventure
1. The Little Prince by Antoine de Saint-Exupéry
It is the time you have wasted for your rose that makes your rose so important.

2. The Alchemist by Paulo Coelho
It’s the possibility of having a dream come true that makes life interesting.

3. Tippi: My Book of Africa by Tippi Degré






4. The Hitchhiker's Guide to the Galaxy by Douglas Adams
“You know,” said Arthur, “it’s at times like this, when I’m trapped in a Vogon airlock with a man from Betelgeuse, and about to die of asphyxiation in deep space that I really wish I’d listened to what my mother told me when I was young.”
“Why, what did she tell you?”
“I don’t know, I didn’t listen.”

5. The Lord of the Rings by J.R.R. Tolkien
“All that is gold does not glitter,
Not all those who wander are lost”

6. On the Road by Jack Kerouac
… the only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars and in the middle you see the blue centerlight pop and everybody goes “Awww!”

7. One Hundred Years of Solitude by Gabriel García Márquez
The secret of a good old age is simply an honorable pact with solitude.

The books that let me get inside someone's head
1. One Flew Over the Cuckoo's Nest by Ken Kesey
Never before did I realize that mental illness could have the aspect of power, power. Think of it: perhaps the more insane a man is, the more powerful he could become.

2. Man's Search for Meaning by Viktor Frankl
Between stimulus and response there is a space. In that space is our power to choose our response. In our response lies our growth and our freedom.

3. The White Tiger by Aravind Adiga
Iqbal, that great poet, was so right. The moment you recognize what is beautiful in this world, you stop being a slave. To hell with the Naxals and their guns shipped from China. If you taught every poor boy how to paint, that would be the end of the rich in India.

4. The Color Purple by Alice Walker
The Olinka girls do not believe girls should be educated. When I asked a mother why she thought this, she said: “A girl is nothing to herself; only to her husband can she become something.”
“What can she become?” I asked.
“Why, she said, the mother of his children.”
“But I am not the mother of anybody’s children,” I said, “and I am something.”

5. We Need New Names by NoViolet Bulawayo
Look at the children of the land leaving in droves, leaving their own land with bleeding wounds on their bodies and shock on their faces and blood in their hearts and hunger in their stomachs and grief in their footsteps. Leaving their mothers and fathers and children behind, leaving their umbilical cords underneath the soil, leaving the bones of their ancestors in the earth, leaving everything that makes them who and what they are, leaving because it is no longer possible to stay. They will never be the same again because you cannot be the same once you leave behind who and what you are, you just cannot be the same.

6. The Things They Carried by Tim O'Brien
But in a story, which is a kind of dreaming, the dead sometimes smile and sit up and return to the world.

7. Shooting an Elephant by George Orwell
And it was at this moment, as I stood there with the rifle in my hands, that I first grasped the hollowness, the futility of the white man’s dominion in the East. Here was I, the white man with his gun, standing in front of the unarmed native crowd – seemingly the leading actor of the piece; but in reality I was only an absurd puppet pushed to and fro by the will of those yellow faces behind. I perceived in this moment that when the white man turns tyrant it is his own freedom that he destroys. He becomes a sort of hollow, posing dummy, the conventionalized figure of a sahib.

8. "Surely You're Joking, Mr. Feynman!": Adventures of a Curious Character by Richard P. Feynman
I began to read the paper. It kept talking about extensors and flexors, the gastrocnemius muscle, and so on. This and that muscle were named, but I hadn’t the foggiest idea of where they were located in relation to the nerves or to the cat. So I went to the librarian in the biology section and asked her if she could find me a map of the cat.
“A map of the cat, sir?” she asked, horrified. “You mean a zoological chart!” From then on there were rumors about some dumb biology graduate student who was looking for a “map of the cat.”
When it came time for me to give my talk on the subject, I started off by drawing an outline of the cat and began to name the various muscles.
The other students in the class interrupt me: “We know all that!”
“Oh,” I say, “you do? Then no wonder I can catch up with you so fast after you’ve had four years of biology.” They had wasted all their time memorizing stuff like that, when it could be looked up in fifteen minutes.

The books that are masterpieces of prose
1. Lolita by Vladimir Nabokov
Lolita, light of my life, fire of my loins.

2. Stiff: The Curious Lives of Human Cadavers by Mary Roach
The way I see it, being dead is not terribly far off from being on a cruise ship. Most of your time is spent lying on your back. The brain has shut down. The flesh begins to soften. Nothing much new happens, and nothing is expected of you.

3. Calypso by David Sedaris
…for we were middle-class and I’d been raised to believe that our social status inoculated us against severe misfortune. A person might be broke from time to time—who wasn’t?—but you could never be poor the way that actual poor people were: poor with lice and missing teeth … At what point had I realized that class couldn’t save you, that addiction or mental illness didn’t care whether you’d taken piano lessons or spent a summer in Europe?

4. The Liars' Club by Mary Karr
So over the years, Daddy and I grew abstract to each other. We knew each other in theory and loved in theory. But if placed in proximity - when I came home, say - any room we sat in would eventually fall into a soul-sucking quiet I could hardly stand.

5. Angela's Ashes by Frank McCourt
He says, you have to study and learn so that you can make up your own mind about history and everything else but you can’t make up an empty mind. Stock your mind, stock your mind. You might be poor, your shoes might be broken, but your mind is a palace.

  
    [Twitter thread]

Last year, I wrote Career advice for recent Computer Science graduates about the decision-making process I went over when choosing my first full-time job after college. Now that I’ve just left that first job, I want to share some lessons that I wish I knew when I started.

1. It's all about the people

Before starting working, I had a naive expectation that my social life would carry on from college into the real world – all the friends I took for granted in school would continue living within a 5-minute walk from me and being available whenever I needed them.

The transition was hard. Many of my closest friends moved away. Some stuck around, but commute and work and adulthood got in the way and we never seem able to find time to see each other.

In the disappointment of it all, my coworkers stepped in and filled the gap.

I didn’t think about it when choosing a team – professional life is supposed to be orthogonal to personal life, right? – but I lucked out. I found on my team the two things that matter the most fresh out of college: friendship and mentorship.

1.1 Friendships
When I first joined, everyone else on my team was already married with kids. People with kids don’t have time to deal with my post-college angst. As the team grew, however, I found a group of coworkers who were also going through the same transition. We had lunch together at work every day. On the weekends, we did board game nights, BBQs, potlucks. We even spent one weekend together in Monterey.

Some of my coworkers became more friends than coworkers. I can’t pretend that there weren’t any moments I was frustrated at work, either because of work itself or something else going on in my life. Having a friend nearby who I could talk to with total trust made a big difference.

1.2 Mentorship
A more senior member on our team took me under his wings. He became my biggest advocate and taught me so much, from writing papers to handling corporate politics – inevitable in any big corporation – to choosing what waterproof shoes to buy. I wish I had been more proactive in learning from him.

I’m surprised that many don’t have a mentor for their first job. It’s not that they don’t want one. They just never ask. When starting a new job, you should choose a coworker with whom you get along and ask them to be your mentor. If they say no, try again with someone else. Most people are receptive to being a mentor – it’s flattering to be asked. Check in with them once a month to talk over what you’re working on, understand what they’re working on, and get their thoughts on your progress. Your managers can be your mentors too, but it’s nice to have a second opinion.

I first learned this lesson at the beginning of the decade during my travels: “Traveling is not about where you are, but who you are with.” I learned this lesson again at the end of the decade. It’s not about the company. It’s not about the product. It’s not even about the money. It’s all about the people you work with.

Takeaways

  Find a team that has people who are in the same life stage as you.
  Ask a coworker to be your mentor.


2. It's important to measure progress
In school, we know how we’re supposed to progress: we finish one year and move onto the next. If you unexpectedly fail an exam, you know you need to reevaluate your priorities. After graduation, I had this fear that I’d keep on making bad life choices and have no way to find out until it’s too late.

To avoid it, I decided that I needed three things:

  well-defined goals
  a way to evaluate whether I’ve met those goals
  an early warning system to let me know when I veer off track


My specific goals and metrics are too personal to share, but my goals are along three main axes: become a better engineer, become a better writer, and maximize my future career options.

In my professional life, one metric I used was NVIDIA’s internal level structure and feedback loop. My manager was kind enough to explain what he needed to see from me to move me to the next level. If your manager doesn’t bring it up, you should schedule an 1:1 with them to talk about it. Discuss with them your possible career trajectories within and outside the organization, how long it usually takes for someone with your background to move to the next level.

I also asked my manager what gaps in my skills and knowledge he thinks I should focus on. Most people are trained to always tell you you’re doing a great job, so you need to frame your questions and attitude in a way that encourages honest feedback. It takes courage to give critical feedback. When you receive some, accept it with grace, even if it’s something you don’t want to hear.

In my personal life, I had an accountability pact with Paul Warren, one of my closest friends. Last June, I told him what I’d been up and he told me: “Neighbor, I’ve known you for five years and I’ve never seen you this unproductive.” Some might find it harsh, but it was what I needed to hear. This conversation prompted me to reexamine how I’d been spending my time, reevaluate my priorities, and work on my bad habits.

Realizing how helpful the conversation was for both of us, Paul and I decided to check in every two weeks to make sure we’re doing what we say we’ll do. We want to help each other live better, not just work better, so we talk about everything from work to relationships. It’s reassuring to have someone you trust follow your progress in life to make sure that you’re on track. I’m also glad to have Paul back as a stable figure in my life.

Takeaways

  Have well-defined goals.
  Talk to your manager about your career paths, how long it usually takes for someone to move to the next level, and what they need to see from you to move you to the next level.
  Have an accountability friend. Schedule to check in with them once or twice a month.


3. Structure is good
In school, I hated how structured my time was. My weekly schedule was always the same. My quarters were planned around exams and deadlines. I couldn’t wait to not have classes anymore so I could sleep whenever I wanted.

As a typical software engineer at a typical big tech, I had a lot of freedom in scheduling my days. Most tech companies in the Bay don’t care where and when you do your job, as long as you get it done. When I started, I took full advantage of it. If I wanted to sleep in, I’d sleep in and add a few extra work hours to the end of my day. If I didn’t want to change out of my pajamas, I’d work from home.

I liked it, but I couldn’t get anything done. The more options I have, the more time I spend on evaluating them. It took me forever to schedule anything. I kept switching from one task to another whenever I was bored, wasting time on context switching. My sleeping schedule was so erratic I had a hard time falling asleep and was always tired.

Then I decided to subject myself to something I never thought I would: daily routines. I go to work, leave work, and sleep at the same time every day. Scheduling meetings becomes much easier. My sleep improves. I became more disciplined and more productive. I even managed to carve out 40 minutes each day – during my commute – to read books. No email, no social media, no reviewing code. The whole world with all its problems ceases to exist. The only thing that matters in those precious minutes is my own little universe.

As I talked to people about this change, many have also told me that they’ve come to appreciate having some structure in their lives. A friend chose to live far enough from the office so that he has to take the same shuttle at the same time each day to and from work. Another told me that following a routine gives him peace of mind – as long as he can still have his latte at 9am the world can’t be falling apart. Routines allow us to go on autopilot for many daily tasks, freeing up our mental energy for things that matter.

Takeaways

  Sleep at the same time.
  Carve out time for things you enjoy doing, so that you have something to look forward to everyday.
  Don’t forget to exercise.


4. Your job doesn't define your self-worth
Recently, I had a great conversation with someone I met at a group dinner. When I told my friend about it, she asked: “What does he do?” I have no idea. I never asked.

It bothers me that the first thing we ask when meeting someone is their job, and the first thing we tell other people about ourselves is where we work. Within two minutes of meeting someone, we get their entire employment history: where they are now, what they did before, where they went to school.

This reduces each person’s self-worth to their resume. This makes us lazy – we don’t feel the need to get to know someone now that we can fill in the missing picture with the stereotypes we’ve already formed about their jobs.

On top of that, if we keep introducing ourselves by our jobs, we let our jobs define ourselves. When our jobs don’t go well or we don’t have one, we feel like we’re nothing.

People ask me that if I don’t talk about jobs, what do I talk about? Everything else. I like to know what people are thinking about, what problems they see that don’t have the solutions to yet, what they find underrated, what technology/book/movie/music/etc. changed their perspectives. My goal when meeting someone new is to figure out what they know that I don’t.

I also use the opportunity to get a new perspective on what I’ve been thinking about. If I’ve been thinking about machine learning interviews, I’d ask them about their interview experiences, both as a candidate and an interviewer. If I’m writing about technical writing, I’d ask about their writing process and if there’s any piece of technical writing that they consider exemplary.

It’s hard to get people to open up right away, so you need to ask the right questions to warm them up. It’s a skill that I’m still learning and practicing. Sometimes I fail and people think I’m a weirdo. But when I succeed, I have a great time, learn something new, even gain a new friend.

I’d like to add a reminder that your career doesn’t have to be just your job. Many opportunities I get come from what I do outside my job – people know me from my side projects, companies invite me to give talks about a topic I’ve written about. I separate what I do for my job from what I do for myself, taking advantage of the time I haven’t sold to my employer yet to work on what interests me. When you have a career outside of your job, you have more leverage at work, which allows more job flexibility.

Takeaways

  Get to know people for who they are, not just their jobs. Make friends, not connections.
  No matter how well your job pays, you’re still selling your time for less than what it’s worth. Preserve your time to work for yourself.
  Do side projects.
  Have a career outside of your job.


What's next?
Last year, I chose to join NVIDIA because I thought it was a great company. My time there reinforced that notion. I’m grateful for having the chance to work with so many wonderful people on challenging projects.

I knew it’d be hard to say goodbye, but I didn’t expect it to be that hard. As I turned in my badge, I was overwhelmed by a sense of loss. One of my coworkers had tears in her eyes. Another refused to believe that I was leaving. Jensen Huang was at my farewell party (by chance) and was really nice about it. It hurt me when my teammates asked: “Are you leaving because you don’t like us?”

I love my teammates. I wish I had more time to work with them. However, as I’m still trying to learn as much as I can, I need a different environment to learn a different set of skills.

I’m joining an early stage startup that focuses on the machine learning production pipeline. We’re still in stealth so there’s not much I can say, except that the team is of a caliber that I’ve never seen at a startup. They are a rare breed of researchers with excellent engineering skills. I’m honored and excited to be part of the team. I can’t wait to show you what we’ve been working on!

P.S. Yes, we’re hiring. Hit me up if you want to chat :-)

Acknowledgment
Thanks Ben Krause for being an amazing first reader! Thanks Lifan Zeng for helpful feedback!

  
    [Twitter thread]

With 51 workshops, 1428 accepted papers, and 13k attendees, saying that NeurIPS is overwhelming is an understatement. I did my best to summarize the key trends I got from the conference. This post is generously edited by the wonderful Andrey Kurenkov.

Disclaimer: This post doesn’t reflect the view of any of the organizations I’m associated with. NeurIPS is huge with a lot to take in, so I might get something wrong. Feedback is welcome!

Table of Contents

  Deconstructing the deep learning black box
  New approaches to deep learning
2.1 Deep learning with Bayesian principles
2.2 Graph neural networks
2.3 Convex optimization
  Neuroscience x Machine Learning
  Keyword analysis
  NeurIPS by numbers
  Conclusion


1. Deconstructing the deep learning black box

Lately, there has been a lot of reflection on the limitations of deep learning. A few examples:

  Facebook’s director of AI is worried about the computational wall. Companies should not expect to keep making progress just with bigger deep learning systems because “right now, an experiment might be in seven figures, but it’s not going to go to nine or ten figures .. nobody can afford that.”
  Yoshua Bengio gave Gary Marcus as an example of someone who frequently points out deep learning’s limitations. Bengio summarized Gary Marcus’s view as “Look, deep learning doesn’t work.” Gary Marcus disputed this characterization.
  Yann Lecun addressed this trend: “I don’t get why, all of a sudden, we read all these stories and tweets that claim ‘progress in AI is slowing’ and ‘Deep Learning is hitting a wall’ … I have been pointing out those two limitations and challenges in pretty much every single one of my talks of the last 5 years … So, no, the identification of these limitations is not new. And there is no slow down.”


In this climate, it’s nice to see an explosion of papers exploring the theory behind deep learning, why and how it works. At this year’s NeurIPS, there are 31 papers on the convergence of various techniques. The outstanding new directions paper award goes to Vaishnavh Nagarajan and J. Zico Kolter’s Uniform convergence may be unable to explain generalization in deep learning, with the thesis that the uniform convergence theory by itself can’t explain the ability of deep learning to generalize. As the dataset size increases, theoretical bounds on generalization gap (the gap between a model’s performance on seen and unseen data) also increase while the empirical generalization gap decreases.




Image from Vaishnavh Nagarajan's oral presentation



The neural tangent kernel (NTK) is a recent research direction which aims to understand the optimization and generalization of neural networks. It came up in several spotlight talks and many conversations I had with people at NeuIPS. By building on the well-known notion that fully connected neural nets are equivalent to Gaussian processes in the infinite-width limit, Arthur Jacot et al. were able to study their training dynamics in function space instead of parameter space. They proved that “during gradient descent on the parameters of an ANN, the network function (which maps input vectors to output vectors) follows the kernel gradient of the functional cost w.r.t. a new kernel: the NTK.” They also showed that when a finite layer version of the NTK is trained with gradient descent, its performance converges to the infinite-width limit NTK and then stays constant during training.

Some NeurIPS papers that build upon NTK:

  Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers
  On the Inductive Bias of Neural Tangent Kernels


However, many believe that NTK can’t fully explain deep learning. The hyperparameter settings needed for a neural net to approach the NTK regime – small learning rate, large initialization, no weight decay – aren’t often used to train neural networks in practice. The NTK perspective also states that neural networks would only generalize as well as kernel methods, but empirically they have been observed to generalize better.

Colin Wei et. al’s paper Regularization Matters: Generalization and Optimization of Neural Nets v.s. their Induced Kernel theoretically demonstrates that neural networks with weight decay can generalize much better than NTK, suggesting that studying L2-regularized neural networks could offer better insights into generalization. The following papers from this NeurIPS also show that conventional neural networks can outperform NTK:

  What Can ResNet Learn Efficiently, Going Beyond Kernels?
  Limitations of Lazy Training of Two-layers Neural Network


Many papers analyze the behavior of different components of neural networks. Chulhee Yun et al. presented Small ReLU networks are powerful memorizers: a tight analysis of memorization capacity and showed that “3-layer ReLU networks with Omega(sqrt(N)) hidden nodes can perfectly memorize most datasets with N points.”

Shirin Jalali et al.’s paper Efficient Deep Learning of Gaussian Mixture Models started with the question: “The universal approximation theorem states that any regular function can be approximated closely using a single hidden layer neural network. Can depth make it more efficient?” They showed that in the case of optimal Bayesian classification of Gaussian Mixture Models, such functions can be approximated with arbitrary precision using O(exp(n)) nodes in a neural network with one hidden layer, but only O(n) nodes in a two-layer network.

In one of the more practical papers, Control Batch Size and Learning Rate to Generalize Well: Theoretical and Empirical Evidence, Fengxiang He and team trained 1,600 ResNet-110 and VGG-19 models using SGD on CIFAR datasets and found that the generalization capacity of these models correlate negatively with batch size, positively with learning rate, and negatively with batch size/learning rate ratio.




Image by He et al.



At the same time, Yuanzhi Li et al.’s Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks states that “a two layer network trained with large initial learning rate and annealing provably generalizes better than the same network trained with a small learning rate from the start … because the small learning rate model first memorizes low noise, hard-to-fit patterns, it generalizes worse on higher noise, easier-to-fit patterns than its large learning rate counterpart.”

While these theoretical analyses are fascinating and important, it can be hard to aggregate them into a big picture because each of them focuses on one narrow aspect of the system.

2. New approaches to deep learning

NeurIPS this year featured a wide range of methods outside stacking layers on top of each other. The three directions I’m excited about: Bayesian learning, graph neural networks, and convex optimization.

2.1 Deep learning with Bayesian principles

Bayesian learning and deep learning are very different, as highlighted by Emtiyaz Khan in his invited talk Deep Learning with Bayesian Principles. According to Khan, deep learning uses a ‘trial and error’ approach – let’s see where experiments take us – whereas Bayesian principles force you to think of a hypothesis (priors) beforehand.







Compared to regular deep learning, there are two main appeals to Bayesian deep learning: uncertainty estimation and better generalization on small datasets. In real world applications, it’s not enough that a system makes a prediction. It’s important to know how certain it’s about each prediction. For example, a prediction of cancer with 50.1% certainty requires different treatments from the same prediction with 99.9% certainty. With Bayesian learning, uncertainty estimation is a built-in feature.

Traditional neural networks give single point estimates – they output a prediction on a datapoint using a single set of weights. Bayesian neural networks, on the other hand, use a probability distribution over the network weights and output an average prediction of all sets of weights in that distribution, which has the same effect as averaging over many neural networks. Thus, Bayesian neural networks are natural ensembles, which act like regularization and can prevent overfitting.

Bayesian neural networks with millions of parameters are still computationally expensive to train. Converging to a posterior might take weeks, so approximating methods such as variational inference have become popular. The Probabilistic Methods – Variational Inference session features 10 papers on this variational Bayesian approach.

Some NeurIPS papers on Bayesian deep learning that I enjoyed reading:

  Importance Weighted Hierarchical Variational Inference
  A Simple Baseline for Bayesian Uncertainty in Deep Learning
  Practical Deep Learning with Bayesian Principles


2.2 Graph neural networks (GNNs)

For years, I’ve been talking about how graph theory is among the most underrated topics in machine learning. I’m glad to see graphs are all the rage at NeurIPS this year.


    Graph representation learning is the most popular workshop of the day at #NeurIPS2019 . Amazing how far the field has advanced. I did not imagine so many people would get into this when I started working on graph neural nets back in 2015 during an internship. Time flies... pic.twitter.com/H8LM0BMvK4— Yujia Li (@liyuajia) December 13, 2019 


Graphs are beautiful and natural representations for many types of data such as social networks, knowledge bases, game states. The user-item data used for recommendation systems can be represented as a bipartite graph in which one disjoint set consists of users and another consists of items.

Graphs can also represent outputs of neural networks. As Yoshua Bengio reminded us in his invited talk, any joint distribution can be represented as a factor graph.

This makes graph neural networks perfect for tasks such as combinatorial optimization (e.g. travelling salesman, scheduling), identity matching (is this Twitter user the same as this Facebook user?), recommendation systems.

The most popular graph neural network is graph convolutional neural network (GCNN), which is expected since they both encode local information. Convolutions encode a bias towards finding relationships between neighboring parts of the inputs. Graphs encode the most closely related parts of the input via edges.




Image by Gasse et al.



Some papers on GNNs that I like:

  Exact Combinatorial Optimization with Graph Convolutional Neural Networks
  Yes, there’s a paper that fuses two hottest trends this year: Graph Neural Tangent Kernel: Fusing Graph Neural Networks with Graph Kernels
  My favorite poster presentation at NeurIPS: (Nearly) Efficient Algorithms for the Graph Matching Problem on Correlated Random Graphs








Recommended reading outside NeurIPS:

  Thomas N. Kipf’s Graph Convolutional Networks blog post
  Kung-Hsiang, Huang’s A Gentle Introduction to Graph Neural Networks (Basics, DeepWalk, and GraphSage)


2.3 Convex optimization

I’ve low-key worshipped Stephen Boyd’s work on convex optimization from afar, so it was delightful to see it getting more popular at NeurIPS – there are 32 papers related to the topic (1, 2). Stephen Boyd and J. Zico Kolter’s labs also presented their paper Differentiable Convex Optimization Layers which showed how to differentiate through the solutions of convex optimization problems, making it possible to embed them in differentiable programs (e.g. neural networks) and learn them from data.

Convex optimization problems are appealing because they can be solved exactly (1e-10 error tolerance is attainable) and quickly. They also don’t produce weird/unexpected outputs, which is crucial for real-world applications. Even though many problems encountered in the wild are nonconvex, decomposing them into a sequence of convex problems can work well.

Neural networks are also trained using algorithms for convex optimization. However, while the emphasis in neural networks is to learn things from scratch, in an end-to-end fashion, applications of convex optimization problems emphasize modeling systems explicitly, using domain-specific knowledge. When it’s possible to model a system explicitly, in a convex way, usually much less data is needed. The work on differentiable convex optimization layers is one way to mix the benefits of end-to-end learning and explicit modeling.

Convex optimization is especially useful when you want to control a system’s outputs. For example, SpaceX uses convex optimization to land its rockets, and BlackRock uses it for trading algorithms. It’d be really cool to see convex optimization used in deep learning, the way Bayesian learning is now.

Some NeurIPS papers on convex optimization recommended by Akshay Agrawal.

  Acceleration via Symplectic Discretization of High-Resolution Differential Equations
  Hamiltonian descent for composite objectives








3. Neuroscience x Machine Learning

According to the analysis of NeurIPS 2019 Program Chair Hugo Larochelle, the category that got the highest pump in acceptance rate is Neuroscience. In their invited talks, both Yoshua Bengio (From System 1 Deep Learning to System 2 Deep Learning) and Blaise Aguera y Arcas (Social Intelligence) urged the machine learning community to think more about the biological roots of natural intelligence.







Bengio’s talk introduced consciousness into the mainstream machine learning vocabulary. The core ingredient of Bengio’s consciousness is attention. He compared machine attention mechanism to the way our brains choose what to pay attention to: “Machine learning can be used to help brain scientists better understand consciousness, but what we understand of consciousness can also help machine learning develop better capabilities.” According to Bengio, a consciousness-inspired approach is the way to go if we want machine learning algorithms that can generalize to out-of-distribution samples.







Aguera y Arcas’s talk is my favorite at the conference. It’s theoretically rigorous but practical. He argued how optimization is inadequate to capture human-like intelligence: “Optimization is not how life works… Brains don’t just evaluate a function. They develop. They’re self-modifying. They learn through experience. A function doesn’t have those things.” He called for “a more general, biologically inspired synapse update rule that allows but doesn’t require a loss function and gradient descent.”

This NeurIPS trend is consistent with my observation that many people in AI are moving to neuroscience. They bring neuroscience back into machine learning.


    Some of the smartest people I know are leaving AI research for engineering/neuroscience. Their reasons?1. We need to understand how humans learn to teach machines to learn.2. Research should be hypothesis -> experiments, but AI research rn is experiments -> justifying results.— Chip Huyen (@chipro) November 15, 2019 


4. Keyword analysis

Let’s check out a more global view of what papers at the conference were about. I first visualized all the 1,011 paper titles from NeurIPS 2018 and 1,428 papers titles from NeurIPS 2019 with vennclouds. The black area in the middle is the list of words that are common in both 2018 and 2019 papers.







I then measured the proportional percentage change of these keywords from 2018 to 2019. For example, if in 2018, 1% of all the accepted papers have the keyword ‘X’ and in 2019, that number is 2%, then the proportional change is (2 - 1) / 1 = 100%. I plotted the keywords with the absolute proportional change of at least 20%.







Key points:

  Reinforcement learning is gaining ground even outside robotics. Some of the keywords with significant positive change are bandit, feedback, regret, control.
  Generative models are still popular. GANs still capture our imagination, but are much less hype-y.
  Recurrent and convolutional neural networks are literally so last year.
  Hardware keyword is also on the rise, signaling more hardware-aware algorithms. This is the answer to the concern that hardware is the bottleneck in machine learning.
  I’m sad that data is on the decline. I was so excited to check out the Algorithms – Missing Data poster session only to find out it has one poster Missing Not at Random in Matrix Completion: The Effectiveness of Estimating Missingness Probabilities Under a Low Nuclear Norm Assumption.
  Meta sees the highest increase in proportion this year. Meta-meme by Jesse Mu.








  Even though Bayesian goes down, uncertainty goes up. Last year, there were many papers that use Bayesian principles, but not for deep learning.


5. NeurIPS by numbers


  7k papers submitted to the main conference. 1,428 accepted papers. 21% acceptance rate.
  13k attendees, which, by my estimation, means at least half of all attendees don’t present a paper there.
  57 workshops, 4 focused on inclusion: Black in AI, Women in Machine Learning, LatinX in AI, Queer in AI, New In Machine Learning, Machine Learning Competitions for All.
  16k pages of conference proceedings.
  12% of all the accepted papers have at least one author from Google or Deepmind.
  87 papers are from Stanford, making it the academic institution with the highest number of accepted papers.


    Stuck at the airport so I browsed #neurips2019 papers. Out of 1429 accepted papers, 167 (~12%) have at least one author from Google/DeepMind, same as Microsoft, Facebook, IBM, & Amazon combined.Is there any stats on the % of reviewers who are Google/DeepMind affiliated? pic.twitter.com/bXPoB135PA— Chip Huyen (@chipro) September 7, 2019 


  250 papers, 16.7%, are on applications.
  648 is the number of citations as of Dec 2019 for the test of time award paper, Lin Xiao’s Dual Averaging Method for Regularized Stochastic Learning and Online Optimization. This is a proof that citation count and contribution don’t necessarily correlate.
  75% of the papers with link to code in the camera-ready version, compared to only 50% from last year.
  2,255 reviews mention looking at the code accompanying submission.
  173 papers claimed for the Reproducibility Challenge on OpenReview.
  31 posters at the NeurIPS Workshop on Machine Learning for Creativity and Design. Several people have told me it’s their favorite part of NeurIPS. Shout out to Good Kid the band for their amazing performance at the closing banquet. Check out their music on Spotify if you haven’t already!



Some days they are ML researchers, other days they are rockstars; tonight they are both! - Good Kid Band pic.twitter.com/Y8Y7eYrGkI— Daniel Nkemelu (@DanielNkemelu) December 15, 2019 



  11 talks at the first Retrospectives: A Venue for Self-Reflection in ML Research workshop, which was another favorite.


The atmosphere of NeurIPS is captured well by atteendess on Twitter.


    A fun little demonstration of the scale of #NeurIPS2019 - video of people heading into keynote talk.This is 9 minutes condensed down to 15 seconds, and this is not even close to all the attendees! pic.twitter.com/1VqAHZoqtj— Andrey Kurenkov 🤖 @ Neurips (@andrey_kurenkov) December 12, 2019 


    I went to NeurIPS for the first time in 2015. I believe it was in Montreal. I wrote about my terrible experience and was going to share anonymously but friends told me that it would be obvious who wrote it :) 4 years later, this conference has changed a lot 1\— Timnit Gebru (@timnitGebru) December 15, 2019 


Conclusion
I found NeurIPS overwhelming, both knowledge-wise and people-wise. I don’t think anyone can read 16k pages of the conference proceedings. The poster sessions are oversubscribed which makes it hard to talk to the authors. I undoubtedly missed out on a lot. If there’s a paper you think I should read, please let me know!

However, the massiveness of the conference also means a confluence of many research directions and people to talk to. It was nice to be exposed to work outside my subfield and learn from researchers whose backgrounds and interests are different from my own.

It was also great to see the research community moving away from the ‘bigger, better’ approach. My impression walking around the poster sessions is that many papers only experimented on small datasets such as MNIST and CIFAR. The best paper award, Ilias Diakonikolas et al.’s Distribution-Independent PAC Learning of Halfspaces with Massart Noise, doesn’t have any experiment.

I’ve often heard young researchers worrying over having to join big research labs to get access to computing resources, but NeurIPS proves that you can make important contributions without throwing data and compute at a problem.

During a NewInML panel that I was a part of, someone said he didn’t see how the majority of papers at NeurIPS could be used in production. Neil Lawrence said maybe he should look into other conferences. NeurIPS is more theoretical than many other machine learning conferences, and it’s okay. It’s important to pursue fundamental research.

Overall, I had a great time at NeurIPS, and am planning on attending it again next year. However, for those new in machine learning, I’d recommend ICLR as their first academic conference. It’s smaller, shorter, and more application-oriented. Next year, ICLR will be in Ethiopia, an amazing country to visit.

Acknowledgment
I’d like to thank Akshay Agrawal, Andrey Kurenkov, and Colin Wei for helping me with this draft. Fun fact: Colin Wei is one of a few PhD candidates with 4 papers accepted at NeurIPS this year – 3 of them made spotlight!

  
    [Twitter thread, Reddit discussion]

During the process of working on my book on machine learning interviews, one question that I’ve been asked many times by candidates is the onsite-to-offer ratio, e.g. what percentage of people who interview at a company eventually get an offer. Another number that recruiters and hiring managers are keen to know is the yield rate on offers of different companies – e.g. what percentage of people who are extended an offer by a company accept that offer?

Those numbers are undoubtedly tracked by HR within each company, but as far as I know, no company volunteers this information. I decided to approximate those numbers using the largest public aggregator of interviews that I know of: Glassdoor. This is far from a perfect resource because of the following biases:


  few people actually leave reviews for anything online
  those who do are likely compelled by either a really good or really bad experience
  those who receive offers are more likely to give reviews than those who don’t
  those who accept offers are more likely to give reviews than those who decline
  junior candidates are more likely to give reviews than senior candidates


However, hopefully a large number of reviews might be able to smoothen out some of the noise. Moreover, if all reviews suffer from the same biases, they are still useful for comparison across companies.

This post consists of 6 parts:


  Data: description of the dataset.
  Interview results: onsite-to-offer ratio and offer yield rate.
  Sourcing candidates: how candidates are recruited for onsite interviews. The sources include campus recruiting, online application, referrals, staffing agency, and inhouse recruiters.
  Interview experience: which company gives candidates the best and the worst experiences.
  Interview difficulty: which company has the most difficult interviews.
  What candidates say about each company.


Data

I focus only on software engineering related roles such as Software Engineer and Data Scientist, for both junior and senior levels (though the majority is junior). For each review, the following information is collected:


  Result: No offer / declined offer / accepted offer
  Difficulty: Easy interview / medium interview / hard interview
  Experience: Negative / neutral / positive
  Review: including application process, interview process, interview questions


I discarded all reviews with missing values. I was able to gather 15,897 reviews from 27 major tech companies with at least 100 reviews for software engineering related roles. Technically, a review can come from a candidate at any point in the interviewing process, but review samples suggest that most of the reviews are from candidates that have done onsites, so interviews in this analysis will be treated as such.

Below is the aggregated count for all SWE-related roles at each company. The number of reviews seem proportional to the number of SWEs each company hires. The largest SWE employers are, unsurprisingly, Google, Amazon, Facebook, and Microsoft. The relatively small number of reviews for Apple and Netflix compared to other FAANG companies are because:


  Apple is not a software company.
  Netflix is actually not that big (~6,000 employees compared to ~40,000 employees at Facebook).


You can see the raw count of reviews for each role at each company here.




Glassdoor software engineering interview count for major tech companies



The first thing to notice from this dataset is that interview feedback varies widely for different roles even within the same companies. For example, only 25.4% of candidates for the role of Data Scientist at Facebook find their interviews difficult, but that number is 36.4% for the role of Senior Software Engineer. Similarly, only 51% of candidates for the role of Senior Software Engineer have a positive experience of their interview process, but 69.5% of candidates for Data Engineer enjoy the process.







The variability is due to the fact that different roles have different recruiting pipelines (e.g. senior roles rely more on recruiters cold-emailing while junior roles rely more on campus recruiting), different expectations, and different interviewing processes. Some companies have the same process for the entire company and new hires go through a team matching process after joining, but most companies let each team conduct their hiring separately, which leads to more variations within the same company.

Before you continue reading this post, I hope that you take all the biases and variations in these reviews into account. The findings are not conclusive and are up for discussion.

Interview results

The first thing I wanted to know is onsite-to-offer ratios (the percentage of onsites that lead to offers) and offer yield rates (the percentage of offers that are accepted by candidates).







From the graph, 18.83% of onsite candidates at Google get offers, and out of all those with offers, 70% accept their offers. Due to the biases of online reviews listed above, we should expect the actual numbers to be much lower. From talking to recruiters I know + reading sources online, I gather that the onsite-to-offer ratios here are a few percentage points higher than the actual percentages. E.g. this and this claim that the onsite-to-offer ratio for Google is 10-20% and Amazon 20% (keep in mind that they are both from anonymous people on the Internet). Similarly, the offer yield rate of near 90% is unheard of. Recruiters have told me that if you can get 80% of candidates to accept your offers, you’re golden.

The 10 companies with the lowest onsite-to-offer ratios are all Internet giants (Yelp, Google, Facebook, Airbnb, Amazon, etc.) They are also known to be highly selective companies. Companies with high onsite-to-offer ratios aren’t necessarily unselective. They might be more selective during the screening process and only interview candidates that they already know or really like. Onsites are costly, so the higher the onsite-to-offer ratio, the more financially sound the process.

There’s a very strong correlation between the onsite-to-offer ratio and the offer yield rate. If we consider all 27 companies, the correlation is 0.81. If we consider only 10 companies with at least 300 reviews (Apple, Uber, Amazon, Google, Microsoft, Oracle, Yelp, Cisco, IBM, Facebook), the correlation is even higher, 0.89. This means that the higher the onsite-to-offer ratio, the higher the yield rate. This makes sense because if a candidate passes the interview process at selective companies like Google or Facebook, they probably have other attractive offers to choose from. Those selective companies are also known to make competitive offers, which candidates can use to negotiate with companies that they really want to work for.

Sourcing candidates

The next thing I look into is how companies source candidates. Every single hiring manager I’ve talked to told me that referrals matter a lot. But exactly how much referrals matter vary from company to company, from role to role. Referrals for senior roles matter a lot more referrals for junior roles, which are due to several reasons. First, junior referrals likely come from junior engineers who are less experienced at judging their friends’ abilities. Second, senior roles are expected to shoulder more leadership responsibilities, so personality and culture fit are much more important.

For junior roles, about 10 - 20% of candidates that get to onsites are referred, with Uber leading the chart with almost 30%. For senior roles, that numbers are higher. Salesforce, Uber, and Cisco all have approximately 30% of their senior onsite candidates referred. If we take into account the fact that the number of applicants who are referred into a company is minuscule compared to the number of applicants who apply to the same company by other means, the chance of a referred applicant is so much higher than the chance of a random candidate.











For junior roles, the biggest source for onsite candidates is campus recruiting. Microsoft and Oracle have more than half of their interviewees recruited through campus events such as career fairs and tech talks. Hot Internet companies like Google, Facebook, and Airbnb rely less on campus recruiting, but it still accounts for between 20 and 30% of their onsites. This means that big tech companies concentrate a huge chunk of their recruiting effort to a handful of popular engineering schools (aka the Tech Ivy: Stanford, UC Berkeley, MIT, Caltech, CMU, University of Toronto, University of Waterloo). The students recruited from those schools then refer their former classmates, who in turn refer event more of their former classmates. The circle goes on, turning those major tech companies into a Tech Ivy alumni mixer.

It makes sense for recruiters to target their most promising sources, but it’s statistically gloomy for candidates who don’t go to a popular engineering college or don’t go to college at all. If you’re one of those candidates, my personal suggestion is build up an impossible-to-ignore online portfolio and wait for recruiters to throw themselves at you. Write in-depth technical blog posts or papers, contribute to open-source projects, do hackathons, compete in coding competitions, and most importantly, publish your work so that the world knows what an amazing engineer you are. Around 15 to 25% of onsite candidates for junior roles are sourced by recruiters. For senior roles, these numbers double. If you’ve had years of experience but have no relevant skills/portfolio to attract recruiters or no friends to refer you, you’re in trouble.

If all else fails, submit your applications and hope for the best. Companies that are the friendliest to online applicants are Twitter, Amazon, and Airbnb with roughly half of the onsites being online applicants. Companies among the most likely to say ‘thank you next’ to hopeful online applicants are Facebook, Microsoft, and Oracle.

Interview experience
Everyone complains that the interview process is broken. It’s not entirely true, at least from the perspective of the candidates who get interviews. 60% of candidates report positive interview experience.







As a company best known for providing sale solutions, Salesforce is apparently very good at selling themselves to candidates. Candidates interviewing at Salesforce are most likely to report positive experiences. Other companies high up in the list include Intel, Adobe, and SAP. The two companies that seem to give candidates the worst interview experience are Netflix and Snap – only one-third of their candidates report positive. However, they are also the two companies with the least amount of reviews, so this observation could be due to selection bias.







There’s a strong correlation (0.75) between the onsite-to-offer ratio and the percentage of positive experiences. If you receive an offer, you are more likely to think fondly of the process. There’s also a strong negative correlation (-0.67) between the ratio of negative interview experiences and offer yield rate.







The more negative experience a candidate has, the less likely they are to accept the offer. If a candidate who receives an offer has a positive interview experience, the probability that they accept the offer is a whopping 87.5%. However, if that candidate has a negative interview experience, the yield rate is only 33.8%.







In general, senior candidates are harder to please than junior candidates. Senior candidates are often interviewers themselves and expect more from their interviewers. This might explain the abysmal Netflix interview experience. While all other companies keep their shares of senior interviews to under one third, Netflix exclusively hire senior positions. Netflix doesn’t have interns. They don’t even hire recent graduates.







Interview difficulty

At first, I plotted the raw difficulty score calculated by Glassdoor. By this metric, Google and Airbnb interviews are perceived to be the most difficult by candidates, whereas IBM interviews are perceived to be the easiest.

The actual difficulty is more complicated than that. A candidate rejected for a job is more likely to think of interviews for that job as difficult. The correlation between the onsite-to-offer ratio and the percentage of candidates who find the interviews difficult is -0.49.







What candidates say about each company
I thought it’d be fun to visualize word clouds for each company from all the reviews for that company. I remove the company names and locations from the reviews. Can you guess which word cloud describes which company. The labels are: Google, Netflix, Microsoft, Amazon. The answers are at the end of the article.






---------





---------





---------







  Amazon
  Netflix
  Google
  Microsoft


This analysis is done as part of my research for my upcoming book on Machine Learning Interviews. You can get more details + sign up to get updates on the book’s progress here.

Acknowledgements: This post is made possible by the kind support of  Ben Krause, Miles Brundage, Larissa Schiavo, Karson Elmgren, and several other wonderful people who would like to remain anonymous.

  
    People interested in learning machine learning on their own often tell me that there are so many courses online and they don’t know which ones to take. So I’ve put together 10 free online courses on machine learning that I find the most helpful. They should be taken in order.

1. Probability and Statistics by Stanford Online
See course materials

This wonderful, self-paced course covers basic concepts in probability and statistics spanning over four fundamental aspects of machine learning: exploratory data analysis, producing data, probability, and inference.

Alternatively, you might want to check out this excellent course in statistical learning: “An Introduction to Statistical Learning with Applications in R”.

2. 18:06 Linear Algebra by MIT
See course materials

The best linear algebra course I’ve seen, taught by the legendary professor Gilbert Strang. I’ve students describe this as “life changing”.

3. CS231N: Convolutional Neural Networks for Visual Recognition by Stanford
See video lectures (2017)
See course notes

Whether you’re into computer vision or not, CS231N will help you become a better machine learning researcher/practitioner. CS231N balances theories with practices. The lecture notes are well written with visualizations and examples that explain difficult concepts such as backpropagation, gradient descents, losses, regularizations, dropouts, batchnorm, etc.

4.Practical Deep Learning for Coders by fast.ai
See course materials

With the ex president of Kaggle as one of its co-founders, this hands-on course focuses on getting things up and running. It has a forum with helpful discussions about the latest best practices in machine learning.

5. CS224N: Natural Language Processing with Deep Learning by Stanford
See video lectures (2017)
See course materials

Taught by one of the most influential (and most down-to-earth) researcher, Christopher Manning, this is must-take course for anyone interested in natural language processing. The course is well organized, well taught, and up-to-date with the latest NLP research.

6. Machine Learning by Coursera
See course materials

Originally taught at Stanford, Andrew Ng’s course is probably the most popular machine learning course in the world. Its Coursera version has been enrolled by more 2.5M people as of writing. This course is theory-heav, so students would benefit more from the course if they have taken more practical courses such as CS231N, CS224N, and Practical Deep Learning for Coders.

7. Probabilistic Graphical Models Specialization by Coursera
See course materials

Unlike most AI courses that introduce small concepts one by one or add one layer on top of another, this specialization tackles AI top down as it asks you to think about the relationships between different variables, how you represent those relationships, what independence you’re assuming, what exactly you’re trying to learn when you say machine learning. This specialization will change the way you approach machine learning. Warning: this specialization isn’t easy. You can also consult detailed notes written by Stanford CS228’s TAs here.

8. Introduction to Reinforcement Learning by DeepMind
See lecture videos

Reinforcement learning is hard. Luckily, David Silver is here to the rescue. This course provides a great introduction to RL with intuitive explanations and fun examples, taught by one of the world’s leading RL experts.

9. Full Stack Deep Learning Bootcamp by Berkeley
See course materials

Most courses only teach you how to train and tune your models. This is the only one I’ve seen that shows you how to design, train, and deploy models from A to Z. This is also a great resource for those struggling with the machine learning system design questions in interviews.

10. How to Win a Data Science Competition: Learn from Top Kagglers by Coursera
See course materials

With all the knowledge we’ve learned, it’s time to head over to Kaggle to build some machine learning models to gain experience and win some money. Warning: Kaggle grandmasters might not necessarily be good instructors.



I find the order works for me, but some people, including Jeremy Howard from fast.ai, disagreed on the order the courses should be taken. You can join the discussion on Twitter here:


This thread is a combination of 10 free online courses on machine learning that I find the most helpful. They should be taken in order.— Chip Huyen (@chipro) August 3, 2019 


This list will be published as part of my upcoming Machine Learning Interviews book. If you want to be notified when the book is out, follow me on Twitter or sign up for my mailing list.





  #mc_embed_signup{background:#fff; clear:left; font:14px Lora,Lora,serif;  width:370px;}
  /* Add your own MailChimp form style overrides in your site stylesheet or in this style block.
     We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */



	
		
			Email Address 
			
		
	
		
		
	    
	
	
	


(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';}(jQuery));var $mcj = jQuery.noConflict(true);



  
    I recently shared on Twitter that I’m working on a book on machine learning interviews. It’s written with the candidates in mind, but hiring managers who saw the early drafts have told me that they find the book useful to learn how other companies are hiring and rethink their own process.


I'm working on a book on machine learning interviews so I've been spending the last few months talking to companies about their hiring process for ML roles. This thread is a summary of what I've learned. It will be updated as the book progresses. (1/n)— Chip Huyen (@chipro) July 19, 2019


I asked interviewers and candidates for machine learning roles what are bad interview questions and these are some of their answers. What do you think are good/bad interview questions? pic.twitter.com/gorjYCuWWr— Chip Huyen (@chipro) July 20, 2019



In November 2019, I gave a guest lecture at the Full Stack Deep Learning Bootcamp on the topic. Here is the link to the slides: “Machine learning interviews: Lessons from both sides”.

I also open-sourced the first draft of Chapter 9: Machine Learning Systems Design here.

A lot of people have asked me for more details about the book, so I decided to post the book’s introduction here. This is an early draft but it should give you an idea of what the book is about. To be notified when the book is out, you can follow me on Twitter or sign up for my mailing list.





  #mc_embed_signup{background:#fff; clear:left; font:14px Lora,Lora,serif;  width:370px;}
  /* Add your own MailChimp form style overrides in your site stylesheet or in this style block.
     We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */



	
		
			Email Address 
			
		
	
		
		
	    
	
	
	


(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';}(jQuery));var $mcj = jQuery.noConflict(true);



If you’d like to share with me your interview experience or want me to take a look at your hiring pipeline, please shoot me an email (see footnote).

Chapter 1: Introduction

If you’ve picked up this book because you want a job in machine learning – whether as a machine learning engineer, a framework engineer, or a research scientist – but don’t know where to start, what to expect, or how to prepare for interviews, you’re not alone. Despite the aggressive growth in the number of machine learning related jobs and the number of people applying for these jobs, there is little public consensus on machine learning interviews.

According to LinkedIn and Indeed, machine learning jobs are among the fastest-growing and highest-paid jobs in the US, with the average base salary 30% higher than the already very high average base salary of a full-stack developer. Yet, because the use of machine learning in the industry is relatively new, companies are still figuring out what skills they want to see in candidates and how to evaluate those skills. This makes it hard for candidates to know what to expect during their interviews and to prepare effectively.

One of my students, after asking his recruiter about the scope of his upcoming interviews, was told: “Everything.” When I ask my friends or search online for preparation tips, the answers I get are along the lines of: “Just do a lot of coding challenges and read all the popular machine learning books.”

About This Book

Over the last two years, I’ve helped many friends and students prepare for their machine learning interviews at both big companies, including the good old FAANG (Facebook, Amazon, Apple, Netflix, Google), and startups that have raised from seed round to series D. I give them mock interviews and take notes of the questions they were asked in the real interviews as well as the processes they went through. I have also talked to founders and hiring managers at those companies about the challenges of hiring machine learning talents and where their pipelines succeed or fail at addressing those challenges.

At NVIDIA, I’m actively involved in the hiring process, having taken steps from cold emailing candidates whose work I love, screening resumes, doing exploratory and technical interviews, debating whether or not to hire a candidate, to trying to convince a candidate to choose us over competitive offers. I’ve also consulted several startups on their machine learning hiring pipelines.

I myself have got offers for machine learning roles at Google, NVIDIA, Snap, Netflix, and Primer. I have dropped out of the interviewing processes at several other big companies because the timing wasn’t right, the role wasn’t suitable, and/or the process was just a pain. I have also been rejected.

This book is the result of the collective wisdom of many people who have sat on the both sides of the table and who have spent a lot of time thinking about the hiring process. It was written with the candidates in mind, but hiring managers who saw the early drafts told me that they found the book helpful to learn how other companies are hiring, and to rethink their own process.

The book consists of four parts. The first part provides an overview of the machine learning interview process, what types of machine learning roles are available, what skills each role requires, what kinds of questions are often asked, and how to prepare for them. This part also explains the interviewers’ mindset and what kind of signals they look for.

The second part consists of 300 knowledge questions, each noted with its level of difficulty – interviews for more senior roles should expect harder questions – that cover important concepts and common misconceptions in machine learning.

The third part, my favorite, consists of 30 open-ended questions to test your ability to put together what you know to solve practical challenges. These questions test your problem solving skills as well as the extent of your experiences in implementing and deploying machine learning models. Some companies call them “machine learning systems design” questions. Almost all companies I’ve talked to ask at least a question of this type in their interview process, and they are the questions that candidates often find to be the hardest.

This book is not a replacement to machine learning textbooks nor a shortcut to game the interviews. Think of this book as a tool to consolidate your existing theoretical and practical knowledge in machine learning. The questions in this book can also help identify the areas of knowledge that you should focus on improving. Each topic is accompanied by resources that will help you strengthen your knowledge in that topic.

About The Questions

The questions in this book were selected out of thousands of questions. Most of them have been asked in actual interviews for machine learning roles at companies both big and small. To give you an even better sense of what an entire interview is like, the last part of the book features lists of questions for four full-length technical interviews at DeepMind, OpenAI, IBM, and a startup working on natural language processing.

You will find a few questions that are technically incorrect or ambiguous. This is on purpose. Sometimes, interviewers ask incorrect or ambiguous questions to see whether candidates will correct them, point out the edge cases, or ask for clarification. I hope that the answers provided make these questions clear. However, there is also a non-zero chance that the answer provided is simply wrong. If you believe you have found any errors in the answers, please let me know.

Even though most answers are sufficiently mathematically rigorous, I prioritize intuitive explanations over dense mathematical equations. I also prioritize knowing how to use something instead of just knowing the equations. For example, instead of asking a candidate to write out the exact algorithm for K-means clustering, I ask in what scenarios K-means doesn’t work. One doesn’t need to know machine learning to call sklearn.cluster.KMeans, but one needs to understand K-means to know when not to use it.

People often ask me: “Don’t you worry that candidates will just memorize the answers in this book and game the system?” First, I don’t encourage interviewers to ask the exact questions listed in this book, but I hope this book provides a framework for interviewers to distinguish good questions from bad ones. Second, there’s nothing wrong with memorizing something as long as that memorization is useful. The problem begins when memorization is either:
impractical: candidates memorize something just so they can pass the interviews and never use that knowledge again.
prioritized over understanding: candidates are unable to reuse and apply that knowledge in new situations.

For this book, I chose only the questions that I and many of my helpful colleagues deemed practical. For every concept, I ask the question: “Where in machine learning is it used?” If I can’t find a good answer after extensive research, the concept is discarded. For example, while I chose to include questions about inner product and outer product, I left out cross product. You can see the list of discarded questions in the list of “Bad questions” on our GitHub repository.  Keep in mind that as the field expands, many concepts that people used to think of as not useful have proven otherwise. This book might leave out concepts that aren’t popular right now but might be all that AI researchers ever talk about in 2030.

This book has a small number of “what” questions – questions that ask candidates to explain particular concepts. They aren’t necessarily good interview questions, but are good questions to ask yourself while you’re preparing for interviews. Most of the questions in this book are about “why” and “how” things work, questions that test for understanding instead of rote memorization. The book doesn’t explain every concept, especially concepts that can easily be looked up online. If a piece of knowledge can be easily acquired, it isn’t worth testing for.

Please keep in mind that interviewing is a process, and questions aren’t asked in isolation. Your answer to each question is evaluated as part of your performance during the entire process. A candidate who claims to work with convolutional neural networks (CNNs) and fails to answer a question about convolutions is going to be evaluated differently from a candidate who doesn’t work with CNNs at all. Even though the book provides answers for all questions, interviewers often care more about the way you go about answering than the actual objective correctness of your answers.

Interviews are stressful, even more so when they are for your dream job. As someone who has been in your shoes, and might again be in your shoes in the future, I just want to tell you that it doesn’t have to be so bad. Each interview is a learning experience. An offer is great, but a rejection isn’t necessarily a bad thing, and is never the end of the world. I was pretty much rejected for every job when I began this process. Now I just get rejected at a less frequent rate. Keep on learning and improving. You’ve got this!

  
    [Twitter thread]

Disclaimer:

This project is not peer-reviewed. There are a few things to take into account when you read this article:


  As of October 2018, Twitter has 326 million monthly active users, which is a very small fraction of the world’s population. Twitter users in each city might not be representative of that city.
  The amount of data I used is also small and subjected to sampling bias.
  Some of the methods used are keyword-based which are sensitive to spelling variations.
  This project undoubtedly reflects my biases. I’m also not from the US and might be oblivious to many aspects of politicial correctness. I’d appreciate any feedback you might have!


I’d like to thank Fernando Pereira for pointing out the shortcomings of this project.



Have you ever been to a party in which someone mentions that they just came back from another city and someone else feels the urge to compare said city with the current city they are in? Something along the lines of: “Oh I love London. It’s so much more diverse than the Bay Area.” Or some people start a heated argument about which city is the best. If you’re worldly and don’t have your own list of five favorite cities ready to throw at random strangers, are you even worldly?

I’ve always wondered about the validity of those remarks. Generalizing from personal experiences is a slippery slope and prone to stereotyping. So I thought: “Why not use data to compare cities?” I’ve also had the suspicion that the Bay Area is a peculiar place. I’m hoping that data can show me exactly how peculiar.

This post consists of the following sections:


  Data
  How people in different cities describe themselves
  What people talk about
  Unique emojis in each city
  The most unique city
  Conclusion


Data

The only source of public, semi-geo-tagged data I could think of was Twitter. I started with a seed number of users in various locations extracted from the Kaggle dataset Sentiment140 and used the Twitter API to find these users’ followers. I picked the following 13 English-speaking metropolitan areas, mostly because of the availability of data. I got 96k users from these 13 areas.


  US Cities (9): Atlanta, Austin, Bay Area, Boston, Chicago, Washington DC, Los Angeles, New York City, Seattle
  Australian Cities (2): Melbourne, Sydney
  Canadian Cities (1): Toronto
  UK Cities (1): London





MetroTwitter data statistics



I also found 223K users that aren’t in these areas which I collectively put in “Other”. These users come from a wide range of locations all over the world, both rural and urban areas.




Distribution of locations of users in Other



For each user in these 13 areas, I collected the following information:


  Bio (as of April 2019)
  Tweets with timestamp, retweet, favorite count


For users in “Other”, I only collected their bios and not their tweets (because it would take forever).




~50% of all tweets from all cities are between 2017 - 2019. Example: Bay Area.



For data processing, I tokenized the text with spaCy, removed stopwords, and replaced urls with  and rare words with .

I only show a selected number of visualizations in this post. Code, anonymized data, and more visualizations for this blog post can be found on GitHub here. If you clone the Jupyter notebook keywords from the GitHub repo, you can enter a keyword to rank cities by the frequency of that keyword and compare how popular a set of keywords are within a city.

How people in different cities describe themselves

My first attempt was to try to create a word cloud of each city based on each token’s frequency. I used Andreas Mueller’s wonderful word_cloud library.

There are clear differences among cities. For example, while music is a big part of any city, it’s a comparatively small part of the bios from the Bay Area and Boston, and completely absent in Washington DC. I’ve always thought it strange that people use “music” to identify themselves. Who doesn’t listen to music? How Bay Area of me.

A sad thing to note is that “love” is prominent in all cities except Washington DC. Some of the common words in DC bios include “opinion”, “view”, and “tweet”, which come from disclaimers like: “Tweets/views/opinions are my own.”


















However, the differences aren’t that easy to spot because of many common words. I thought it would be more revealing to plot only the differences between two cities. For each pair of cities, I subtracted the words in common from both vocabularies, and visualized the words that are left in a pair of word clouds.

The first thing I notice is that the differences are startling and very much enforce the stereotypes we have about each city. The Bay Area leans heavily towards tech and entrepreneurship, Washington DC politics, Seattle software engineering (dominated by Amazon and Microsoft, and not so much by startups). LA is all about that ‘Netflix and chill’ life. The homogeneity of the Bay Area and DC is rather depressing to look at. I also find it funny that the biggest word in the Bay Area is ‘product’ and the biggest word in LA is ‘producer.’ This reminds me of the saying: “If you’re not paying for the product, you’re the product.” We’re all just products in the Bay.




The difference between bios in Bay Area and LA

---------------------------------------------------------------------------


The difference between bios in Bay Area and DC

---------------------------------------------------------------------------


The difference between bios in Bay Area and Seattle



Given that Google, Snapchat, and Tinder all have offices in LA, there’s a surprising lack of tech in the city. LA loses to almost all cities, even Atlanta, when it comes to tech.




The difference between bios in Los Angeles and Atlanta



People have told me that Austin feels like a smaller version of the Bay Area because of its vibrant startup scene. Looking at the chart, I’d say Austin is a better version of the Bay. In Austin, people still care about family, music, and art. In the Bay, it’s all tech.




The difference between bios in the Bay Area and Austin



Comparing each city to the rest of the world reveals a dimension that is missing in almost all cities: religion. For example, if you compare the Bay Area to the rest of the world, some of the keywords that are most frequently used in “Other” but not in the Bay Area are: “god”, “jesus”, “christ”, “christian”. A keyword that is uniquely missing in the Bay Area bios is: “old”. Ageism is real. Nobody wants to admit to being old here. The lack of keywords like “mother”, “wife”, “kids”, and “married” in the Bay is what I take as a given.




The difference between bios in the Bay Area and the rest of the world



From this dataset, we can figure out where CEOs, founders, VCs, and coffee drinkers live. You’re right, they are all in Bay Area.




Founders, CEOs, VCs, coffee drinkers all live in the Bay Area



The Bay Area isn’t doing that badly for many non-tech careers. We’re behind entertainment hubs like NYC, London, and LA, but we’re doing much better than cities like Boston and Austin. This area also has the highest number of students.








What people talk about

For the tweets, I subsampled 2M tweets from each city in order to make it fair to compare across cities (also faster for creating visualizations). Since I don’t have tweets from users outside these 13 metropolitan areas, for the “Other” category, I combined tweets from all cities.

Again, when I created word clouds for cities separately, the differences are harder to notice as most clouds are dominated by common keywords: ‘love’, ‘work’, ‘GOT’ (ahhh mother dragon why), ‘friend’.








When I visualize only the differences between two cities, the differences are usually dominated by the local sport teams, the local newspapers, some minor pronunciation variations across countries (‘center’ vs ‘centre’, ‘mom’ vs ‘mum’).

Atlanta stands out for mixtape, Horoscope, repurposed NSFW self-identifying words, and a surprising lack of talk on Trump.




Atlanta vs Boston in tweets



From the graphs, the biggest local sport teams in the Bay might as well be API and Android.




Bay Area vs Chicago in tweets






Bay Area vs DC in tweets



Coming from the Bay where public transportation is a pain and often serves as a conversation starter, I used to think that complaining about the local transportation is an indispensable part of urban life. I was wrong. There are cities that take an interest in their local public transportation such as Chicago (CTA, METRA), New York (MTA), and Toronto (TTC), but most cities seem oblivious to the struggle of people in the Bay. There are two possible scenarios:


  The public transportation in that city is so great there’s not much to complain about.
  The public transportation in that city is so bad people just don’t use it.





Bay Area vs New York City in tweets



I ranked the cities based on the frequency of the keyword “transportation” in their tweets and saw that the cities that are most upset with their transportation include the Bay Area, Toronto, Austin (the traffic), and Chicago. The high frequency of the keyword “transportation” in DC is likely due to a lot of talk on transportation policy, but a friend who used to live there told me there’s a non-trivial amount of complaining. The reason that non-American cities like Melbourne, Sydney, and London rank low on the list might be because they use different words when talking about transportation, such as “transport” or “transit”.








A look at DC’s word clouds compared to the rest of the world makes me deeply concerned for the wellbeing of DC residents and their feline population. DC and Melbourne somehow are the only two cities that talk more about cats than dogs.















As expected, Bay Area beats everyone else by a long shot for tech talk. Unexpectedly, Atlanta ranks second for AI. Could it be that “AI” means something else there?








People of the Bay Area are concerned about social issues too. We’re more environmentally conscious than all other cities except Washington DC. By comparison, Atlanta is the least susceptible to environmental discussions. We talk about the ‘homeless’ even more than Washington DC, a city filled with people whose jobs are to talk about policies on homelessness. The housing crisis in the Bay is worrisome. We should learn from Melbourne and Sydney, two cities that don’t have much to complain about regarding homelessness. It’s expected that the Bay dominates the talk on privacy – we created the problem in the first place.







We’re also undisputed champions in talking about weed, burrito, crossfit, and boba, beating even LA. The rest of the world really needs to hop on that boba train.








Let’s take a closer look at what Bay Area people care about. Hint: vegan > racism.








Unique emojis in each city

For this task, I used all 180M tweets from all 13 areas. I get the frequencies of emojis that appear at least 5 times in each city, and pick the emojis with frequencies at least twice as high compared to all cities. Of course, the Bay Area is winning with rockclimbing, scooter, and flipping the bird, NYC with no pedestrians, London with music and alcohol, DC with bald eagle and Pinocchio (thanks to the WashingtonPost fact checker’s Pinocchio ratings), and Chicago with peanuts. Emojis in the pictures below are ranked from most to least used.













The most unique city

When I told Jessie about the project, she said: “Okay that’s great but what is the most unique city?” Since each city has different things to offer, I thought it would be more telling to measure a city’s uniqueness by what is missing in that city. For each city, I picked the words that appear at least twice as frequently in the tweets of all other cities. Then I ranked all cities by the sum of frequencies of those words. Sadly, the Bay Area wins again. While we’re leading in talks on tech, entrepreneurship, the future, and social issues, we seem to be lacking in all other aspects. Melbourne, Sydney, and London rank high on the list but it could simply be because they have different spellings compared to other cities in this dataset, which are mostly American.








Conclusion

This project took me an ungodly amount of time, but also gave me the ungodly pleasure of seeing my suspicions confirmed (I have to admit that I might have chosen the kind of analyses that helped confirm my suspicions). Yes, the homogeneity of the Bay Area is appalling. Yes, my social life is dominated by techies who smoke pot, obsess over burritos, and introduce themselves as “previously @google @facebook @stripe” but at least my home is weirder than your home.

This project also made me sad. When I was a still kid living in a small village, I wanted to come to a big city because I thought it would be a melting pot of diverse ideas. While some cities are still wonderfully colorful, other cities like the Bay Area and Washington DC seem to be moving towards specialization, hosting only one type of people and endorsing only one kind of talk. I’m also concerned by the prevalence of brand names in our bios: we identify ourselves by the companies we work for and tie our self-worth to the brands we’re associated with.

There are so many more patterns I found from looking at the data, and there are so many more that I failed to recognize. I also didn’t go into more details about cities other than the Bay Area because I’m biased. For those interested, the rest of the visualizations about all cities can be found here. You can also play with the data here. If you find something interesting or just want to discuss the project, feel free to shoot me an email or find me on Twitter @chipro.

Acknowledgements: I’d like to thank Miles Brundage, Larissa Schiavo, Matthew Conley, Nguyen Pham, Jason Li, and Jessie Duan for helping me with this post. I’m lucky to have so many wonderful people in my life.

  
    [Twitter thread]
Disclaimer: This post doesn’t reflect the view of any of the organizations I’m associated with and is probably peppered with my personal and institutional biases. The Reinforcement Learning section was written by my wonderful intern and colleague Oleksii Hrinchuk.

1. Inclusivity

The organizers pressed the importance of inclusivity in AI by making sure that the first two main talks, the Sasha Rush’s opening remarks and Cynthia Dwork’s invited talk, were about fairness and equality. Some of the worrisome statistics include:


  Only 8.6% of presenters and 15% of participants are women.
  2/3rd of all the LGBTQ+ researchers aren’t out professionally.
  All 8 invited speakers are white.




A slide from Sasha Rush's opening remarks


Unfortunately, it still feels like the average AI researcher is unapologetically disinterested. While all other workshops were oversubscribed, the AI for Social Good workshop was pretty empty until Yoshua Bengio showed up. During the multitude of conversations I had at ICLR during, no one ever mentioned diversity except that one time I wondered out loud why I was invited to this one tech event that I didn’t seem to fit in and a good friend said: “A somewhat offensive answer would be because you’re a woman.”

One reason is that the topic isn’t “technical”, and therefore spending time on it won’t help advancing your career in research. Another is that there’s still some stigma against social advocacy. A friend once told me to ignore a dude who was trolling me in a group chat because “he likes to make fun of people who talk about equality and diversity.” I have friends who wouldn’t discuss anything on diversity online because they don’t want to be “associated with that kind of topic.”

2. Unsupervised representation learning & transfer learning

A major goal of unsupervised representation learning is to discover useful data representations from unlabeled data to use for subsequent tasks. In Natural Language Processing, unsupervised representation learning is often done with language modeling. The representations learned are then used for tasks such as sentiment analysis, name entity recognition, and machine translation.

Some of the most exciting papers coming out in the last year are about unsupervised representation learning in natural language processing, starting with ELMo (Peters et al.), ULMFiT (Howard et al.), OpenAI’s GPT (Radford et al.), BERT (Devlin et al.), and of course, the too-dangerous-to-be-released GPT-2 (Radford et al.).

The full GPT-2 model was demoed at ICLR and it’s terrifyingly good. You can enter almost any prompt and it’d write the rest of the article. It can write buzzfeed articles, fanfiction, scientific papers, even definitions of made-up words. But it doesn’t sound entirely human yet. The team is working on GPT-3, bigger and hopefully better. I can’t wait to see what it can produce.

While the computer vision community is the first to get transfer learning to work, the base task – training a classification model on ImageNet – is still supervised. A question I keep hearing from researchers in both communities is “How can we get unsupervised representation learning to work for images?”

Even though most big-name research labs are already working on it, there’s only one paper presented at ICLR: “Meta-Learning Update Rules for Unsupervised Representation Learning” (Metz et al.). Instead of updating the weights, their algorithm updates the learning rule. The representations learned from the learned learning rule is then fine-tuned on a small number of labeled samples for the task of image classification. They were able to find learning rules that achieved accuracy > 70% on MNIST and Fashion MNIST.

The authors have part of the code public but not all of it because “it’s tied to the compute.” The outer loop requires about 100k training steps and 200 hours on 256 CPUs.



The inner- and outer-loop for meta-learning (Metz et al.)


I have a feeling that we will see a lot more papers like this in the near future. Some of the tasks that can be used for unsupervised learning include: autoencoding, predicting image rotations (this paper by Gidaris et al. was a hit at ICLR 2018), predicting the next frame in a video.

3. Retro ML

Ideas in machine learning are like fashion: they go around in a circle. Walking around the poster sessions is like taking a stroll down the memory lane. Even the highly-anticipated ICLR debate ended up being about priors vs structure, which is a throwback to Yann LeCun and Christopher Manning’s discussion last year and weirdly resembles the age-old debate between Bayesians and frequentists.

The Grounded Language Learning and Understanding project at the MIT Media Lab was discontinued in 2001, but grounded language learning made a comeback this year with two papers, dressed in the clothes of reinforcement learning.


  DOM-Q-NET: Grounded RL on Structured Language (Jia et al.) - an RL algorithm  to learn to navigate webs by filling up fields and clicking links, given a goal expressed in natural language.
  BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning (Chevalier-Boisvert et al.) -  an OpenAI Gym-compatible platform with a hand-crafted bot agent that simulates a human teacher to guide agents in learning a synthetic language.


My thoughts on these two papers are perfectly summarized by AnonReviewer4:

“… the methods proposed here look very similar to methods that have been studied for quite a while in the semantic parsing literature. Yet this paper only cites recent deep RL papers.  I think the authors would benefit greatly from familiarizing themselves with this literature. I think the semantic parsing community would also benefit from this … But the two communities don’t really talk to each other much, it seems, even though in some cases we are working on very similar problems.”

Deterministic Finite Automata (DFA) also found its place in the world of deep learning this year with two papers:

  Representing Formal Languages: A Comparison Between Finite Automata and Recurrent Neural Networks (Michalenko et al.)
  Learning Finite State Representations of Recurrent Policy Networks (Koul et al.)


The main motivation behind both papers is that since the space of hidden states in RNNs is enormous, is it possible to reduce the number of states into a finite number of states? I’m skeptical that DFA can effectively represent RNNs for languages, but I really like the idea of learning RNNs during training and then converting it into DFA for inference, as presented in Koul et al.’s paper. The resulting finite representations require as few as 3 discrete memory states and 10 observations for the game of Pong. The finite state representation also help with interpreting RNNs.



Three stages of learning a DFA from a RNN (Koul et al.)




Extracted automata (Koul et al.)


4. RNN is losing its luster with researchers
The relative change in submission topics from 2018 to 2019 shows that RNN takes the biggest dip. This isn’t surprising because while RNNs are intuitive for sequential data, they suffer from a massive drawback: they can’t be parallelized and therefore can’t take advantage of the biggest factor that has driven progress in research since 2012: compute power. RNNs have never been popular in CV or RL, and for NLP, they are being replaced by attention-based architectures.



RNN is losing its luster. Image from ICLR 2019's supplement statistics


Does that mean that RNN is dead? Not really. One of the two best paper awards this year is for “Ordered neurons: Integrating tree structures into Recurrent Neural Networks.” (Shen et al.). Other than this paper and two on automata mentioned above, there are nine more papers on RNNs accepted this year, most of them dig deep into the mathematical foundations of RNNs instead of discovering new applications for RNNs.

RNNs are still very alive and kicking in the industry, especially for companies that deal with time-series data such as trading firms, and these firms, unfortunately, don’t usually publish their work. Even if RNN is unattractive to researchers right now, who knows it might make a comeback in the future.

5. GANs are still going on strong
Even though GAN takes a negative relative change compared to last year, the number of papers actually went up, from ~70 to ~100. Ian Goodfellow delivered a popular invited talk on GAN and was constantly mobbed by admirers. By the last day, he had to flip his badge so that people wouldn’t see his name.

The entire first poster session was dedicated to GANs. There are new GAN architectures, improvements on old GAN architecture, GAN analyses, GAN applications from image generation to text generation to audio synthesis. There are PATE-GAN, GANSynth, ProbGAN, InstaGAN, RelGAN, MisGAN, SPIGAN, LayoutGAN, KnockoffGAN, etc. and I have no idea what any of these means because I’m illiterate when it comes to GAN literature. I’m also sorely disappointed that Andrew Brock didn’t call his large scale GAN model giGANtic.



All hail the dogball (Brock et al.)


The GAN poster session reveals just how polarized the community is when it comes to GAN. Some of the comments I’ve heard from non-GAN peeps include: “I can’t wait for all this GAN thing to blow over”, “As soon as somebody mentions adversarial my brain just shuts down.” For all I know, they might just be jealous.

6. The lack of biologically inspired deep learning

Given all the fuss about gene sequencing and CRISPR babies, it’s surprising that there aren’t more papers on deep learning in biology at ICLR. There are a grand total of six papers on the topic:

Two on biologically-inspired architectures

  Biologically-Plausible Learning Algorithms Can Scale to Large Datasets (Xiao et al.)
  A Unified Theory of Early Visual Representations from Retina to Cortex through Anatomically Constrained Deep CNNs (Lindsey et al.)


One on Learning to Design RNA (Runge et al.)

Three on protein manipulation:

  Human-level Protein Localization with Convolutional Neural Networks (Rumetshofer et al.)
  Learning Protein Structure with a Differentiable Simulator (Ingraham et al.)
  Learning protein sequence embeddings using information from structure (Bepler et al.)




Retina inspired convoluntional neural network (Lindsey et al.)


There’s zero paper on genomics. Neither was there any workshop on the topic. It’s sad but it also shows a huge opportunity for deep learning researchers who are interested in biology or biologists who are interested in deep learning.

Random fact: the first author of the retina paper, Jack Lindsey, is still a senior in college (Stanford). Kids these days urrggh.

7. Reinforcement learning is still the most popular topic by submissions

Works presented at the conference indicate that the RL community is moving from model-free methods to sample-efficient model-based and meta-learning algorithms. The shift was likely motivated by extremely high scores on Mujoco continuous control benchmarks set by TD3 (Fujimoto et al., 2018) and SAC (Haarnoja et al., 2018), and on Atari discrete control tasks set by R2D2 (Kapturowski et al., ICLR 2019).

Model-based algorithms – which learn a model of the environment from data and exploit it to plan or generate more data – have finally reached the asymptotic performance of their model-free counterparts while using 10-100x less experience (MB-MPO (Rothfuss et al.)). This advantage makes them suitable for real-world tasks. While a single learned simulator is likely to be flawed, its errors can be mitigated with more complex dynamics models, such as an ensemble of simulators (Rajeswaran et al.). Another way to apply RL to real world problems is to allow the simulator to support arbitrarily complex randomizations: the policy trained on a diverse set of simulated environments might consider the real world as “yet another randomization” and succeed in it (OpenAI).

Meta-learning algorithms which perform fast transfer learning among multiple tasks have also improved a lot both in terms of sample-efficiency and performance (ProMP (Rothfuss et al.), PEARL (Rakelly et al.)). These improvements brought us closer to “the ImageNet moment of RL” when we may use control policies learned from other tasks instead of training them from scratch (which is impossible for complex tasks).



PEARL (Rakelly et al.) outperforms previous meta-RL methods both in terms of asymptotic performance and meta-training sample efficiency across six benchmark tasks.


A large portion of accepted papers, together with the entire Structure and Priors in RL workshop, was devoted to integrating some knowledge about the environment into the learning algorithms. While one of the main strengths of early deep RL algorithms was generality (e.g. DQN uses the same architecture for all Atari games without knowing anything  about any particular game), new algorithms show that incorporating prior knowledge helps with more complex tasks. For example, in Transporter Network (Jakab et al.), the agent uses prior knowledge to conduct more informative structural exploration.

To sum up, during the last 5 years, the RL community has developed a variety of efficient tools for solving RL problems in a model-free setup. Now it’s time to come up with more sample-efficient and transferable algorithms to  apply RL to real world problems.

Random fact: Sergey Levine is probably the person with the most papers at ICLR this year with 15 accepted papers.

8. Most accepted papers will be quickly forgotten

When I asked a well known researcher what he thinks of the accepted papers this year, he chuckled: “Most of them will be forgotten as soon as the conference is over.” In a field moving as fast as machine learning where state-of-the-art results are broken after weeks, if not days, it’s not surprising that most accepted papers are already outperformed by the time they are presented. For example, according to Borealis AI, for ICLR 2018, “seven out of eight defense papers were broken before the ICLR conference even started.”

One comment I frequently heard during the conference is how random the acceptance/rejection decisions are. I won’t name any but some of the most-talked about/most cited papers in the last few years were rejected from the conferences they were originally submitted to. Yet, many of the accepted papers will go on for years without ever being cited.

As someone doing research in the field, I frequently face existential crisis. Whatever idea I have, it seems like someone else is already doing it, better and faster. What’s the point of publishing a paper if it’ll never be of any use to anyone? Somebody help!

Conclusion

There are definitely more trends I’d like to cover such as:

  optimizations and regularizations: The Adam vs SGD debate is continued. Many new techniques are proposed and some are quite exciting. It seems like every lab is developing their own optimizer these days – even our team is working on a new optimizer that should be released soon.
  evaluation metrics: As generative models become more and more popular, it’s inevitable that we need to come up with some metrics to evaluate generated outputs. Metrics for generated structured data are questionable enough, but metrics for generated unstructured data such as open-domain dialogues and GAN-generated images are uncharted territory.


However, the post is getting long and I need to get back to work. If you want to learn more, David Abel published his detailed notes (55 pages). For those who want to see what else is hot at ICLR 2019, I found this graph from ICLR 2019’s supplementary statistics particularly indicative.



Someone should write a paper on "A robust probabilistic framework for universal, transferrable unsupervised meta-learning"


I really enjoyed ICLR. The conference is big enough to find many people to have interesting conversations with yet small enough to not have to wait in line for things. The length of the conference, 4 days, is also just right. NeurIPS is a bit too long and usually by the end of the fourth day, I’d be walking by posters and thinking: “Look at all the wonderful knowledge I could be acquiring right now but don’t want to.”

The biggest takeaway I got from the conference is not just ideas, but also motivation. Seeing researchers my age doing wonderful things helps me see the beauty of research and motivates me to work hard myself. It’s also nice to have an entire week to catch up on papers and old friends. 10/10 would recommend.

  
    [Twitter thread]

I wrote this article a while ago but didn’t publish it because I didn’t want to give the book free publicity. However, I’ve just found out that the author, Antonio García Martínez, is now a writer at WIRED. I’m terrified to think what will happen when someone like him is a gatekeeper to how Silicon Valley is represented.

While looking for books on the tech world, I came across Chaos Monkeys: Obscene Fortune and Random Failure in Silicon Valley. The book is advertised as “exposé of life inside the tech bubble.” A glimpse into his mind and the rave reviews of his male-dominated audience, I suddenly understood why there aren’t more women in tech.

The first two chapters of the book explain the author’s journey into the tech world. Dropping out after five years in the physics PhD program at UC Berkeley, he became a quant at Goldman Sachs. After three years, he returned to the Bay Area to work at a startup that optimizes ads. Together with two of his colleagues there, he built his own ads optimizing startup, not without a lawsuit from his former employer. When his team was negotiating an acquisition offer from Twitter, he deceived his cofounders to negotiate a separate deal with Facebook, royally screwing everyone in the process. He became a product manager for ads at the social network giant until he was fired two years later. He eventually landed at Twitter.

There are certain aspects of his personal and professional life that I find troublesome. He devoted a chapter to his drag race on the 101 to Stinson, swerving on “the wrong side of the road doing low triple digits … as traffic was thick on a Friday afternoon”, risking the lives of many commuters just because “losing is worse than death.” He relegated the responsibilities of child rearing to the mother of his children because he believes “better no father than a bad father.” A common theme in his career is that everyone around him seems to be an insufferable asshole, upon whom he lavishes his extensive vocabulary of profanity. This reminds me of a quote by Raylan Givens: “If you run into assholes all day, you’re the asshole.”

Given that the author seems to live in a world where people are judged based on their levels of testosterones, I shouldn’t have been surprised by the way he perceives women. It surfaced unequivocally early on in the book:

“Most women in the Bay Area are soft and weak, cosseted and naive despite their claims of worldliness, and generally full of shit. They have their self-regarding entitlement feminism, and ceaselessly vaunt their independence, but the reality is, come the epidemic plague or foreign invasion, they’d become precisely the sort of useless baggage you’d trade for a box of shotgun shells or a jerry can of diesel.”

Whenever a female character appears in his book, he makes sure to comment on her looks, preferably with creative phrases like “jaw-droppingly hot”, “‘got lost on the way to New York Fashion Week’ hot”, “jeans-clad ass.” When a woman isn’t ugly, her sole purpose in the office is to be ogled at. He nicknamed one female coworker at Facebook “PMMess”, which, any Audible listener will recognize, is a high-school spelling of “PMS”.

“PMMess, as we’ll call her, was composed of alternating Bézier curves from top to bottom: convex, then concave, and then convex again, in a vertical undulation you couldn’t take your eyes off of. Unlike most women at Facebook (or in the Bay Area, really) she knew how to dress; forties-style, form-fitting dresses from neck to knee were her mainstay. Her blond hair was offset by olive skin, and bright blue eyes shone like headlights from her neotenic face.”

To make sure that readers understand how many sperms he is capable of producing, the author takes pain to highlight his sexual escapades. He described in disturbing details a drunken romp with a female coworker in a Facebook broom closet. He proudly declared himself as a male “who’s played it fast and loose with the safe-sex rules”, yet his PhD level of physics doesn’t seem to help him understand that unprotected sex leads to pregnancy (not to mention STDs – having godparents who have spent their whole lives fighting HIV has conditioned me to respond violently to unprotected sex). He seems unable to sympathize with a woman’s pregnancy scare: “Look, woman, unless you’ve got a screaming infant in your arms and it looks like me, we have nothing to talk about.”

When he got a woman he had just met on a dating site a week ago pregnant and she confirmed her desire to keep the child “whatever my thoughts on the matter”, he lamented that he “had been snookered into fatherhood via warm smiles and pliant thighs.” “Smiles” rhymes with “thighs”, he must have felt like a poet. I thought this species of men has gone extinct in the bubble of higher education, but here we encounter one, thriving in his natural habitat of tech bros: a man who has unprotected sex with strangers and expects women to have abortions on demand.

Women aren’t the only minority the author has problems with. He’s also uncomfortable among people of color. He compared a coworker who has a “thick Indian accent” to “bored auto-rickshaw drivers in front of Connaught Place, Delhi, who’d overcharge you a hundred rupees to go down the street to Paharganj.” He evoked his disgust for East Palo Alto, “the local slum that once had the highest murder rate in the Bay Area” by mentioning that “two of the local schools are named after César Chávez and Ron McNair, an African American astronaut”, as in, “how low could this neighborhood be if the best people they knew were colored.”

What particularly bothered me is not his disparaging view of people of minorities, but how this view is perceived. On Goodreads, the book gathers an average rating of 3.73. While this isn’t a great number – the average rating on Goodreads is between 3.8 and 4.0 – it doesn’t make any sense given the abundance of misogyny in the book. If an article on the New York Times or Wired calls its female readers “soft and weak, cosseted and naive, generally full of shit” and refers to “feminism” as “self-regarding entitlement”, it would have no doubt be forced to be taken down with profuse apologies from the editors. Yet, we have this book published by one of the top 5 publishers in the US. Somebody must have edited it, reviewed it, cut out excerpts to promote it – and nobody saw a problem? TechCrunch called it “this year’s best non-business book about business.” The New York Times cautiously opened with “there is plenty not to like” about the book, but still gave it a lukewarm review.

I decided to delve into the reviews section to see who his fans are. As of Jan 15, 2019, the book has 787 reviews but Goodreads only allows me to see 299 of them. Goodreads has more female users than male users, which isn’t a surprise given that Pew research has shown that women read more than men. According to the site, in the first year of publication, women make up 80% of a female author’s audience and 50% of a male author’s.

Curiously enough, Chaos Monkeys’ audience is made up mostly of men. Out of 299 reviews, 16 were written by users whose genders I couldn’t verify. Among the other 283 reviews, 215 (76%) were authored by men.

The distributions of ratings are staggeringly different. On average, men give the book a rating of 3.6 while women give this book a rating of slightly below 3. One in 6.8 women feels compelled enough to give the book a one-star rating, while that number is one in 16.5 for men. When it comes to misogyny, men are apparently more tolerant than women.

A common reaction of female readers to this book is revulsion. Elyssa gave the book a two-star: “His language and attitude is solid evidence of why there are so few women in tech. If I had to work with men like him, I would leave the field, too.” A less flattering review comes from Neda, who gave the book a one-star:

“I wish I could give this book a lower star. I absolutely hated every single word of this book. misogynistic, arrogant, self serving, an absolutely horrible example of a human being and what passes as a software engineer these days. Antonio should be a model for how not to bring up your sons and not allow your daughters to meet. Every single phrase in this book reeks of his egotism and self gratifying life style. He is the prime example of the era we live in, driven by money, power and narcissism… shame on the publisher who killed trees to get this out onto the bookshelves.”

Reviews among his male audience are more polarized. There are men who hate the book. There are men who live in the author’s mindset and wholeheartedly give the book 5-star. What I find most worrisome are the reviews in the middle of the spectrum, those along the line of: “Well there’s misogyny but it’s a fun read.” For example, Shashwat gave the book a four-star with the review: “Great book by an honest writer, even though some parts may classify as chauvinistic.” Albert who gave the book a three-star after admitting that the book belongs to the “gonzo tech-testosterone lit genre.”

There should be no “but” after sexism or racism. It’s one thing to ignore it altogether – not everyone has the fortitude and integrity to stare down a can of worms. It’s another to bring it up and shrug it off as something inconsequential. It’s to say: I know people of minorities are being mistreated all around me, but I can live with that.

This indifference is what allows misogyny to proliferate. As a woman in tech, I feel unsafe not only because people like this author ogle at and marginalize me, but also because those who see it happen do nothing about it. Here’s a man who boasts about abandoning his children, lying to his co-founders, marginalizing women and people of colors. Yet he continues getting hired by big tech companies and being embraced by decision makers in tech who happen to be men.

When I shared my frustration about this book with my friends, most of them told me to ignore it: “He’s just an asshole, not worth your time.” We can ignore a random asshole on the street, but how can we ignore a plethora of them in our workplace, especially in managerial positions? Ignoring the book is the same as allowing more books like it to be published, encouraging misogyny to be the norm.

I confronted the author and he responded with a lengthy, convoluted message that can be summarized as: “you’re leaving a one-star review for Silicon Valley, not my book.” He defended himself as a messenger who sheds light on the culture of Silicon Valley. Whatever uncomfortable feeling I might have for the book is because of the culture itself. “Hate the game, don’t hate the player” – he’s just doing what everyone else is doing. Yet, there would be no game if there were no player. There would be no sexism in Silicon Valley if there were no one to perpetuate it.

Presenting himself as the typical, standard version of heterosexual males in tech not only is a gross insult to men everywhere – I know plenty of men in tech who are accomplished without ever having to abandon their children or nickname their female coworkers “PMS” – but also gives the confused men of the digital revolution an easy way out. Why strive to be loving husbands, attentive fathers, and respectful colleagues, when you can be greedy, deceitful chauvinists and get away with it?

The lack of women in tech is a complicated problem. Attacking or ignoring one book written by a misogynist won’t solve it. However, rejecting the book as a typical narrative of our industry might be a good start. The book tells the story of an uninspiring, morally questionable individual in tech, who stands out only for the way he disparages people of minorities. It’s not “a guide to the spirit of Silicon Valley” as the author and his publisher try to present. Men don’t have to be like the author, and women don’t have to work with, even tolerate, men like the author to fit into the tech world.


  
    Since graduation, whenever I hang out with friends, our conversations eventually turn to: “how do you meet new people?” When I ask someone how they are doing, their answer is invariably “good, but I miss having my friends around.”

Making friends is hard. In college, you can befriend someone just for the sake of being friends. After college, relationships suddenly become transactional. You need a reason to meet someone. They, in turn, constantly evaluate what you can bring to the table. I’ve had so many disheartening experiences when I meet someone, they ask what I do, and as soon as I give an answer that’s irrelevant to their businesses, their eyes glaze over.

When a conversation is not transactional, it’s often mistaken for a romantic attempt. When a guy I barely know asks to meet, I sometimes wonder if it’s a date. This mentality also makes me afraid to reach out to guys because I’m afraid they’ll think I’m hitting on them.

The list below is drawn upon many conversations I’ve had. If you have other tips, I’d love to hear them.

1. Do cool things
When I asked a Thiel Fellow how she knows so many interesting people, she shrugged: “I do cool things.” Chris Lengerich, who just finished his six months being an entrepreneur in residence at Khosla to start his own company, told me the same thing. “Don’t try to make connections for the sake of making connections. They’re superficial. Do cool things and people will be attracted to you.” If you aren’t doing anything interesting, what are you going to talk to the person you just met about?

Most of the relationships I’ve built outside schools are from doing things that I care about: through teaching the TensorFlow course, through writing, through traveling, and through doing AI research.

Debnil Sur, my friend at Interstellar, asked: “How do you know that what you’re doing is cool?” I find that almost anytime I try to do something because I think other people think it’s cool, I fail. Whereas if I do something that I truly care about, my enthusiasm attracts people. There are many things I didn’t think were cool until I met someone who was passionate about it.

2. Provide value
Jerry Lu, a Wharton MBA candidate and associate at Lux Capital, told me that his strategy for building meaningful relationships is to provide values: “When I meet someone new, I try to figure out what I can do for that person.” It isn’t rocket science that we’re more inclined to add someone to our lives if that person can add positive net values. Yet, I’m guilty of reaching out to people who can do something for me while I should be asking myself what I can do for that person.

To be able to provide values to other people, we first need to have some values to provide. For Jerry, he organizes events to connect like-minded people. When he comes across something he knows I’m interested in, he sends it my way.

The personal strategy to provide values of the Thiel Fellow Noor Siddiqui is roping her friends in while crossing off items on her own bucket list: “If i’m really excited about climbing a mountain, or putting up street art, will bring along a friend or two that might enjoy it as well.”

It’s not easy to put other people’s needs ahead of my own. I have to constantly remind myself: do someone a favor before asking that person for a favor. I also started copying my friend Lucio Dery to end my messages with “Please let me know if there’s anything I can help you with”, and make a point of seeing it through if someone takes up the offer.

3. Use social media
I know there’s this zen thing going on that encourages people to get off social media. Occasionally, I also feel the need to disconnect from online channels to focus on myself. But overall, social media has given me much more than what it has taken away.

Social media is a tool: whether it’s good or bad depends on how we use it. For me, it’s a way to spread my ideas, explain my work, and above all, sell myself. There exist people who are so good that the world seeks them out regardless. But I’m not one of those geniuses. Most people aren’t. If I don’t get on social media, I’ll just become invisible.

One channel that I highly recommend is Twitter. I’ve met many interesting people who’ve become my friends (shout out to David Dao, Miles Brundage, Joshua Browder). When I share my work on Twitter and it gets retweeted, it almost feels like cheating – I know people who are doing much better work but don’t get recognition.

Thoughtful blog posts are also a great way to showcase your critical thinking and reach more audience. I enjoy any post that shows that the author has thought deeply on the topic and can give me a new perspective or teach me something new. I especially love technical blog posts that turn difficult technical concepts into popular science – these posts show that the author not only understands the concept but also has great communication skills.

Newsletters are making a comeback. John Luttig, an associate at Founders Fund, recently started a weekly newsletter to share one thing he’s learned and one question he’s thinking about. “When people hear from you every week, you’re top of mind and they loop you in on things more often. People are sending me more deals and asking me to hang out,” he told me.

One problem with social media is that it takes time to build the audience. The hardest part is to grow from 0 to the first 1000 followers. Luckily, there are plenty of articles online on how to grow your audience.

4. Don’t try to impress people superficially
One mistake I made initially (and still occasionally) is that I try way too hard to befriend people I look up to. That has never turned out well.

This sounds shitty to write out loud and I know some would be quick to blame it on my insecurity, but I’ve learned to be realistic about who I should try to befriend. I’m not saying that all successful people are snobs, it’s just that I haven’t done enough to interest them.

Omar Rizwan, another Thiel Fellow, summed it up well over our brunch: “I prefer becoming friends with peers, people who are at the same stage in life as me and are working towards the same thing. It’s hard to be friends with people way ahead of me because I have nothing to offer.”

My rule of thumb when meeting someone new: if I feel the need to impress them, I should say goodbye and walk away.

5. Have a strategy for each meeting
My roommate Rex thought it was strange for me to reach out to strangers: “I wouldn’t know what to talk to them about.” It was strange for me to hear that too: “Why would I want to meet someone if I don’t know what to talk to that person about?”

I only reach out to people when there are things I want to discuss. It could be about their current work, something they did, or some specific insights. If we click and become friends, that’s great. If we don’t, I learn something new and I hope they get something out of it too.

Devon Zuegel, who’s currently hosting a show on crypto for a16z, put it beautifully in a tweet:

What are your strategies for having great conversations? A few of mine:


  Collect good questions and keep a written list
  Allow for almost-awkwardly long silences; other person will fill them with something interesting
  Listen for words/models they keep using that are atypical


6. List of questions to get to know someone
Getting to know someone is as much a science as an art. This topic has been heavily studied and you can find zillion of lists of questions that claim to help you get to know someone.

I love Noor’s blog post How to Get to Know Someone. To quote Noor:

I’ve found that good conversations start from questions that get your (future) friend to:


  recall an emotionally charged memory (positive or negative)
  recall a rarely accessed memory (nostalgia)
  generate a novel response (think about something they hadn’t thought about before).


Bad conversations occur when the person you’re talking to is:


  providing canned responses to questions, having a conversation with you that they have had with someone else a moment ago
  not learning anything about themselves
  not feeling understood by you


Her has a list of questions that go from easy (What thing did you buy for under $50 that brought you the most joy/convenience/utility?) to hard (What is the most significant thing you’ve changed your mind on in the last year?) to challenging (What is your most radical belief?).

In that spirit, I’ve also made a list of questions I use when I’ve run out of organic things to talk about. These questions depend a lot on the circumstance we meet. I rarely have to use them, as I’ve found that listening and mirroring the other person is much more effective.


  What has made you really happy in the last month?
  What are you reading?
  What’s something that you’ve learned recently that surprises you?
  What’s your story? [Everyone has a story yet in my experience, almost everyone being asked this question for the first time is slightly taken aback. However, if you show that you genuinely care, they might tell you a story that they haven’t told many people before.]
  
    
      
        
          Would your [10
          18
          20
          ..] year-old self be surprised about where/who you are today?
        
      
    
  
  What’s the best career decision you’ve ever made? What’s the worst?
  How would you describe your friend circles?
  What quality do you value the most in a friend?
  What’s the first impression that people often have of you which isn’t true?
  What do you think of the stereotype of [the career they are in]? For this question, they might ask back: what’s the stereotype? You can avoid answering this question with: “You should know this better than I do!”
  What’s the riskiest thing that you’ve done?
  What is the most valuable skill that got you where you are today? [mildly complementary]
  Your parents must be proud of you!
  What’s the biggest problem you’re facing?
  What would you want to change about your life right now? [A lighter version: what would you want to spend more/less time on?]
  Is there anything I can help you with?


Closing thoughts
Just as you can’t hack dating, you can’t hack meaningful relationships. No matter how many tips you use, it eventually boils down to whether you’re comfortable in each other’s company. Are you adding values to each other’s life? Are you creating an environment for other people to be themselves? Are you living an exciting life which other people would want to be part of?

I’m nowhere close to being an expert on the topic. I can be painfully awkward at times, which leads to cringe-worthy social interactions, which reinforces the self-inflicted belief that I should stay within my protective shell. But there are two things that keep me going, and I hope that they’ll help you too: 1) other people are just as afraid of getting hurt as we are. 2) every friend that we have now was once a stranger. So open up and put yourself out there. It’ll all be worth it in the end. [Or not.]

Acknowledgement: I would like to thank Edward Lu, Raymond Nie, John Luttig, Noor Siddiqui, Jerry Lu, Chris Lengerich for reading the draft and providing super helpful feedback.

  
    [Twitter thread]

Ever since graduation, people have been asking me: “What’s now?” My answer has been an unequivocal: “I don’t know.” I used to think that by the time I finished my master’s degree, I would know what to do. I thought I’d be a “master”.

Boy, was I wrong. School did little to prepare me for the post-school world. The academic environment provides continual feedback – you go off track a little and somebody is sure to let you know, even guide you back in. In the real world, I have this fear that I’ll make a series of wrong decisions and nobody will tell me until it’s too late. A wrong job choice could cost me a few years, together with many opportunities that should have come with a better choice.

When I looked up career advice for recent graduates online, most of the articles I found are concerned with how to get a job. I don’t want to sound like a snob, but realistically, for many recent grads with degrees in a demanding field like CS, the question is less about “what job can I get” and more about “what job should I get.” The availability of options doesn’t make the decision any easier. If anything, it threw someone with a massive case of FOMO like me into panic mode. I kept iterating through a series of questions: “Should I do a PhD?”, “Should I work for a big company or a startup?”, “Should I do my own startup?”, “Should I do engineering or something more social?”, “Should I just leave tech and pursue my passion for writing?”

Over the last year, I’ve bothered many people, both in industry and academia, with these questions, and I’ve been so lucky that some of them were kind enough to sit me down and share their insights with me. As their advice was extremely helpful to me, I thought they might be also useful for others who will one day have to go through the process that I did. This article is an attempt to put into words the overwhelming thought process that I went through and the advice that I received. If you just want the advice without the storytelling, go straight to the last section.

PhD or no PhD

My family farms in a small village in Vietnam, so the US academic world was obscure to me. I had no idea what a PhD was about, what people looked for in an PhD application, or how I should prepare for it if I ever wanted to apply. It was only when I started hanging out with PhD students at the beginning of my last year that I realized: “Wow, these people are really smart. They work on interesting problems. I want to be like them.”

I quickly realized that “PhD or no PhD” is a topic that everyone seemed to have an opinion on. I also realized that 100% of the professors I talked to (aka those who already did their PhDs) told me I should do a PhD, and 100% of the people in industry told me I should not.

Arguments supporting PhD include:


  You’ll have time to immerse yourself in research.
  If you want to become a professor, you have to do a PhD.
  Many top research labs such as DeepMind only interview PhD candidates.
  You won’t be too poor as AI internships pay well.


Arguments supporting no-PhD include:


  There should be more people joining industry to bring research into production.
  By the time you finish your PhD, what you learn might no longer be relevant.
  Many professors have side gigs in the industry anyway so you can still work with them.
  You won’t be poor for the next five years.


At first, I decided to go with doing PhD. Since it was too late for me to prepare my PhD app, my professors suggested that I apply next year and spend the year in between strengthening my app, so I lined up a bunch of research internships.

I graduated a quarter early and spent the next three months traveling. I wasn’t trying to find myself, but I stumbled upon it. During that time, I wrote every day for fun but didn’t read a single paper. I realized that I wanted to do a PhD not because I wanted to do AI research, but because I wanted to be the person who did AI research. The notion strengthened when I caught up with my PhD friends and saw them spend every waking moment talking/thinking about AI – I didn’t share their passion. I wanted something different. How different? I was still trying to figure it out.

Be a sell-out or follow your passion

Stanford offers a CS + English major. We used to joke that this major is for those who love writing but also want to get a job. Then a friend told me I was one of those.

I didn’t major in CS for the sake of getting a job. I got hooked on it from the first introductory class because the subject was fascinating. I love engineering, but my three months off rekindled the belief that writing is the biggest love of my life. Since I already spent nearly 4 years pursuing my degrees in CS, I longed to have some time invested in writing. I was also afraid of settling into the stereotypical complacency of being just another software engineer in Silicon Valley.

Pursuing your passion, it turns out, is not legal in the US when you’re an international student. To stay in the US, I have to have a job related to my field of study. I can, of course, go live in another country. The idea of living on some South American beach and writing is quite romantic. But AI is such a fast-changing field that I was already a bit disoriented after a quarter off – what if I couldn’t get back into the field after a year? Plus, logistics and immigration would be a nightmare.

When I came to my professor with this dilemma, he was confused: “Why do you have to choose between engineering and writing? Why not both?” There are many people who are accomplished in their technical fields but also prolific writers. His unquestioning confidence in my ability to do both inspired my own confidence: “You’re right. I can do both.” I can work full time in tech and spend evenings/weekends writing. I used to spend an ungodly amount of time doing homework + teaching in college so I’m sure I can spare 20+ hr/week for writing.

Should I do my own startup?

Graduating from Stanford and living in Silicon Valley, I can’t escape the startup stereotype. Some have suggested that I’m the “startup type.” Some have even asked me to become their co-founder.

I have, more than once, been tempted. The idea of building something from scratch is appealing. I’ll undoubtedly learn a lot, not only about the problem I try to solve but also about how to inspire people to work with me, how to raise money, how to run an organization, how to sell my product…  I have a lot of friends doing their own startups and their lives are a lot of things, but never boring. Plus, they have a substantially larger chance of becoming billionaires than I do.

Just reading news about this or that twenty-something raising millions of dollars makes it seem so easy. “Even that Yo app raised $1.5M!” someone once cited this as a reason why I should do a startup. But I’ve also seen enough of my friends to know that the startup life is stressful, cut-throat, and constrained by so many external factors. A fair share of my friends’ startups have already failed even though they are extremely smart, had great ideas and were backed by prestigious investors. Every time I feel like crap, I call up my startup friends, see their struggles, and feel glad that I’m not them.

I might be in the minority here, but I think starting a company just for the sake of starting a company is devoid of reason and a waste of everyone’s time. I wouldn’t start my own company until I’ve got at least three things:


  A problem I want to dedicate my life to solve.
  A belief that I can solve it.
  A co-founder I can work with for an extended period of time without either of us trying to murder the other.


Right now, I have none of those, so I’ll just hang around. I also think that working for someone else for a few years will much better prepare me, both finance-wise and skill-wise, to work for myself.

Big companies or startups

With the above factors internalized, I started my internship at NVIDIA. I cancelled my other internship plans and looked into full-time jobs. That was when my next big question arose: “Should I work for a big company or a startup?”

I had interned at both a big company and a startup during my undergrad. My impressions were very much aligned with what is usually said of the trade-off between big company stability and startup high impact (and high risk). Among my friends who chose not to do PhD, about 40% went to big companies, 40% worked for startups, and the rest started their own companies. They all gave me compelling arguments. The pros/cons below, of course, vary from company to company.



When I shared this concern with people, many told me to do what made me happy. While I appreciate the sentiment, this advice often left me even more confused. What does “happiness” even mean? How am I supposed to measure it? “Happiness” is also relative. Given enough time for adjustment, we can teach ourselves to be happy with just about anything.

There were two pieces of advice that I found helpful. The first was: “Which one gives you a once-in-a-lifetime opportunity?” Do you believe that the startup you’re considering is working on something truly important and it’s your once-in-a-lifetime opportunity to contribute to it? Or do you believe that working for a tech giant is your once-in-a-lifetime opportunity?

The second was: “What are you optimizing for?” An easy thing to optimize for is money. Some of my friends interview with multiple companies and go to the highest bidder. Some optimize for new experiences and choose a job that allows them to travel and meet a lot of people. Some optimize for prestige and go to the company that is the most well-known in their field.

At this stage in my life, I optimize for personal growth. I want a job that gives me the most freedom to grow. It means that the job should allow me to work with great colleagues/mentors and challenge myself as much as possible.

NVIDIA

In the end, I chose to stay at NVIDIA for the following reasons.

1. The culture

I’ve talked to many companies, and NVIDIA stands out as a big company that feels like a startup. One of the NVIDIA’s cultural tenets is SOL, speed of light – you have to move fast, much faster than a regular big organization. I talked to my manager about converting to full-time one afternoon and got my offer the next day.

On Glassdoor, some say that the startup-like culture means a higher workload. Some find the lack of organizational structure at NVIDIA disorienting. But I absolutely love the dynamics. As the company is growing so fast, there are so much to do. An employee can go out there and challenge themselves as much as they want.

The hierarchy is flat. You can reach out to everyone and they will listen to you. Even as an intern, I had the opportunity to work with two senior managers. Jensen Huang once offered me a beer. My experience is far from unique.

Also, Jensen Huang is the coolest CEO ever. His style is fire (leather jacket yo) and he has a wicked sense of humor. He invited all interns to his house for an end of summer party and everyone loved him. It takes a cool CEO to build a cool company.

2. The work

NVIDIA is known for hardware, but they also have a strong software engineering team. My manager lets me choose what I want to work on. The work ranges from research to large-scale production, so there are plenty of projects for me to choose from. My work is not restricted to a tiny piece of code, and I can be the owner of a project. I can also work with people from other teams. Once, I was impressed with someone’s work and he was on another team. I reached out to him and we did a project together.

I realized that I loved my work at NVIDIA when one Saturday, I suddenly had the urge to come to the office to finish some experiments. I have never felt bored at work. There is always something to work on or someone to talk to. I’m learning so much.

3. The impact

Unlikely many big companies, NVIDIA managers share the company’s strategy with their staff. Even as a low-level engineer, I feel like I know where the company is headed.

NVIDIA is ambitious. They offer many projects with huge potential. I can make a difference with the project I’m working on. The managers watch out for your work and make sure they get the attention they deserve. I once volunteered to do something for the company and my work got detailed feedback from one VP and two senior directors. This is unlikely to happen at other big companies.

4. The fight
The hardware competition is red hot right now. Every tech giant seems to want to a piece of it. Working at NVIDIA gives me a front-row seat to watch it unfold.

When I asked my manager why he moved here from Apple, he said it’s because of “the fight”. For some reason I haven’t been able to internalize, being part of NVIDIA seems heroic. I want to do my best to help NVIDIA win the fight.

Some people might ask: what about the pay? Did money really not factor in my thought process at all? It’d be a lie to say I wasn’t tempted when someone mentioned their 6-figure sign on bonus fresh out of their master’s program. Like many of peers, I have the goal of one day having enough money to never have to work for money.

But for now, working in tech, I know that whatever job I choose will probably pay me well enough. I believe that if I opt for personal growth, one day, I will have a set of skills desirable enough to make more money than I care to spend. Youth is too precious to be sold for money.

General advice


  Know what you’re optimizing for: money, new experiences, prestige, personal growth, or something else?
  If you don’t know what you want to do, pay attention to what you do in your free time. It’s what you do when nobody is watching that shows your true interests.
  At least for AI, the line between industry and academia is getting blurrier and blurrier. Don’t fret about PhD or no PhD. You can always start your PhD and then drop out, or apply for a PhD after a few years in industry.
  Publish your code on GitHub and invest your time to build some decent GitHub repos. I’ve had more than one company that offered me a job because they were impressed with my GitHub.
  Don’t freak out about your GPA. If you think it’s low (e.g. < 3.3), just don’t put it on your resume. As long as you have a decent technical background (e.g. past internships + side projects + GitHub), nobody cares about your GPA.
  During college, intern at both big companies and startups to get a sense of what kind of organization you want to join.
  The easiest kind of offer you can aim for is an intern-to-full-time conversion offer. The intern interview process can be 3x easier than a full-time interview process.
  Start your job search early, preferably at the beginning of your senior year. Most of my friends get their full-time offers 2-3 quarters in advance. Early job offers give you peace of mind to be yourself in interviews and leverage to negotiate later.
  Don’t give up just because you haven’t heard back from some companies. I know people who sent their resumes to hundreds of companies until they got a job.
  Technical interviews are a pain. Prepare for them a least a month in advance.
  During interviews, ask about the kind of tasks you’ll do in the role, the manager you’ll report to, and the kind of mentorship you’ll get.
  Don’t shy away from negotiation. Even if you don’t work for money, you have every right to be paid your worth.
  In my experience, companies always match offers, even if they say they don’t. I’ve seen two friends with similar experience joining the same company for the same role, but one got $50k more a year because he had a competitive offer.
  Ask people you admire for their experiences and career advice.
  Read Glassdoor reviews to get a sense of what you’re getting yourself into.
  Take some time off between college and your first full-time job, as it’s unlikely you’ll have a vacation again for a long time.
  If the only reason you stay in a job is the pay, leave.
  If you don’t find yourself learning in a job, leave.
  Resist the rat race.
  Stop comparing yourself to other people. No matter how good you are, there will always be someone who’s better than you at something. Instead, compare you of today with you of yesterday.
  Be nice.
  Be kind to yourself. You don’t have to graduate at xx, get a PhD when you’re xx, or become a millionaire when you’re xx. Discover the world. Get to know yourself. Enjoy the process.


Acknowledgements
This post wouldn’t have been possible without long conversations over the year with many people I admire, in both industry and academia. I’d like to thank Christopher Manning, Mykel Kochenderfer, Alexander Rush, Lukasz Kaiser, Laurence Moroney, Danijar Hafner, Lucas Baker, Paul Warren, Jonathan Cohen, Boris Ginsburg, Rex Garland, David Buickians, Dung Ho Chi, and many other friends for putting up with my indecisiveness and unclogging my impossibly dense mind.

P/S: If this post makes you interested in learning more about working at NVIDIA, shoot me an email!


  
    While doing research, I often find myself wondering: “What’s the state-of-the-art result for XYZ right now?” I just want a tool that returns the summary of the latest SOTA research. My usual go-to place is Google, but I quickly realize that Google often returns:


  papers that have a lot of citations, which means they are old.
  articles that discuss SOTA systems. Again, these articles can be a couple of years old.
  irrelevant results that share the same name as XYZ.


At best, I have to skim those results. At worst, I don’t find what I’m looking for at all (I might if I go to the next page results but who does that?) Some examples:





And my personal favorite:



My next stop is Arxiv. Just for the record, I love Arxiv. But their search isn’t that good. For one thing, if you search “Transformer”, it’ll return all results that contain words of the same root including “transformation”, “transforms”, “transformed”. For another, it only shows the first half of the abstracts. You have to click to show more and skim to find relevant numbers.



So last week, while waiting for my roommates to warm up their pizza, I decided to write a script to query arxiv for abstracts that contain a specific keyword and return summaries of those abstracts. Some implementation details:


  I query arxiv directly instead of using their API because their export links somehow don’t return all the results.
  I prioritize abstracts that contain numbers and extract sentences that contain numbers. I mean, if I’m looking for the SOTA perplexity for language models, I just want to see the numbers.
  Each summary contains the paper’s title, first author, date of publication, abstract’s summary, and the link to the paper.
  The script is slow (a few seconds wait) because arxiv server takes a long time to response.
  It currently only works with python3 but I’ve received a request to make it compatible with python2.


The script is simple (283 lines of code) and requires 2 packages (nltk and PyEnchant). I was pleasantly surprised when I queried for a few keywords and it returned exactly what I needed. A couple of my friends at Nvidia, Facebook, and Google tried it and they told me they found it useful. A friend told me that the code was too simple and that I should throw some machine learning into it to get people interested. Talk about machine learning hype.





You can download the script and find the instruction on my GitHub here. Feedback is welcome!


  
